{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/Skin-Cancer-Diagnosis/blob/main/Benchmark%20-%20Resnet%20with%20SMOTE%20on%20Input%20Space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eus_4tUgfEk9",
        "outputId": "234d9326-4047-4445-c944-c083ff0ab658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_x4c0_DTkaa"
      },
      "source": [
        "#Library, atribut, and function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nR2MJBYq-oiB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix, accuracy_score\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN, KMeansSMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9-c7Xghg4SB4"
      },
      "outputs": [],
      "source": [
        "# input image size\n",
        "IMAGE_W = 224\n",
        "IMAGE_H = 224\n",
        "IMG_SIZE = (IMAGE_W,IMAGE_H)\n",
        "num_classes = 7\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "opt_adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "opt_SGD = SGD(learning_rate=0.001)\n",
        "the_arch = 'resnet50'\n",
        "\n",
        "#Callbacks\n",
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/best_model_attention.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/last_model_attention.h5'\n",
        "mc = ModelCheckpoint(best_model_fpath, monitor='val_balanced_acc', mode='max', verbose=1, save_best_only=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_balanced_acc', patience=10, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "early_stopping_monitor = EarlyStopping(patience=30,monitor='val_balanced_acc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JffFid9sOXeo"
      },
      "outputs": [],
      "source": [
        "# load train and test dataset\n",
        "def preprocess_image_input(input_images, arch = the_arch):\n",
        "  input_images = input_images.astype('float32')\n",
        "  if arch == 'inception_v3':\n",
        "    output_ims = tf.keras.applications.inception_v3.preprocess_input(input_images)\n",
        "  else:\n",
        "    output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n",
        "\n",
        "def load_cifar10_dataset():\n",
        "  from keras.datasets import cifar10\n",
        "    # load dataset\n",
        "  (X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
        "    # one hot encode target values\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_val = to_categorical(y_val)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def balanced_acc(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "\n",
        "    tensor1 = tf.math.argmax(y_true, axis=1)\n",
        "    tensor2 = tf.math.argmax(y_pred, axis=1)\n",
        "\n",
        "    cm = tf.math.confusion_matrix(tensor1, tensor2)\n",
        "    \n",
        "    diag = tf.linalg.tensor_diag_part (cm)\n",
        "    tpfn = tf.cast(K.sum(cm, axis = 1), tf.float32) + K.epsilon()\n",
        "    recall = tf.divide(tf.cast(diag, tf.float32),tpfn)\n",
        "    balanced_acc = K.mean(recall)\n",
        "    return balanced_acc\n",
        "\n",
        "def define_base_model(arch = the_arch, start_trainable_layer = 9999, attention=False):\n",
        "  #x = data_augmentation(input_tensor)\n",
        "  #x = layers.Rescaling(1.0 / 255)(input_tensor)  # Rescale inputs\n",
        "  if arch != 'dense':\n",
        "    input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "    #x = UpSampling2D(size=(7,7))(input_tensor)\n",
        "    if arch == 'resnet50':\n",
        "      base_model = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    elif arch == 'inception_v3':\n",
        "      base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    elif arch == 'ResNet':\n",
        "      base_model = ResNet(classes ,image_shape)(input_tensor)\n",
        "    \n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "    if start_trainable_layer != 9999:\n",
        "      for layer in base_model.layers[start_trainable_layer:]:\n",
        "        layer.trainable = True\n",
        "    \n",
        "    x = base_model.output\n",
        "    if attention:\n",
        "      x = Attention(1024,1024,7,8)(x)\n",
        "    \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "  else:\n",
        "    input_tensor = Input(shape=(2048))\n",
        "    x = input_tensor\n",
        "  #x = Flatten()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  #x = Dropout(0.2)(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  predictions = Dense(num_classes, activation='softmax')(x)\n",
        "  model = Model(inputs=input_tensor, outputs=predictions)\n",
        "  model.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "  return model\n",
        "\n",
        "def define_model_resnet():\n",
        "  input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "  x = ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)(input_tensor, training=False)\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  predictions = Dense(num_classes, activation='softmax')(x)\n",
        "  model = Model(inputs=input_tensor, outputs=predictions)\n",
        "  model.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "  return model\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "    # plot loss\n",
        "    plt.subplot(211)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(history.history['loss'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "    # plot accuracy\n",
        "    plt.subplot(212)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        " \n",
        "# scale pixels\n",
        "def norm_pixels(train, test):\n",
        "    # convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    # normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    # return normalized images\n",
        "    return train_norm, test_norm\n",
        "\n",
        "def load_isic2018_dataset(train_under_frac = 0):\n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv') \n",
        "  df_val = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv') \n",
        "\n",
        "  #decode one hot label\n",
        "  df_train[\"Labels\"] = (df_train.iloc[:, 1:]).idxmax(axis=1)\n",
        "  df_val[\"Labels\"] = (df_val.iloc[:, 1:]).idxmax(axis=1)\n",
        "\n",
        "  #random undersampling for training dataset\n",
        "  if train_under_frac !=0:\n",
        "    df_train = df_train.drop(df_train[df_train['Labels'] == 'NV'].sample(frac=train_under_frac).index)\n",
        "\n",
        "  #drop one-hot column\n",
        "  df_train = df_train.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "  df_val = df_val.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "\n",
        "  #make filepaths of the image\n",
        "  dir_train = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_Input/'\n",
        "  dir_val = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_Input/'\n",
        "  df_train['FilePaths'] = dir_train + df_train['image'] + '.jpg'\n",
        "  df_val['FilePaths'] = dir_val + df_val['image'] + '.jpg'\n",
        "  \n",
        "  #load image pixels to dataframe\n",
        "  df_train['image_px'] = df_train['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "  df_val['image_px'] = df_val['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def reset_dataset(df_train, df_val):\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  X_train = preprocess_image_input(X_train, the_arch)\n",
        "  X_val = preprocess_image_input(X_val, the_arch)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def SMOTE_Data(X, y, one_hot = False, k = 5, width = IMAGE_W, height = IMAGE_H, c = 3, type = 'smote'):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  if type == 'borderline':\n",
        "    sm = BorderlineSMOTE(random_state=42, k_neighbors=k)\n",
        "  elif type == 'svm':\n",
        "    sm = SVMSMOTE()\n",
        "  elif type == 'adasyn':\n",
        "    sm = ADASYN(random_state=42, n_neighbors=k)\n",
        "  elif type == 'kmeans':\n",
        "    sm = KMeansSMOTE(k_neighbors=k, kmeans_estimator=10)\n",
        "  else:\n",
        "    sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X.reshape((-1, width * height * c)), y)\n",
        "  X_resampled = X_resampled.reshape(-1, width, height, c)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled\n",
        "\n",
        "def SMOTE_Data2(X, y, one_hot = False, k = 5, type = 'smote'):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  if type == 'borderline':\n",
        "    sm = BorderlineSMOTE(random_state=42, k_neighbors=k)\n",
        "  elif type == 'svm':\n",
        "    sm = SVMSMOTE()\n",
        "  elif type == 'adasyn':\n",
        "    sm = ADASYN(random_state=42, n_neighbors=k)\n",
        "  elif type == 'kmeans':\n",
        "    sm = KMeansSMOTE(k_neighbors=k, kmeans_estimator=10)\n",
        "  else:\n",
        "    sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inner-Borderline SMOTE"
      ],
      "metadata": {
        "id": "BE9FCWBe8deT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class(X, y, c):\n",
        "    xbeg = X[y == c]\n",
        "    ybeg = y[y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "def find_border(xclass, yclass, X, y, cli, n_neigh=5):\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(xclass)\n",
        "    ret = []\n",
        "    for i in range(len(ind)):\n",
        "      ret.append(sum(y[ind[i,j]] != cli for j in range(1,len(ind[i]))))\n",
        "    ret = np.array(ret)\n",
        "    xclass = xclass[np.logical_and(ret < (n_neigh-1),ret > ((n_neigh-1)/2))]\n",
        "    yclass = yclass[np.logical_and(ret < (n_neigh-1),ret > ((n_neigh-1)/2))]\n",
        "\n",
        "    return xclass, yclass\n",
        "def find_inner_border(xclass, yclass, X, y, cli, n_neigh=5):\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "    ret = []\n",
        "    for i in range(len(ind)):\n",
        "      if y[i] != cli:\n",
        "        ret.append(n_neigh)  \n",
        "      else:\n",
        "        ret.append(sum(y[ind[i,j]] != cli for j in range(1,len(ind[i]))))\n",
        "    ret = np.array(ret)\n",
        "    is_border = np.logical_and(ret < (n_neigh-1),ret > ((n_neigh-1)/2))\n",
        "    \n",
        "    ret = []\n",
        "    for i in range(len(ind)):\n",
        "      ret.append(sum(is_border[ind[i,j]] for j in range(1,len(ind[i]))))\n",
        "    ret = np.array(ret)\n",
        "    xclass = X[np.logical_and(np.logical_not(is_border),ret > 0)]\n",
        "    yclass = y[np.logical_and(np.logical_not(is_border),ret > 0)]\n",
        "    return xclass, yclass\n",
        "\n",
        "def G_SM(xclass,n_to_sample,cl, n_neigh = 6):\n",
        "    \n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(xclass)\n",
        "    dist, ind = nn.kneighbors(xclass)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(xclass))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = xclass[base_indices]\n",
        "    X_neighbor = xclass[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "def Borderline_SMOTE(X_train, y_train, random_state=42, k_neighbors=5, start=0, n=7):\n",
        "  #reshape X_train\n",
        "  X_train = X_train.reshape(-1, IMAGE_W * IMAGE_H * 3)\n",
        "  #decode y_train from one-hot encoding\n",
        "  y_train = np.argmax(y_train, axis=1) \n",
        "\n",
        "  counter = Counter(y_train)\n",
        "  key_max = max(counter, key=counter.get)\n",
        "  class_max = counter[key_max]\n",
        "  resx=[]\n",
        "  resy=[]\n",
        "\n",
        "  for i in range(start,n):\n",
        "      xclass, yclass = get_class(X_train, y_train, i)\n",
        "      if xclass.shape[0] == class_max:\n",
        "        continue\n",
        "      xclass_bdr, yclass_bdr = find_inner_border(xclass, yclass, X_train, y_train, i, n_neigh=k_neighbors)\n",
        "      n = class_max - xclass.shape[0]\n",
        "      xsamp, ysamp = G_SM(xclass_bdr,n,i, n_neigh=k_neighbors)\n",
        "      ysamp = np.array(ysamp)\n",
        "      resx.append(xsamp)\n",
        "      resy.append(ysamp)\n",
        "  \n",
        "  resx = np.vstack(resx)\n",
        "  resy = np.hstack(resy)\n",
        "  X_train = np.vstack((resx,X_train))\n",
        "  y_train = np.hstack((resy,y_train))\n",
        "  y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, IMAGE_W, IMAGE_H, 3)\n",
        "  return X_train, y_train"
      ],
      "metadata": {
        "id": "s3UnuaKz8kzJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7sLC2svMuJ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qge6cnxQPnH6",
        "outputId": "74d0aa5c-37bd-4a1b-fbc5-df66c89c92cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5321, 224, 224, 3)\n",
            "(5321, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 1113, 2: 1099, 1: 514, 0: 327, 6: 142, 3: 115})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "df1 = pd.read_pickle(path+\"isic2018_train.pkl\")\n",
        "X_train = df1.loc[:, df1.columns != 'y_train'].to_numpy()\n",
        "X_train = X_train.reshape(-1,IMAGE_W,IMAGE_H,3)\n",
        "y_train = df1.loc[:, df1.columns == 'y_train'].to_numpy()\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "df1 = pd.read_pickle(path+\"isic2018_val.pkl\")\n",
        "X_val = df1.loc[:, df1.columns != 'y_val'].to_numpy()\n",
        "X_val = X_val.reshape(-1,IMAGE_W,IMAGE_H,3)\n",
        "y_val = df1.loc[:, df1.columns == 'y_val'].to_numpy()\n",
        "y_val = to_categorical(y_val)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "4S_ECkMlNgV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_val = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv') \n",
        "df_val[\"Labels\"] = (df_val.iloc[:, 1:]).idxmax(axis=1)\n",
        "df_val = df_val.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "dir_val = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_Input/'\n",
        "df_val['FilePaths'] = dir_val + df_val['image'] + '.jpg'\n",
        "df_val['image_px'] = df_val['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "X_val = np.asarray(df_val['image_px'].tolist())\n",
        "y_val = np.array(df_val['Labels'].values)\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "y_val = label_encoder.fit_transform(y_val)\n",
        "y_val = to_categorical(y_val, num_classes = num_classes)"
      ],
      "metadata": {
        "id": "y7aoc6FgOSkj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = preprocess_image_input(X_val, the_arch)"
      ],
      "metadata": {
        "id": "JmyQbfv7PeGq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMs5ueGqNiWY",
        "outputId": "69e4cbe4-9a53-4a53-cd98-b1ddcdba571a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[-7.2939003e+01, -9.2778999e+01, -8.8680000e+01],\n",
              "         [-6.5939003e+01, -8.5778999e+01, -8.0680000e+01],\n",
              "         [-5.5939003e+01, -7.5778999e+01, -7.0680000e+01],\n",
              "         ...,\n",
              "         [-6.7939003e+01, -8.0778999e+01, -7.2680000e+01],\n",
              "         [-6.6939003e+01, -7.9778999e+01, -7.1680000e+01],\n",
              "         [-6.4939003e+01, -7.7778999e+01, -6.9680000e+01]],\n",
              "\n",
              "        [[-6.7939003e+01, -8.8778999e+01, -8.3680000e+01],\n",
              "         [-6.0939003e+01, -7.9778999e+01, -7.4680000e+01],\n",
              "         [-5.3939003e+01, -7.0778999e+01, -6.3680000e+01],\n",
              "         ...,\n",
              "         [-6.6939003e+01, -8.1778999e+01, -7.2680000e+01],\n",
              "         [-6.5939003e+01, -7.8778999e+01, -7.1680000e+01],\n",
              "         [-6.4939003e+01, -7.8778999e+01, -7.0680000e+01]],\n",
              "\n",
              "        [[-6.3939003e+01, -8.2778999e+01, -7.7680000e+01],\n",
              "         [-5.6939003e+01, -7.4778999e+01, -6.7680000e+01],\n",
              "         [-4.8939003e+01, -6.4778999e+01, -5.8680000e+01],\n",
              "         ...,\n",
              "         [-6.6939003e+01, -8.2778999e+01, -7.1680000e+01],\n",
              "         [-6.6939003e+01, -7.9778999e+01, -7.2680000e+01],\n",
              "         [-6.5939003e+01, -7.8778999e+01, -7.1680000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-6.4939003e+01, -8.3778999e+01, -6.9680000e+01],\n",
              "         [-6.6939003e+01, -8.4778999e+01, -7.1680000e+01],\n",
              "         [-6.4939003e+01, -8.3778999e+01, -7.1680000e+01],\n",
              "         ...,\n",
              "         [-8.5939003e+01, -1.0377900e+02, -1.0168000e+02],\n",
              "         [-8.7939003e+01, -1.0477900e+02, -1.0168000e+02],\n",
              "         [-8.8939003e+01, -1.0377900e+02, -1.0168000e+02]],\n",
              "\n",
              "        [[-6.3939003e+01, -8.2778999e+01, -6.8680000e+01],\n",
              "         [-6.4939003e+01, -8.2778999e+01, -6.9680000e+01],\n",
              "         [-6.4939003e+01, -8.3778999e+01, -7.0680000e+01],\n",
              "         ...,\n",
              "         [-8.7939003e+01, -1.0377900e+02, -1.0168000e+02],\n",
              "         [-8.8939003e+01, -1.0477900e+02, -1.0268000e+02],\n",
              "         [-8.7939003e+01, -1.0377900e+02, -1.0168000e+02]],\n",
              "\n",
              "        [[-6.0939003e+01, -8.0778999e+01, -6.7680000e+01],\n",
              "         [-6.1939003e+01, -8.0778999e+01, -6.9680000e+01],\n",
              "         [-6.5939003e+01, -8.3778999e+01, -6.9680000e+01],\n",
              "         ...,\n",
              "         [-8.8939003e+01, -1.0377900e+02, -1.0168000e+02],\n",
              "         [-8.8939003e+01, -1.0477900e+02, -1.0268000e+02],\n",
              "         [-8.7939003e+01, -1.0377900e+02, -1.0168000e+02]]],\n",
              "\n",
              "\n",
              "       [[[ 2.0060997e+01,  1.2221001e+01,  8.4320000e+01],\n",
              "         [ 2.9060997e+01,  1.7221001e+01,  8.6320000e+01],\n",
              "         [ 4.0060997e+01,  2.4221001e+01,  9.0320000e+01],\n",
              "         ...,\n",
              "         [ 3.9060997e+01,  2.6221001e+01,  8.3320000e+01],\n",
              "         [ 4.0060997e+01,  2.4221001e+01,  8.5320000e+01],\n",
              "         [ 4.1060997e+01,  2.2221001e+01,  8.7320000e+01]],\n",
              "\n",
              "        [[ 2.2060997e+01,  1.5221001e+01,  8.4320000e+01],\n",
              "         [ 3.0060997e+01,  1.9221001e+01,  8.7320000e+01],\n",
              "         [ 3.6060997e+01,  2.5221001e+01,  9.0320000e+01],\n",
              "         ...,\n",
              "         [ 4.1060997e+01,  2.8221001e+01,  8.6320000e+01],\n",
              "         [ 3.8060997e+01,  2.3221001e+01,  8.4320000e+01],\n",
              "         [ 3.9060997e+01,  2.2221001e+01,  8.6320000e+01]],\n",
              "\n",
              "        [[ 2.9060997e+01,  1.7221001e+01,  8.4320000e+01],\n",
              "         [ 3.1060997e+01,  2.1221001e+01,  8.9320000e+01],\n",
              "         [ 3.5060997e+01,  2.6221001e+01,  9.1320000e+01],\n",
              "         ...,\n",
              "         [ 4.1060997e+01,  2.5221001e+01,  8.4320000e+01],\n",
              "         [ 3.8060997e+01,  2.4221001e+01,  8.3320000e+01],\n",
              "         [ 3.5060997e+01,  2.1221001e+01,  8.4320000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 7.0609970e+00,  1.2221001e+01,  6.9320000e+01],\n",
              "         [-2.9390030e+00,  3.2210007e+00,  6.5320000e+01],\n",
              "         [-8.9390030e+00,  2.2100067e-01,  6.2320000e+01],\n",
              "         ...,\n",
              "         [ 1.3060997e+01,  7.2210007e+00,  6.4320000e+01],\n",
              "         [ 1.3060997e+01,  1.0221001e+01,  6.3320000e+01],\n",
              "         [ 1.3060997e+01,  9.2210007e+00,  6.3320000e+01]],\n",
              "\n",
              "        [[ 6.0609970e+00,  1.0221001e+01,  6.9320000e+01],\n",
              "         [-9.3900299e-01,  4.2210007e+00,  6.7320000e+01],\n",
              "         [-8.9390030e+00, -1.7789993e+00,  6.4320000e+01],\n",
              "         ...,\n",
              "         [ 1.1060997e+01,  7.2210007e+00,  6.2320000e+01],\n",
              "         [ 1.0060997e+01,  9.2210007e+00,  5.9320000e+01],\n",
              "         [ 1.1060997e+01,  8.2210007e+00,  6.1320000e+01]],\n",
              "\n",
              "        [[ 6.0609970e+00,  9.2210007e+00,  6.9320000e+01],\n",
              "         [ 6.0997009e-02,  4.2210007e+00,  6.5320000e+01],\n",
              "         [-3.9390030e+00,  1.2210007e+00,  6.3320000e+01],\n",
              "         ...,\n",
              "         [ 1.1060997e+01,  7.2210007e+00,  6.3320000e+01],\n",
              "         [ 9.0609970e+00,  7.2210007e+00,  6.0320000e+01],\n",
              "         [ 9.0609970e+00,  7.2210007e+00,  6.1320000e+01]]],\n",
              "\n",
              "\n",
              "       [[[ 2.2060997e+01,  1.0221001e+01,  4.6320000e+01],\n",
              "         [ 2.6060997e+01,  1.3221001e+01,  4.9320000e+01],\n",
              "         [ 3.5060997e+01,  2.3221001e+01,  5.1320000e+01],\n",
              "         ...,\n",
              "         [ 1.7060997e+01,  1.0221001e+01,  4.8320000e+01],\n",
              "         [ 1.7060997e+01,  1.0221001e+01,  4.9320000e+01],\n",
              "         [ 2.1060997e+01,  1.2221001e+01,  5.1320000e+01]],\n",
              "\n",
              "        [[ 2.8060997e+01,  1.6221001e+01,  4.9320000e+01],\n",
              "         [ 3.2060997e+01,  1.9221001e+01,  5.1320000e+01],\n",
              "         [ 4.0060997e+01,  2.5221001e+01,  5.3320000e+01],\n",
              "         ...,\n",
              "         [ 2.0060997e+01,  1.0221001e+01,  4.7320000e+01],\n",
              "         [ 1.8060997e+01,  1.0221001e+01,  4.9320000e+01],\n",
              "         [ 2.1060997e+01,  1.2221001e+01,  5.1320000e+01]],\n",
              "\n",
              "        [[ 2.9060997e+01,  2.3221001e+01,  5.2320000e+01],\n",
              "         [ 2.9060997e+01,  2.2221001e+01,  5.2320000e+01],\n",
              "         [ 4.1060997e+01,  2.6221001e+01,  5.4320000e+01],\n",
              "         ...,\n",
              "         [ 2.5060997e+01,  1.2221001e+01,  4.9320000e+01],\n",
              "         [ 2.6060997e+01,  1.4221001e+01,  4.9320000e+01],\n",
              "         [ 2.9060997e+01,  1.7221001e+01,  5.1320000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 3.0060997e+01,  2.4221001e+01,  5.7320000e+01],\n",
              "         [ 3.9060997e+01,  3.1221001e+01,  6.3320000e+01],\n",
              "         [ 4.1060997e+01,  3.3221001e+01,  6.5320000e+01],\n",
              "         ...,\n",
              "         [ 3.6060997e+01,  1.4221001e+01,  4.5320000e+01],\n",
              "         [ 2.6060997e+01,  9.2210007e+00,  4.1320000e+01],\n",
              "         [ 2.0060997e+01,  3.2210007e+00,  4.2320000e+01]],\n",
              "\n",
              "        [[ 3.7060997e+01,  2.9221001e+01,  5.9320000e+01],\n",
              "         [ 4.5060997e+01,  3.8221001e+01,  6.5320000e+01],\n",
              "         [ 4.9060997e+01,  3.9221001e+01,  6.8320000e+01],\n",
              "         ...,\n",
              "         [ 3.5060997e+01,  1.3221001e+01,  4.5320000e+01],\n",
              "         [ 2.6060997e+01,  8.2210007e+00,  4.0320000e+01],\n",
              "         [ 2.1060997e+01,  6.2210007e+00,  4.4320000e+01]],\n",
              "\n",
              "        [[ 4.5060997e+01,  3.4221001e+01,  6.1320000e+01],\n",
              "         [ 5.0060997e+01,  4.0221001e+01,  6.6320000e+01],\n",
              "         [ 5.7060997e+01,  4.6221001e+01,  6.9320000e+01],\n",
              "         ...,\n",
              "         [ 3.2060997e+01,  1.0221001e+01,  4.2320000e+01],\n",
              "         [ 2.4060997e+01,  5.2210007e+00,  3.9320000e+01],\n",
              "         [ 2.0060997e+01,  6.2210007e+00,  4.2320000e+01]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[ 9.6060997e+01,  6.4221001e+01,  5.8320000e+01],\n",
              "         [ 1.0006100e+02,  6.6221001e+01,  5.8320000e+01],\n",
              "         [ 1.0106100e+02,  6.7221001e+01,  6.0320000e+01],\n",
              "         ...,\n",
              "         [ 9.5060997e+01,  6.7221001e+01,  6.2320000e+01],\n",
              "         [ 9.5060997e+01,  6.9221001e+01,  6.1320000e+01],\n",
              "         [ 9.7060997e+01,  6.9221001e+01,  6.2320000e+01]],\n",
              "\n",
              "        [[ 9.6060997e+01,  6.6221001e+01,  5.8320000e+01],\n",
              "         [ 9.8060997e+01,  6.6221001e+01,  5.7320000e+01],\n",
              "         [ 1.0006100e+02,  6.8221001e+01,  6.0320000e+01],\n",
              "         ...,\n",
              "         [ 9.5060997e+01,  7.0221001e+01,  6.2320000e+01],\n",
              "         [ 9.6060997e+01,  6.8221001e+01,  6.3320000e+01],\n",
              "         [ 9.6060997e+01,  6.8221001e+01,  6.4320000e+01]],\n",
              "\n",
              "        [[ 9.9060997e+01,  6.8221001e+01,  5.8320000e+01],\n",
              "         [ 1.0006100e+02,  6.8221001e+01,  5.9320000e+01],\n",
              "         [ 1.0006100e+02,  6.8221001e+01,  5.9320000e+01],\n",
              "         ...,\n",
              "         [ 9.4060997e+01,  6.9221001e+01,  6.3320000e+01],\n",
              "         [ 9.6060997e+01,  6.8221001e+01,  6.2320000e+01],\n",
              "         [ 9.6060997e+01,  6.9221001e+01,  6.3320000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 9.6060997e+01,  6.9221001e+01,  6.1320000e+01],\n",
              "         [ 9.7060997e+01,  7.1221001e+01,  6.1320000e+01],\n",
              "         [ 9.5060997e+01,  6.9221001e+01,  5.8320000e+01],\n",
              "         ...,\n",
              "         [ 1.0306100e+02,  7.3221001e+01,  6.7320000e+01],\n",
              "         [ 1.0206100e+02,  7.2221001e+01,  6.6320000e+01],\n",
              "         [ 1.0406100e+02,  7.3221001e+01,  6.6320000e+01]],\n",
              "\n",
              "        [[ 9.5060997e+01,  6.9221001e+01,  6.0320000e+01],\n",
              "         [ 9.4060997e+01,  6.9221001e+01,  6.0320000e+01],\n",
              "         [ 9.4060997e+01,  6.8221001e+01,  5.9320000e+01],\n",
              "         ...,\n",
              "         [ 1.0106100e+02,  7.1221001e+01,  6.6320000e+01],\n",
              "         [ 1.0206100e+02,  7.1221001e+01,  6.8320000e+01],\n",
              "         [ 1.0306100e+02,  7.1221001e+01,  6.7320000e+01]],\n",
              "\n",
              "        [[ 9.5060997e+01,  6.8221001e+01,  6.0320000e+01],\n",
              "         [ 9.3060997e+01,  6.7221001e+01,  5.9320000e+01],\n",
              "         [ 9.4060997e+01,  6.8221001e+01,  5.8320000e+01],\n",
              "         ...,\n",
              "         [ 1.0106100e+02,  7.0221001e+01,  6.4320000e+01],\n",
              "         [ 1.0106100e+02,  7.2221001e+01,  6.6320000e+01],\n",
              "         [ 1.0106100e+02,  7.4221001e+01,  6.6320000e+01]]],\n",
              "\n",
              "\n",
              "       [[[ 4.5060997e+01,  1.0221001e+01,  2.6320000e+01],\n",
              "         [ 4.7060997e+01,  9.2210007e+00,  2.6320000e+01],\n",
              "         [ 4.7060997e+01,  9.2210007e+00,  2.6320000e+01],\n",
              "         ...,\n",
              "         [ 6.1060997e+01,  2.2221001e+01,  3.8320000e+01],\n",
              "         [ 6.1060997e+01,  2.3221001e+01,  3.5320000e+01],\n",
              "         [ 6.0060997e+01,  2.5221001e+01,  3.4320000e+01]],\n",
              "\n",
              "        [[ 4.6060997e+01,  1.0221001e+01,  2.6320000e+01],\n",
              "         [ 4.9060997e+01,  9.2210007e+00,  2.6320000e+01],\n",
              "         [ 4.8060997e+01,  9.2210007e+00,  2.7320000e+01],\n",
              "         ...,\n",
              "         [ 6.1060997e+01,  2.2221001e+01,  3.6320000e+01],\n",
              "         [ 6.2060997e+01,  2.1221001e+01,  3.6320000e+01],\n",
              "         [ 6.0060997e+01,  2.3221001e+01,  3.4320000e+01]],\n",
              "\n",
              "        [[ 4.8060997e+01,  9.2210007e+00,  2.7320000e+01],\n",
              "         [ 4.8060997e+01,  1.0221001e+01,  2.6320000e+01],\n",
              "         [ 4.8060997e+01,  1.0221001e+01,  2.7320000e+01],\n",
              "         ...,\n",
              "         [ 6.1060997e+01,  2.1221001e+01,  3.5320000e+01],\n",
              "         [ 6.0060997e+01,  1.9221001e+01,  3.6320000e+01],\n",
              "         [ 6.0060997e+01,  1.9221001e+01,  3.5320000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 4.7060997e+01,  4.2210007e+00,  2.3320000e+01],\n",
              "         [ 4.6060997e+01,  4.2210007e+00,  2.2320000e+01],\n",
              "         [ 4.6060997e+01,  3.2210007e+00,  2.0320000e+01],\n",
              "         ...,\n",
              "         [ 5.2060997e+01,  1.8221001e+01,  3.4320000e+01],\n",
              "         [ 5.3060997e+01,  1.8221001e+01,  3.5320000e+01],\n",
              "         [ 5.4060997e+01,  1.8221001e+01,  3.3320000e+01]],\n",
              "\n",
              "        [[ 4.6060997e+01, -7.7899933e-01,  1.8320000e+01],\n",
              "         [ 4.5060997e+01,  2.2210007e+00,  2.1320000e+01],\n",
              "         [ 4.5060997e+01,  3.2210007e+00,  1.9320000e+01],\n",
              "         ...,\n",
              "         [ 5.3060997e+01,  1.8221001e+01,  3.5320000e+01],\n",
              "         [ 5.2060997e+01,  1.8221001e+01,  3.3320000e+01],\n",
              "         [ 5.2060997e+01,  1.6221001e+01,  3.2320000e+01]],\n",
              "\n",
              "        [[ 4.5060997e+01, -3.7789993e+00,  1.8320000e+01],\n",
              "         [ 4.5060997e+01,  2.2100067e-01,  2.1320000e+01],\n",
              "         [ 4.7060997e+01,  2.2210007e+00,  1.9320000e+01],\n",
              "         ...,\n",
              "         [ 5.4060997e+01,  1.9221001e+01,  3.5320000e+01],\n",
              "         [ 5.2060997e+01,  1.7221001e+01,  3.4320000e+01],\n",
              "         [ 5.2060997e+01,  1.7221001e+01,  3.3320000e+01]]],\n",
              "\n",
              "\n",
              "       [[[-8.9390030e+00, -8.7789993e+00,  4.3320000e+01],\n",
              "         [-4.9390030e+00, -1.7789993e+00,  5.4320000e+01],\n",
              "         [ 4.0609970e+00,  4.2210007e+00,  5.7320000e+01],\n",
              "         ...,\n",
              "         [ 1.3060997e+01,  1.0221001e+01,  6.1320000e+01],\n",
              "         [ 8.0609970e+00,  6.2210007e+00,  5.8320000e+01],\n",
              "         [ 9.0609970e+00,  3.2210007e+00,  5.7320000e+01]],\n",
              "\n",
              "        [[ 1.6060997e+01,  1.8221001e+01,  6.6320000e+01],\n",
              "         [ 1.0060997e+01,  1.7221001e+01,  7.4320000e+01],\n",
              "         [ 7.0609970e+00,  1.5221001e+01,  7.5320000e+01],\n",
              "         ...,\n",
              "         [ 2.2060997e+01,  2.3221001e+01,  8.0320000e+01],\n",
              "         [ 1.8060997e+01,  2.0221001e+01,  7.9320000e+01],\n",
              "         [ 1.5060997e+01,  1.8221001e+01,  7.7320000e+01]],\n",
              "\n",
              "        [[ 2.3060997e+01,  2.2221001e+01,  6.9320000e+01],\n",
              "         [ 2.4060997e+01,  2.6221001e+01,  8.0320000e+01],\n",
              "         [ 1.8060997e+01,  2.3221001e+01,  7.9320000e+01],\n",
              "         ...,\n",
              "         [ 2.4060997e+01,  2.6221001e+01,  7.6320000e+01],\n",
              "         [ 2.3060997e+01,  2.5221001e+01,  7.9320000e+01],\n",
              "         [ 2.0060997e+01,  2.3221001e+01,  8.1320000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 4.0060997e+01,  4.1221001e+01,  7.9320000e+01],\n",
              "         [ 4.7060997e+01,  5.3221001e+01,  9.3320000e+01],\n",
              "         [ 4.5060997e+01,  5.1221001e+01,  9.2320000e+01],\n",
              "         ...,\n",
              "         [ 5.7060997e+01,  5.0221001e+01,  9.1320000e+01],\n",
              "         [ 5.7060997e+01,  5.0221001e+01,  9.0320000e+01],\n",
              "         [ 5.3060997e+01,  4.7221001e+01,  9.0320000e+01]],\n",
              "\n",
              "        [[ 3.8060997e+01,  4.0221001e+01,  8.0320000e+01],\n",
              "         [ 4.2060997e+01,  4.8221001e+01,  9.3320000e+01],\n",
              "         [ 4.0060997e+01,  4.8221001e+01,  9.2320000e+01],\n",
              "         ...,\n",
              "         [ 5.7060997e+01,  5.0221001e+01,  9.0320000e+01],\n",
              "         [ 5.6060997e+01,  4.8221001e+01,  9.0320000e+01],\n",
              "         [ 5.2060997e+01,  4.7221001e+01,  8.9320000e+01]],\n",
              "\n",
              "        [[ 3.7060997e+01,  4.0221001e+01,  7.8320000e+01],\n",
              "         [ 4.2060997e+01,  4.7221001e+01,  9.2320000e+01],\n",
              "         [ 3.9060997e+01,  4.7221001e+01,  9.1320000e+01],\n",
              "         ...,\n",
              "         [ 5.5060997e+01,  4.7221001e+01,  8.8320000e+01],\n",
              "         [ 5.4060997e+01,  4.7221001e+01,  8.9320000e+01],\n",
              "         [ 4.9060997e+01,  4.5221001e+01,  8.7320000e+01]]]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xArGWuciBt_-",
        "outputId": "a742d4d5-9126-40b6-9a7d-ba2f53705524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14077, 224, 224, 3)\n",
            "(14077, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({4: 2011, 2: 2011, 5: 2011, 3: 2011, 0: 2011, 1: 2011, 6: 2011})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = SMOTE_Data(X_train, y_train, True, type = 'smote')\n",
        "#X_train, y_train = Borderline_SMOTE(X_train, y_train)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_fm_ov = np.append(X_train_fm_ov, np.zeros(shape=(200, 2048), dtype='object'), axis=0)\n",
        "y_train_ov = np.argmax(y_train_ov, axis=1) \n",
        "y_train_ov = y_train_ov.reshape(-1,1)\n",
        "y_train_ov = np.append(y_train_ov, np.zeros(shape=(200, 1), dtype='object'))\n",
        "y_train = to_categorical(y_train_ov)\n",
        "print(X_train_fm_ov.shape)\n",
        "print(y_train.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V5PjA7jFhVU",
        "outputId": "80afb394-3ef2-41ad-f477-4e0203e9535f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14277, 2048)\n",
            "(14277, 7)\n",
            "Counter train data:  Counter({0: 2211, 5: 2011, 4: 2011, 2: 2011, 3: 2011, 1: 2011, 6: 2011})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove rows having all zeroes\n",
        "index = range(14077,14277)\n",
        "y_train = np.delete(y_train, index, axis = 0)\n",
        "X_train = np.delete(X_train, index, axis = 0)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lFpLlexMUaM",
        "outputId": "59dfc36a-e35a-4599-9c65-45a281c87241"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15277, 32, 32, 3)\n",
            "(15277, 7)\n",
            "Counter train data:  Counter({5: 2211, 4: 2211, 2: 2211, 3: 2211, 1: 2211, 6: 2211, 0: 2011})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "V7Z_nccu6QjB"
      },
      "outputs": [],
      "source": [
        "#path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "#df1 = pd.DataFrame(X_train.reshape(X_train.shape[0],-1))\n",
        "#df1['y_train'] = np.argmax(y_train, axis=1).tolist()\n",
        "df2 = pd.DataFrame(X_val.reshape(X_val.shape[0],-1))\n",
        "df2['y_val'] = np.argmax(y_val, axis=1).tolist()\n",
        "#df1.to_pickle(path+\"isic2018_train_under83.pkl\")\n",
        "df2.to_pickle(path+\"isic2018_val.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vIygrW81Ln4z",
        "outputId": "4c8deb1e-d0e1-45da-b94b-df5d309f92a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 0s 0us/step\n",
            "94781440/94765736 [==============================] - 0s 0us/step\n",
            "Epoch 1/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0780 - accuracy: 0.5952 - balanced_acc: 0.5962\n",
            "Epoch 1: val_balanced_acc improved from -inf to 0.41264, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 160s 665ms/step - loss: 1.0780 - accuracy: 0.5952 - balanced_acc: 0.5962 - val_loss: 1.0681 - val_accuracy: 0.6425 - val_balanced_acc: 0.4126 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6235 - accuracy: 0.7653 - balanced_acc: 0.7669\n",
            "Epoch 2: val_balanced_acc improved from 0.41264 to 0.59725, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 150s 675ms/step - loss: 0.6235 - accuracy: 0.7653 - balanced_acc: 0.7669 - val_loss: 0.7226 - val_accuracy: 0.7617 - val_balanced_acc: 0.5973 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4737 - accuracy: 0.8299 - balanced_acc: 0.8316\n",
            "Epoch 3: val_balanced_acc did not improve from 0.59725\n",
            "219/219 [==============================] - 147s 671ms/step - loss: 0.4737 - accuracy: 0.8299 - balanced_acc: 0.8316 - val_loss: 0.6910 - val_accuracy: 0.7306 - val_balanced_acc: 0.5450 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3889 - accuracy: 0.8558 - balanced_acc: 0.8565\n",
            "Epoch 4: val_balanced_acc improved from 0.59725 to 0.61667, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 149s 679ms/step - loss: 0.3889 - accuracy: 0.8558 - balanced_acc: 0.8565 - val_loss: 0.7685 - val_accuracy: 0.7409 - val_balanced_acc: 0.6167 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3212 - accuracy: 0.8854 - balanced_acc: 0.8854\n",
            "Epoch 5: val_balanced_acc did not improve from 0.61667\n",
            "219/219 [==============================] - 147s 672ms/step - loss: 0.3212 - accuracy: 0.8854 - balanced_acc: 0.8854 - val_loss: 0.7087 - val_accuracy: 0.7254 - val_balanced_acc: 0.4467 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9113 - balanced_acc: 0.9108\n",
            "Epoch 6: val_balanced_acc did not improve from 0.61667\n",
            "219/219 [==============================] - 147s 670ms/step - loss: 0.2549 - accuracy: 0.9113 - balanced_acc: 0.9108 - val_loss: 1.0053 - val_accuracy: 0.6528 - val_balanced_acc: 0.4521 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2242 - accuracy: 0.9191 - balanced_acc: 0.9193\n",
            "Epoch 7: val_balanced_acc improved from 0.61667 to 0.62553, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 149s 678ms/step - loss: 0.2242 - accuracy: 0.9191 - balanced_acc: 0.9193 - val_loss: 0.8354 - val_accuracy: 0.7306 - val_balanced_acc: 0.6255 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9362 - balanced_acc: 0.9355\n",
            "Epoch 8: val_balanced_acc did not improve from 0.62553\n",
            "219/219 [==============================] - 147s 672ms/step - loss: 0.1821 - accuracy: 0.9362 - balanced_acc: 0.9355 - val_loss: 0.6936 - val_accuracy: 0.7513 - val_balanced_acc: 0.4775 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1535 - accuracy: 0.9487 - balanced_acc: 0.9484\n",
            "Epoch 9: val_balanced_acc improved from 0.62553 to 0.64111, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 149s 679ms/step - loss: 0.1535 - accuracy: 0.9487 - balanced_acc: 0.9484 - val_loss: 0.7477 - val_accuracy: 0.7565 - val_balanced_acc: 0.6411 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1261 - accuracy: 0.9565 - balanced_acc: 0.9562\n",
            "Epoch 10: val_balanced_acc did not improve from 0.64111\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 0.1261 - accuracy: 0.9565 - balanced_acc: 0.9562 - val_loss: 0.6923 - val_accuracy: 0.7720 - val_balanced_acc: 0.6333 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9580 - balanced_acc: 0.9573\n",
            "Epoch 11: val_balanced_acc did not improve from 0.64111\n",
            "219/219 [==============================] - 147s 671ms/step - loss: 0.1337 - accuracy: 0.9580 - balanced_acc: 0.9573 - val_loss: 0.6958 - val_accuracy: 0.7876 - val_balanced_acc: 0.6038 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9759 - balanced_acc: 0.9764\n",
            "Epoch 12: val_balanced_acc improved from 0.64111 to 0.64517, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 149s 680ms/step - loss: 0.0810 - accuracy: 0.9759 - balanced_acc: 0.9764 - val_loss: 0.7942 - val_accuracy: 0.7668 - val_balanced_acc: 0.6452 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9680 - balanced_acc: 0.9685\n",
            "Epoch 13: val_balanced_acc did not improve from 0.64517\n",
            "219/219 [==============================] - 147s 674ms/step - loss: 0.1099 - accuracy: 0.9680 - balanced_acc: 0.9685 - val_loss: 0.6289 - val_accuracy: 0.8031 - val_balanced_acc: 0.6154 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9723 - balanced_acc: 0.9711\n",
            "Epoch 14: val_balanced_acc did not improve from 0.64517\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 0.0938 - accuracy: 0.9723 - balanced_acc: 0.9711 - val_loss: 0.7500 - val_accuracy: 0.7720 - val_balanced_acc: 0.6235 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9809 - balanced_acc: 0.9806\n",
            "Epoch 15: val_balanced_acc did not improve from 0.64517\n",
            "219/219 [==============================] - 147s 671ms/step - loss: 0.0724 - accuracy: 0.9809 - balanced_acc: 0.9806 - val_loss: 0.7496 - val_accuracy: 0.7927 - val_balanced_acc: 0.5936 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9907 - balanced_acc: 0.9909\n",
            "Epoch 16: val_balanced_acc did not improve from 0.64517\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 0.0390 - accuracy: 0.9907 - balanced_acc: 0.9909 - val_loss: 0.6785 - val_accuracy: 0.8031 - val_balanced_acc: 0.6420 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9969 - balanced_acc: 0.9970\n",
            "Epoch 17: val_balanced_acc did not improve from 0.64517\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 0.0184 - accuracy: 0.9969 - balanced_acc: 0.9970 - val_loss: 0.8748 - val_accuracy: 0.7824 - val_balanced_acc: 0.6353 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9974 - balanced_acc: 0.9973\n",
            "Epoch 18: val_balanced_acc did not improve from 0.64517\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 0.0142 - accuracy: 0.9974 - balanced_acc: 0.9973 - val_loss: 0.7184 - val_accuracy: 0.8238 - val_balanced_acc: 0.6264 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9994 - balanced_acc: 0.9994\n",
            "Epoch 19: val_balanced_acc did not improve from 0.64517\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 0.0083 - accuracy: 0.9994 - balanced_acc: 0.9994 - val_loss: 0.9266 - val_accuracy: 0.7824 - val_balanced_acc: 0.5991 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 0.9999 - balanced_acc: 0.9999\n",
            "Epoch 20: val_balanced_acc did not improve from 0.64517\n",
            "219/219 [==============================] - 148s 674ms/step - loss: 0.0049 - accuracy: 0.9999 - balanced_acc: 0.9999 - val_loss: 1.0569 - val_accuracy: 0.7824 - val_balanced_acc: 0.6008 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 21: val_balanced_acc improved from 0.64517 to 0.65232, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 149s 679ms/step - loss: 0.0035 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 0.9149 - val_accuracy: 0.8135 - val_balanced_acc: 0.6523 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 22: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 0.0028 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 0.8431 - val_accuracy: 0.8342 - val_balanced_acc: 0.6066 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 23: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 148s 674ms/step - loss: 0.0023 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 0.9172 - val_accuracy: 0.8135 - val_balanced_acc: 0.6034 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 24: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 0.0018 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1041 - val_accuracy: 0.7824 - val_balanced_acc: 0.5988 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 25: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 148s 674ms/step - loss: 0.0015 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 0.9880 - val_accuracy: 0.8083 - val_balanced_acc: 0.6068 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 26: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 0.0013 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.0362 - val_accuracy: 0.8083 - val_balanced_acc: 0.6147 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 27: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 0.0012 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.0420 - val_accuracy: 0.8031 - val_balanced_acc: 0.6105 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 28: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 148s 674ms/step - loss: 0.0011 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.0470 - val_accuracy: 0.8083 - val_balanced_acc: 0.6066 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 9.6597e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 29: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 672ms/step - loss: 9.6597e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.0744 - val_accuracy: 0.8083 - val_balanced_acc: 0.6068 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 8.6362e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 30: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 148s 674ms/step - loss: 8.6362e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.0822 - val_accuracy: 0.8083 - val_balanced_acc: 0.6068 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 8.0957e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 31: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 8.0957e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.0869 - val_accuracy: 0.8135 - val_balanced_acc: 0.6155 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 7.2056e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 32: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 148s 674ms/step - loss: 7.2056e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1019 - val_accuracy: 0.8083 - val_balanced_acc: 0.6068 - lr: 5.0000e-04\n",
            "Epoch 33/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 6.8844e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 33: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 148s 675ms/step - loss: 6.8844e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1031 - val_accuracy: 0.8135 - val_balanced_acc: 0.6077 - lr: 5.0000e-04\n",
            "Epoch 34/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 6.7464e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 34: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 6.7464e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1297 - val_accuracy: 0.8031 - val_balanced_acc: 0.6057 - lr: 5.0000e-04\n",
            "Epoch 35/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 6.1984e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 35: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 6.1984e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1155 - val_accuracy: 0.8135 - val_balanced_acc: 0.6077 - lr: 5.0000e-04\n",
            "Epoch 36/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 6.3823e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 36: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 148s 674ms/step - loss: 6.3823e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1395 - val_accuracy: 0.8031 - val_balanced_acc: 0.6057 - lr: 5.0000e-04\n",
            "Epoch 37/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 6.0286e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 37: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 148s 674ms/step - loss: 6.0286e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1286 - val_accuracy: 0.8083 - val_balanced_acc: 0.6068 - lr: 5.0000e-04\n",
            "Epoch 38/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 5.8486e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 38: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 5.8486e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1286 - val_accuracy: 0.8083 - val_balanced_acc: 0.6066 - lr: 5.0000e-04\n",
            "Epoch 39/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 5.6110e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 39: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 5.6110e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1530 - val_accuracy: 0.8031 - val_balanced_acc: 0.6057 - lr: 5.0000e-04\n",
            "Epoch 40/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 5.4558e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 40: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 5.4558e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1323 - val_accuracy: 0.8135 - val_balanced_acc: 0.6077 - lr: 5.0000e-04\n",
            "Epoch 41/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 5.3577e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 41: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 148s 674ms/step - loss: 5.3577e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1641 - val_accuracy: 0.8031 - val_balanced_acc: 0.6057 - lr: 5.0000e-04\n",
            "Epoch 42/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 5.1148e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 42: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 5.1148e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1576 - val_accuracy: 0.8031 - val_balanced_acc: 0.6057 - lr: 2.5000e-04\n",
            "Epoch 43/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 4.9678e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 43: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 4.9678e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1628 - val_accuracy: 0.7979 - val_balanced_acc: 0.6006 - lr: 2.5000e-04\n",
            "Epoch 44/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 5.0453e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 44: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 672ms/step - loss: 5.0453e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1540 - val_accuracy: 0.8135 - val_balanced_acc: 0.6077 - lr: 2.5000e-04\n",
            "Epoch 45/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 4.8684e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 45: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 4.8684e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1636 - val_accuracy: 0.8031 - val_balanced_acc: 0.6057 - lr: 2.5000e-04\n",
            "Epoch 46/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 4.8591e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 46: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 672ms/step - loss: 4.8591e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1614 - val_accuracy: 0.8083 - val_balanced_acc: 0.6129 - lr: 2.5000e-04\n",
            "Epoch 47/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 4.8556e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 47: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 4.8556e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1648 - val_accuracy: 0.8031 - val_balanced_acc: 0.6057 - lr: 2.5000e-04\n",
            "Epoch 48/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 4.6432e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 48: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 4.6432e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1690 - val_accuracy: 0.8083 - val_balanced_acc: 0.6129 - lr: 2.5000e-04\n",
            "Epoch 49/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 4.6470e-04 - accuracy: 1.0000 - balanced_acc: 0.9993\n",
            "Epoch 49: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 672ms/step - loss: 4.6470e-04 - accuracy: 1.0000 - balanced_acc: 0.9993 - val_loss: 1.1561 - val_accuracy: 0.8135 - val_balanced_acc: 0.6137 - lr: 2.5000e-04\n",
            "Epoch 50/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 4.5073e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 50: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 4.5073e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1795 - val_accuracy: 0.8083 - val_balanced_acc: 0.6129 - lr: 2.5000e-04\n",
            "Epoch 51/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 4.6926e-04 - accuracy: 1.0000 - balanced_acc: 1.0000\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 51: val_balanced_acc did not improve from 0.65232\n",
            "219/219 [==============================] - 147s 673ms/step - loss: 4.6926e-04 - accuracy: 1.0000 - balanced_acc: 1.0000 - val_loss: 1.1789 - val_accuracy: 0.8135 - val_balanced_acc: 0.6137 - lr: 2.5000e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# define model\n",
        "model = define_model_resnet()\n",
        "#model.summary()\n",
        "hst = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n",
        "# learning curves\n",
        "summarize_diagnostics(hst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vXnW3lmCgln3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "bf239057-727c-43c1-cab7-40641a67caaf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icgjmi-4UIT-"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SPz8NH1Oylv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6397786-13ca-4c9f-efcf-28e191f9533e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model saved\n"
          ]
        }
      ],
      "source": [
        "#save last model\n",
        "model.save(last_model_fpath)\n",
        "print(\"model saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS3ewyxO_anU",
        "outputId": "2244df62-0123-4f71-d5ba-219278d20c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.9835192157419905\n",
            "balanced accuracy on training 0.9835192157419905\n",
            "accuracy on validation 0.8134715025906736\n",
            "balanced accuracy on validation 0.6951230824924204\n",
            "Score on val data:  (0.6094110323174363, 0.6951230824924204, 0.6423153120897481, None)\n"
          ]
        }
      ],
      "source": [
        "last_model = load_model(last_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = last_model.predict(X_train)\n",
        "y_val_pred = last_model.predict(X_val)\n",
        "\n",
        "#print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W3IyWjdGG4Xq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da0ba3c-7c9a-4e8d-e0d5-253ac6330e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.9850820487319741\n",
            "balanced accuracy on training 0.9850820487319741\n",
            "accuracy on validation 0.8134715025906736\n",
            "balanced accuracy on validation 0.8170927794621176\n",
            "Score on val data:  (0.6917431972789115, 0.8170927794621176, 0.7276186980170946, None)\n"
          ]
        }
      ],
      "source": [
        "best_model = load_model(best_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRWiTnO0MGh"
      },
      "source": [
        "#Cut-off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGnCoIdLyDHS"
      },
      "outputs": [],
      "source": [
        "df_val_pred = pd.DataFrame(y_val_pred, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdyCbloQyWTC"
      },
      "outputs": [],
      "source": [
        "numbers = [float(x)/40 for x in range(11)]\n",
        "for i in numbers:\n",
        "    df_val_pred[i]= df_val_pred.MEL.map(lambda x: 1 if x > i else 0)\n",
        "df_val_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4SQsRx73kgk"
      },
      "outputs": [],
      "source": [
        "y_val_true= [1 if x == 4 else 0 for x in np.argmax(y_val, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcUISWFi0J05"
      },
      "outputs": [],
      "source": [
        "#num = [0.0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
        "cutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificity'])\n",
        "for i in numbers:\n",
        "    cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "    total1=sum(sum(cm1))\n",
        "    Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "    Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "    Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "    cutoff_df.loc[i] =[ i ,Accuracy,Sensitivity,Specificity]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W31LSzov1tCt"
      },
      "outputs": [],
      "source": [
        "cutoff_df[['Accuracy','Sensitivity','Specificity']].plot()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6CIKT94Jqye"
      },
      "outputs": [],
      "source": [
        "i = 0.025\n",
        "cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "total1=sum(sum(cm1))\n",
        "Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U2tkFebL_VC"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ', Accuracy)\n",
        "print('Sensitivity: ', Sensitivity)\n",
        "print('Specificity: ', Specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK4zbtoaAaC"
      },
      "source": [
        "#Confusion Metric on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkPOFLehOmFg"
      },
      "outputs": [],
      "source": [
        "#change melanoma flag back to 4\n",
        "df_val_pred[df_val_pred[i] == 1] = 4\n",
        "#decode one-hot y_val_pred while use cut-off melanoma data\n",
        "condition = df_val_pred[i] == 4\n",
        "y_val_pred2 = np.where(condition, df_val_pred[i], np.argmax(y_val_pred, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOVl6dWlTDLo"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ',accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))\n",
        "print('Balanced accuracy: ',balanced_accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqvYutTKRhR_"
      },
      "outputs": [],
      "source": [
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(np.argmax(y_val, axis=1), y_val_pred2)\n",
        "print(cf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtvW3YeaLlC"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(cf_matrix / cf_matrix.sum(axis=1, keepdims=True), annot=True, \n",
        "            cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix \\n');\n",
        "ax.set_xlabel('\\nPredicted')\n",
        "ax.set_ylabel('Actual ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "ax.yaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,3)\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-1yjWGeKs7"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "#labels_count = df_val['Labels'].value_counts().sort_index()\n",
        "\n",
        "#f = plt.figure(figsize=(15, 6))\n",
        "#s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "#s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K908bbiYwbS"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_pickle(path+\"isic2018_test.pkl\")\n",
        "X_train = df_test.loc[:, df_test.columns != 'y_train'].to_numpy()\n",
        "X_train = X_train.reshape(-1,224,224,3)"
      ],
      "metadata": {
        "id": "cN98sOWPyT3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMY2yvMYxsC"
      },
      "outputs": [],
      "source": [
        "#dir_test = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Test_Input/'\n",
        "#filepaths = sorted( filter( lambda x: (os.path.isfile(os.path.join(dir_test, x))) and (x.endswith('.jpg')),\n",
        "#                        os.listdir(dir_test) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ic95mefkpG3"
      },
      "outputs": [],
      "source": [
        "#df_test = pd.DataFrame(filepaths, columns =['image'])\n",
        "#df_test['FilePaths'] = dir_test + df_test['image']\n",
        "#df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBa1TxPuY8ni"
      },
      "outputs": [],
      "source": [
        "#df_test['image_px'] = df_test['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60LYAT7VsNOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33bd32fe-fac9-4d07-87a9-2cce7336b6dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1512, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "#X_test = np.asarray(df_test['image_px'].tolist())\n",
        "#print(np.array(X_test).shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df3 = pd.DataFrame(X_test.reshape(X_test.shape[0],-1))\n",
        "#df3.to_pickle(path+\"isic2018_test.pkl\")"
      ],
      "metadata": {
        "id": "JC17MArBxhSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXnnIIwC4cHE"
      },
      "outputs": [],
      "source": [
        "#preprocess\n",
        "#X_test = preprocess_image_input(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7ml90JZ8FK"
      },
      "source": [
        "Calculate y_pred from training and testing for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeDTXdaMLmyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ea64da-79f0-424f-f913-5b0bd6c66170"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1512, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "X_test = model1.predict(X_test)\n",
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIX0AmEFNv3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b58601a-b55c-4ef2-b817-f788416ed73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y_pred2 (1512, 7)\n"
          ]
        }
      ],
      "source": [
        "# predicting\n",
        "#CHANGE THE MODEL IF NECESSARY\n",
        "Y_test2 = model.predict(X_test)\n",
        "#Y_pred2 = model2.predict(X_test)\n",
        "print(\"Y_pred2\", Y_pred2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oeArO5CtxGb"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame(Y_pred2, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "df_pred['image'] = df_test['FilePaths'].map(lambda x: x.replace(dir_test, '').replace('.jpg', ''))\n",
        "df_pred = df_pred[['image', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']]\n",
        "df_pred.set_index(\"image\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ynyd8PjT589"
      },
      "outputs": [],
      "source": [
        "#update MEL data using cut-off value\n",
        "df_pred.MEL[df_pred.MEL > i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjRdONoQVMq0"
      },
      "outputs": [],
      "source": [
        "df_pred.loc[df_pred.MEL > i, ['NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']] = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "K4Iv_3s4z0R9",
        "outputId": "a7dca29b-4c36-4693-d619-9bd440e6a18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   MEL        NV       BCC     AKIEC       BKL        DF  \\\n",
              "image                                                                      \n",
              "ISIC_0034524  0.002193  0.930769  0.003427  0.000460  0.036592  0.004333   \n",
              "ISIC_0034525  0.122186  0.828252  0.002661  0.010148  0.009148  0.021819   \n",
              "ISIC_0034526  0.020197  0.003637  0.000938  0.028031  0.947131  0.000066   \n",
              "ISIC_0034527  0.131679  0.808747  0.000006  0.000279  0.059272  0.000017   \n",
              "ISIC_0034528  0.005777  0.840918  0.000002  0.000037  0.151937  0.001316   \n",
              "\n",
              "                      VASC  \n",
              "image                       \n",
              "ISIC_0034524  2.222793e-02  \n",
              "ISIC_0034525  5.786379e-03  \n",
              "ISIC_0034526  1.199605e-08  \n",
              "ISIC_0034527  6.856867e-08  \n",
              "ISIC_0034528  1.348875e-05  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5268f553-0d89-461e-9c87-fc1271d7e6d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MEL</th>\n",
              "      <th>NV</th>\n",
              "      <th>BCC</th>\n",
              "      <th>AKIEC</th>\n",
              "      <th>BKL</th>\n",
              "      <th>DF</th>\n",
              "      <th>VASC</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ISIC_0034524</th>\n",
              "      <td>0.002193</td>\n",
              "      <td>0.930769</td>\n",
              "      <td>0.003427</td>\n",
              "      <td>0.000460</td>\n",
              "      <td>0.036592</td>\n",
              "      <td>0.004333</td>\n",
              "      <td>2.222793e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ISIC_0034525</th>\n",
              "      <td>0.122186</td>\n",
              "      <td>0.828252</td>\n",
              "      <td>0.002661</td>\n",
              "      <td>0.010148</td>\n",
              "      <td>0.009148</td>\n",
              "      <td>0.021819</td>\n",
              "      <td>5.786379e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ISIC_0034526</th>\n",
              "      <td>0.020197</td>\n",
              "      <td>0.003637</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.028031</td>\n",
              "      <td>0.947131</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>1.199605e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ISIC_0034527</th>\n",
              "      <td>0.131679</td>\n",
              "      <td>0.808747</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.059272</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>6.856867e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ISIC_0034528</th>\n",
              "      <td>0.005777</td>\n",
              "      <td>0.840918</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.151937</td>\n",
              "      <td>0.001316</td>\n",
              "      <td>1.348875e-05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5268f553-0d89-461e-9c87-fc1271d7e6d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5268f553-0d89-461e-9c87-fc1271d7e6d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5268f553-0d89-461e-9c87-fc1271d7e6d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOnjc3RJ0e4T"
      },
      "outputs": [],
      "source": [
        "df_pred.to_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/response_Borderline-SMOTE.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0MghVs0tsGw"
      },
      "source": [
        "result: 0.656"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UswA0co2y1wl"
      },
      "source": [
        "#Exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnqJYIONy34l"
      },
      "outputs": [],
      "source": [
        "input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "base_model = ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model(input_tensor, training=False)\n",
        "x = Attention(2048,2048,7,8)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "res50 = Model(inputs=input_tensor, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcn8hQg3J8yP"
      },
      "outputs": [],
      "source": [
        "#Train i-last layer\n",
        "# summarize feature map shapes\n",
        "for i in range(len(model.layers)):\n",
        "    layer = model.layers[i]\n",
        "    # summarize output shape\n",
        "    print(i, layer.name, layer.output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA7Af2Y73FUv"
      },
      "outputs": [],
      "source": [
        "X_train = res50.predict(X_train)\n",
        "X_val = res50.predict(X_val)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krJiAb1m3QNf"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = SMOTE_Data2(X_train, y_train, True)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfcFpsBwM0d4"
      },
      "source": [
        "#Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_s6OIGKM26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "371bd24a-4bf9-491a-e0ec-78b7eb06e6d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ff488085aaa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_current_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_preprocessing_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessingLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Image preprocessing layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pandas._testing import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m \u001b[0mcython_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'core'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as tk\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,AveragePooling2D,BatchNormalization,Add,ZeroPadding2D,Flatten,Dense,Input,LeakyReLU,Softmax,ReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class Attention(tk.layers.Layer):\n",
        "    \n",
        "    def __init__(self,input_channels,output_channel,kernel_size,groups):\n",
        "        super().__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channel = output_channel    \n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = 1\n",
        "        self.groups = groups\n",
        "\n",
        "        assert output_channel % groups == 0\n",
        "        \n",
        "        self.rel_h = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,kernel_size,1,output_channel//2),stddev = 0.1)) \n",
        "        #output_channels//2 is the number of channels on which the relative position will be considered,1 denotes the number of those filters and the one after that too and (kernel_size,1) denotes the size of that filter\n",
        "        self.rel_w = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,1,kernel_size,output_channel//2),stddev = 0.1)) \n",
        "\n",
        "        self.key_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.query_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.value_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "\n",
        "    def call(self,x):\n",
        "        \n",
        "        batch,height,width,channels = x.shape\n",
        "        x_padded = ZeroPadding2D(padding=(self.kernel_size//2,self.kernel_size//2))(x)\n",
        "        query = self.query_weights(x)\n",
        "        value = self.value_weights(x_padded)\n",
        "        key = self.key_weights(x_padded)\n",
        "        #key,query and value will have the shape of (batch,height,width,depth)\n",
        "        keys = tf.image.extract_patches(images = key,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        value = tf.image.extract_patches(images = value,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        no_of_kernels = key.shape[-2] - self.kernel_size + 1\n",
        "        keys = tf.reshape(keys,shape = (-1,no_of_kernels, no_of_kernels,self.kernel_size,self.kernel_size,self.output_channel))\n",
        "        key_split_h,key_split_w = tf.split(keys,num_or_size_splits = 2,axis = -1)\n",
        "        key_with_rel = tk.layers.concatenate([key_split_h + self.rel_h,key_split_w + self.rel_w],axis = -1) \n",
        "        \n",
        "        #reshaping the query and key\n",
        "        key_with_rel = tf.reshape(key_with_rel,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        query  = tf.reshape(query,(-1,self.groups,no_of_kernels,no_of_kernels,1,self.output_channel//self.groups))        \n",
        "        value = tf.reshape(value,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        \n",
        "        #multiplication  of key and query\n",
        "        #assert key_with_rel.shape == query.shape        \n",
        "        key_prod_query = query*key_with_rel\n",
        "        \n",
        "        # Now the function is passed through the softmax and is multiplied with the values\n",
        "        s = Softmax(axis = -2)(key_prod_query)\n",
        "        y = tf.einsum('bnchwk,bnchwk->bnchk',s,value)\n",
        "        y = tf.reshape(y,(-1,height,width,self.output_channel))\n",
        "        return y\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            \"input_channels\": self.input_channels, \n",
        "            \"output_channel\": self.output_channel, \n",
        "            \"kernel_size\": self.kernel_size, \n",
        "            \"stride\": self.stride, \n",
        "            \"groups\": self.groups, \n",
        "            \"rel_h\": self.rel_h, \n",
        "            \"rel_w\": self.rel_w, \n",
        "            \"key_weights\": self.key_weights, \n",
        "            \"query_weights\": self.query_weights, \n",
        "            \"value_weights\": self.value_weights\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE8Ziq-BlEP4"
      },
      "source": [
        "#Oversampling on feature map level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Lm05Zet_B5am",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f18f78e-6e90-4b5d-91b6-944860b5f616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_4 [(None, 32, 32, 3)] True\n",
            "1 up_sampling2d_3 (None, 224, 224, 3) False\n",
            "2 conv1_pad (None, 230, 230, 3) False\n",
            "3 conv1_conv (None, 112, 112, 64) False\n",
            "4 conv1_bn (None, 112, 112, 64) False\n",
            "5 conv1_relu (None, 112, 112, 64) False\n",
            "6 pool1_pad (None, 114, 114, 64) False\n",
            "7 pool1_pool (None, 56, 56, 64) False\n",
            "8 conv2_block1_1_conv (None, 56, 56, 64) False\n",
            "9 conv2_block1_1_bn (None, 56, 56, 64) False\n",
            "10 conv2_block1_1_relu (None, 56, 56, 64) False\n",
            "11 conv2_block1_2_conv (None, 56, 56, 64) False\n",
            "12 conv2_block1_2_bn (None, 56, 56, 64) False\n",
            "13 conv2_block1_2_relu (None, 56, 56, 64) False\n",
            "14 conv2_block1_0_conv (None, 56, 56, 256) False\n",
            "15 conv2_block1_3_conv (None, 56, 56, 256) False\n",
            "16 conv2_block1_0_bn (None, 56, 56, 256) False\n",
            "17 conv2_block1_3_bn (None, 56, 56, 256) False\n",
            "18 conv2_block1_add (None, 56, 56, 256) False\n",
            "19 conv2_block1_out (None, 56, 56, 256) False\n",
            "20 conv2_block2_1_conv (None, 56, 56, 64) False\n",
            "21 conv2_block2_1_bn (None, 56, 56, 64) False\n",
            "22 conv2_block2_1_relu (None, 56, 56, 64) False\n",
            "23 conv2_block2_2_conv (None, 56, 56, 64) False\n",
            "24 conv2_block2_2_bn (None, 56, 56, 64) False\n",
            "25 conv2_block2_2_relu (None, 56, 56, 64) False\n",
            "26 conv2_block2_3_conv (None, 56, 56, 256) False\n",
            "27 conv2_block2_3_bn (None, 56, 56, 256) False\n",
            "28 conv2_block2_add (None, 56, 56, 256) False\n",
            "29 conv2_block2_out (None, 56, 56, 256) False\n",
            "30 conv2_block3_1_conv (None, 56, 56, 64) False\n",
            "31 conv2_block3_1_bn (None, 56, 56, 64) False\n",
            "32 conv2_block3_1_relu (None, 56, 56, 64) False\n",
            "33 conv2_block3_2_conv (None, 56, 56, 64) False\n",
            "34 conv2_block3_2_bn (None, 56, 56, 64) False\n",
            "35 conv2_block3_2_relu (None, 56, 56, 64) False\n",
            "36 conv2_block3_3_conv (None, 56, 56, 256) False\n",
            "37 conv2_block3_3_bn (None, 56, 56, 256) False\n",
            "38 conv2_block3_add (None, 56, 56, 256) False\n",
            "39 conv2_block3_out (None, 56, 56, 256) False\n",
            "40 conv3_block1_1_conv (None, 28, 28, 128) False\n",
            "41 conv3_block1_1_bn (None, 28, 28, 128) False\n",
            "42 conv3_block1_1_relu (None, 28, 28, 128) False\n",
            "43 conv3_block1_2_conv (None, 28, 28, 128) False\n",
            "44 conv3_block1_2_bn (None, 28, 28, 128) False\n",
            "45 conv3_block1_2_relu (None, 28, 28, 128) False\n",
            "46 conv3_block1_0_conv (None, 28, 28, 512) False\n",
            "47 conv3_block1_3_conv (None, 28, 28, 512) False\n",
            "48 conv3_block1_0_bn (None, 28, 28, 512) False\n",
            "49 conv3_block1_3_bn (None, 28, 28, 512) False\n",
            "50 conv3_block1_add (None, 28, 28, 512) False\n",
            "51 conv3_block1_out (None, 28, 28, 512) False\n",
            "52 conv3_block2_1_conv (None, 28, 28, 128) False\n",
            "53 conv3_block2_1_bn (None, 28, 28, 128) False\n",
            "54 conv3_block2_1_relu (None, 28, 28, 128) False\n",
            "55 conv3_block2_2_conv (None, 28, 28, 128) False\n",
            "56 conv3_block2_2_bn (None, 28, 28, 128) False\n",
            "57 conv3_block2_2_relu (None, 28, 28, 128) False\n",
            "58 conv3_block2_3_conv (None, 28, 28, 512) False\n",
            "59 conv3_block2_3_bn (None, 28, 28, 512) False\n",
            "60 conv3_block2_add (None, 28, 28, 512) False\n",
            "61 conv3_block2_out (None, 28, 28, 512) False\n",
            "62 conv3_block3_1_conv (None, 28, 28, 128) False\n",
            "63 conv3_block3_1_bn (None, 28, 28, 128) False\n",
            "64 conv3_block3_1_relu (None, 28, 28, 128) False\n",
            "65 conv3_block3_2_conv (None, 28, 28, 128) False\n",
            "66 conv3_block3_2_bn (None, 28, 28, 128) False\n",
            "67 conv3_block3_2_relu (None, 28, 28, 128) False\n",
            "68 conv3_block3_3_conv (None, 28, 28, 512) False\n",
            "69 conv3_block3_3_bn (None, 28, 28, 512) False\n",
            "70 conv3_block3_add (None, 28, 28, 512) False\n",
            "71 conv3_block3_out (None, 28, 28, 512) False\n",
            "72 conv3_block4_1_conv (None, 28, 28, 128) False\n",
            "73 conv3_block4_1_bn (None, 28, 28, 128) False\n",
            "74 conv3_block4_1_relu (None, 28, 28, 128) False\n",
            "75 conv3_block4_2_conv (None, 28, 28, 128) False\n",
            "76 conv3_block4_2_bn (None, 28, 28, 128) False\n",
            "77 conv3_block4_2_relu (None, 28, 28, 128) False\n",
            "78 conv3_block4_3_conv (None, 28, 28, 512) False\n",
            "79 conv3_block4_3_bn (None, 28, 28, 512) False\n",
            "80 conv3_block4_add (None, 28, 28, 512) False\n",
            "81 conv3_block4_out (None, 28, 28, 512) False\n",
            "82 conv4_block1_1_conv (None, 14, 14, 256) False\n",
            "83 conv4_block1_1_bn (None, 14, 14, 256) False\n",
            "84 conv4_block1_1_relu (None, 14, 14, 256) False\n",
            "85 conv4_block1_2_conv (None, 14, 14, 256) False\n",
            "86 conv4_block1_2_bn (None, 14, 14, 256) False\n",
            "87 conv4_block1_2_relu (None, 14, 14, 256) False\n",
            "88 conv4_block1_0_conv (None, 14, 14, 1024) False\n",
            "89 conv4_block1_3_conv (None, 14, 14, 1024) False\n",
            "90 conv4_block1_0_bn (None, 14, 14, 1024) False\n",
            "91 conv4_block1_3_bn (None, 14, 14, 1024) False\n",
            "92 conv4_block1_add (None, 14, 14, 1024) False\n",
            "93 conv4_block1_out (None, 14, 14, 1024) False\n",
            "94 conv4_block2_1_conv (None, 14, 14, 256) False\n",
            "95 conv4_block2_1_bn (None, 14, 14, 256) False\n",
            "96 conv4_block2_1_relu (None, 14, 14, 256) False\n",
            "97 conv4_block2_2_conv (None, 14, 14, 256) False\n",
            "98 conv4_block2_2_bn (None, 14, 14, 256) False\n",
            "99 conv4_block2_2_relu (None, 14, 14, 256) False\n",
            "100 conv4_block2_3_conv (None, 14, 14, 1024) False\n",
            "101 conv4_block2_3_bn (None, 14, 14, 1024) False\n",
            "102 conv4_block2_add (None, 14, 14, 1024) False\n",
            "103 conv4_block2_out (None, 14, 14, 1024) False\n",
            "104 conv4_block3_1_conv (None, 14, 14, 256) False\n",
            "105 conv4_block3_1_bn (None, 14, 14, 256) False\n",
            "106 conv4_block3_1_relu (None, 14, 14, 256) False\n",
            "107 conv4_block3_2_conv (None, 14, 14, 256) False\n",
            "108 conv4_block3_2_bn (None, 14, 14, 256) False\n",
            "109 conv4_block3_2_relu (None, 14, 14, 256) False\n",
            "110 conv4_block3_3_conv (None, 14, 14, 1024) False\n",
            "111 conv4_block3_3_bn (None, 14, 14, 1024) False\n",
            "112 conv4_block3_add (None, 14, 14, 1024) False\n",
            "113 conv4_block3_out (None, 14, 14, 1024) False\n",
            "114 conv4_block4_1_conv (None, 14, 14, 256) False\n",
            "115 conv4_block4_1_bn (None, 14, 14, 256) False\n",
            "116 conv4_block4_1_relu (None, 14, 14, 256) False\n",
            "117 conv4_block4_2_conv (None, 14, 14, 256) False\n",
            "118 conv4_block4_2_bn (None, 14, 14, 256) False\n",
            "119 conv4_block4_2_relu (None, 14, 14, 256) False\n",
            "120 conv4_block4_3_conv (None, 14, 14, 1024) False\n",
            "121 conv4_block4_3_bn (None, 14, 14, 1024) False\n",
            "122 conv4_block4_add (None, 14, 14, 1024) False\n",
            "123 conv4_block4_out (None, 14, 14, 1024) False\n",
            "124 conv4_block5_1_conv (None, 14, 14, 256) False\n",
            "125 conv4_block5_1_bn (None, 14, 14, 256) False\n",
            "126 conv4_block5_1_relu (None, 14, 14, 256) False\n",
            "127 conv4_block5_2_conv (None, 14, 14, 256) False\n",
            "128 conv4_block5_2_bn (None, 14, 14, 256) False\n",
            "129 conv4_block5_2_relu (None, 14, 14, 256) False\n",
            "130 conv4_block5_3_conv (None, 14, 14, 1024) False\n",
            "131 conv4_block5_3_bn (None, 14, 14, 1024) False\n",
            "132 conv4_block5_add (None, 14, 14, 1024) False\n",
            "133 conv4_block5_out (None, 14, 14, 1024) False\n",
            "134 conv4_block6_1_conv (None, 14, 14, 256) False\n",
            "135 conv4_block6_1_bn (None, 14, 14, 256) False\n",
            "136 conv4_block6_1_relu (None, 14, 14, 256) False\n",
            "137 conv4_block6_2_conv (None, 14, 14, 256) False\n",
            "138 conv4_block6_2_bn (None, 14, 14, 256) False\n",
            "139 conv4_block6_2_relu (None, 14, 14, 256) False\n",
            "140 conv4_block6_3_conv (None, 14, 14, 1024) False\n",
            "141 conv4_block6_3_bn (None, 14, 14, 1024) False\n",
            "142 conv4_block6_add (None, 14, 14, 1024) False\n",
            "143 conv4_block6_out (None, 14, 14, 1024) False\n",
            "144 conv5_block1_1_conv (None, 7, 7, 512) False\n",
            "145 conv5_block1_1_bn (None, 7, 7, 512) False\n",
            "146 conv5_block1_1_relu (None, 7, 7, 512) False\n",
            "147 conv5_block1_2_conv (None, 7, 7, 512) False\n",
            "148 conv5_block1_2_bn (None, 7, 7, 512) False\n",
            "149 conv5_block1_2_relu (None, 7, 7, 512) False\n",
            "150 conv5_block1_0_conv (None, 7, 7, 2048) False\n",
            "151 conv5_block1_3_conv (None, 7, 7, 2048) False\n",
            "152 conv5_block1_0_bn (None, 7, 7, 2048) False\n",
            "153 conv5_block1_3_bn (None, 7, 7, 2048) False\n",
            "154 conv5_block1_add (None, 7, 7, 2048) False\n",
            "155 conv5_block1_out (None, 7, 7, 2048) False\n",
            "156 conv5_block2_1_conv (None, 7, 7, 512) False\n",
            "157 conv5_block2_1_bn (None, 7, 7, 512) False\n",
            "158 conv5_block2_1_relu (None, 7, 7, 512) False\n",
            "159 conv5_block2_2_conv (None, 7, 7, 512) False\n",
            "160 conv5_block2_2_bn (None, 7, 7, 512) False\n",
            "161 conv5_block2_2_relu (None, 7, 7, 512) False\n",
            "162 conv5_block2_3_conv (None, 7, 7, 2048) False\n",
            "163 conv5_block2_3_bn (None, 7, 7, 2048) False\n",
            "164 conv5_block2_add (None, 7, 7, 2048) False\n",
            "165 conv5_block2_out (None, 7, 7, 2048) False\n",
            "166 conv5_block3_1_conv (None, 7, 7, 512) False\n",
            "167 conv5_block3_1_bn (None, 7, 7, 512) False\n",
            "168 conv5_block3_1_relu (None, 7, 7, 512) False\n",
            "169 conv5_block3_2_conv (None, 7, 7, 512) False\n",
            "170 conv5_block3_2_bn (None, 7, 7, 512) False\n",
            "171 conv5_block3_2_relu (None, 7, 7, 512) False\n",
            "172 conv5_block3_3_conv (None, 7, 7, 2048) False\n",
            "173 conv5_block3_3_bn (None, 7, 7, 2048) False\n",
            "174 conv5_block3_add (None, 7, 7, 2048) False\n",
            "175 conv5_block3_out (None, 7, 7, 2048) False\n",
            "176 global_average_pooling2d_3 (None, 2048) True\n",
            "177 dense_9 (None, 1024) True\n",
            "178 dense_10 (None, 512) True\n",
            "179 dense_11 (None, 7) True\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(model.layers)):\n",
        "  layer = model.layers[i]\n",
        "  print(i, layer.name, layer.output_shape, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "KqeSic6NmLsR"
      },
      "outputs": [],
      "source": [
        "# redefine model to output right after the first hidden layer\n",
        "end = 176\n",
        "model1 = Model(inputs=model.inputs, outputs=model.layers[end].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ZVHYG9Rwm28i"
      },
      "outputs": [],
      "source": [
        "# get feature map for first hidden layer\n",
        "X_train_fm = model1.predict(X_train)\n",
        "X_val_fm = model1.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_ov=y_train_ov.reshape(-1,1)"
      ],
      "metadata": {
        "id": "Xx0OnnZPl7_t"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "19hK7aQNeAQo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "b8f396bc-3191-4617-bcf6-03ae9db86042"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-a6db11aa46d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_fm_ov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_ov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE_Data2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fm_ov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_ov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"borderline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fm_ov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_ov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_fm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-923dba00babe>\u001b[0m in \u001b[0;36mSMOTE_Data2\u001b[0;34m(X, y, one_hot, k, type)\u001b[0m\n\u001b[1;32m    180\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m   \u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \"\"\"\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;34m\"multilabel-sequences\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     ]:\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'unknown'"
          ]
        }
      ],
      "source": [
        "X_train_fm_ov, y_train_ov = SMOTE_Data2(X_train_fm_ov, y_train_ov, False, 5, type=\"borderline\")\n",
        "print(X_train_fm_ov.shape)\n",
        "print(y_train_ov.shape)\n",
        "print(X_val_fm.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train_ov, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5qP4iyYcnAYa"
      },
      "outputs": [],
      "source": [
        "model2 = Model(inputs=model.layers[end+1].input, outputs=model.layers[len(model.layers)-1].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Pzdjs0WbvDB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56efbf52-7ec8-46d8-f3b1-64dc0d7c35db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "207/219 [===========================>..] - ETA: 0s - loss: 0.6973 - accuracy: 0.7847 - balanced_acc: 0.7836\n",
            "Epoch 1: val_balanced_acc improved from -inf to 0.37572, saving model to /content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5\n",
            "219/219 [==============================] - 1s 5ms/step - loss: 0.6959 - accuracy: 0.7850 - balanced_acc: 0.7842 - val_loss: 0.8254 - val_accuracy: 0.7047 - val_balanced_acc: 0.3757 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.7956 - balanced_acc: 0.7942\n",
            "Epoch 2: val_balanced_acc improved from 0.37572 to 0.38198, saving model to /content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5\n",
            "219/219 [==============================] - 1s 5ms/step - loss: 0.6496 - accuracy: 0.7956 - balanced_acc: 0.7942 - val_loss: 0.7612 - val_accuracy: 0.7254 - val_balanced_acc: 0.3820 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "212/219 [============================>.] - ETA: 0s - loss: 0.6218 - accuracy: 0.8041 - balanced_acc: 0.8042\n",
            "Epoch 3: val_balanced_acc improved from 0.38198 to 0.41372, saving model to /content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.8053 - balanced_acc: 0.8052 - val_loss: 0.7932 - val_accuracy: 0.7150 - val_balanced_acc: 0.4137 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "215/219 [============================>.] - ETA: 0s - loss: 0.5914 - accuracy: 0.8144 - balanced_acc: 0.8142\n",
            "Epoch 4: val_balanced_acc did not improve from 0.41372\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.5913 - accuracy: 0.8145 - balanced_acc: 0.8144 - val_loss: 0.8079 - val_accuracy: 0.7098 - val_balanced_acc: 0.3867 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "212/219 [============================>.] - ETA: 0s - loss: 0.5685 - accuracy: 0.8212 - balanced_acc: 0.8195\n",
            "Epoch 5: val_balanced_acc did not improve from 0.41372\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.5692 - accuracy: 0.8206 - balanced_acc: 0.8199 - val_loss: 0.7986 - val_accuracy: 0.7047 - val_balanced_acc: 0.4116 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.5512 - accuracy: 0.8264 - balanced_acc: 0.8265\n",
            "Epoch 6: val_balanced_acc did not improve from 0.41372\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.5500 - accuracy: 0.8260 - balanced_acc: 0.8260 - val_loss: 0.7969 - val_accuracy: 0.7047 - val_balanced_acc: 0.4072 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "214/219 [============================>.] - ETA: 0s - loss: 0.5341 - accuracy: 0.8313 - balanced_acc: 0.8321\n",
            "Epoch 7: val_balanced_acc did not improve from 0.41372\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.5329 - accuracy: 0.8319 - balanced_acc: 0.8325 - val_loss: 0.7780 - val_accuracy: 0.7254 - val_balanced_acc: 0.3876 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.5172 - accuracy: 0.8382 - balanced_acc: 0.8368\n",
            "Epoch 8: val_balanced_acc improved from 0.41372 to 0.42867, saving model to /content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.5155 - accuracy: 0.8386 - balanced_acc: 0.8373 - val_loss: 0.7827 - val_accuracy: 0.7306 - val_balanced_acc: 0.4287 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "212/219 [============================>.] - ETA: 0s - loss: 0.5029 - accuracy: 0.8437 - balanced_acc: 0.8444\n",
            "Epoch 9: val_balanced_acc improved from 0.42867 to 0.44567, saving model to /content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.5023 - accuracy: 0.8440 - balanced_acc: 0.8449 - val_loss: 0.7690 - val_accuracy: 0.7254 - val_balanced_acc: 0.4457 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.4912 - accuracy: 0.8455 - balanced_acc: 0.8451\n",
            "Epoch 10: val_balanced_acc did not improve from 0.44567\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.4894 - accuracy: 0.8466 - balanced_acc: 0.8464 - val_loss: 0.7818 - val_accuracy: 0.7098 - val_balanced_acc: 0.4036 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.4762 - accuracy: 0.8518 - balanced_acc: 0.8519\n",
            "Epoch 11: val_balanced_acc did not improve from 0.44567\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.4761 - accuracy: 0.8514 - balanced_acc: 0.8515 - val_loss: 0.7326 - val_accuracy: 0.7409 - val_balanced_acc: 0.4188 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.4648 - accuracy: 0.8564 - balanced_acc: 0.8559\n",
            "Epoch 12: val_balanced_acc did not improve from 0.44567\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.4651 - accuracy: 0.8561 - balanced_acc: 0.8554 - val_loss: 0.7470 - val_accuracy: 0.7202 - val_balanced_acc: 0.4400 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.4527 - accuracy: 0.8623 - balanced_acc: 0.8639\n",
            "Epoch 13: val_balanced_acc did not improve from 0.44567\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.4530 - accuracy: 0.8618 - balanced_acc: 0.8633 - val_loss: 0.7834 - val_accuracy: 0.7047 - val_balanced_acc: 0.4376 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "213/219 [============================>.] - ETA: 0s - loss: 0.4437 - accuracy: 0.8629 - balanced_acc: 0.8610\n",
            "Epoch 14: val_balanced_acc did not improve from 0.44567\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.4440 - accuracy: 0.8627 - balanced_acc: 0.8610 - val_loss: 0.7693 - val_accuracy: 0.7306 - val_balanced_acc: 0.4243 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.4343 - accuracy: 0.8681 - balanced_acc: 0.8681\n",
            "Epoch 15: val_balanced_acc did not improve from 0.44567\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.4325 - accuracy: 0.8693 - balanced_acc: 0.8690 - val_loss: 0.7468 - val_accuracy: 0.7254 - val_balanced_acc: 0.4144 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "211/219 [===========================>..] - ETA: 0s - loss: 0.4201 - accuracy: 0.8713 - balanced_acc: 0.8701\n",
            "Epoch 16: val_balanced_acc improved from 0.44567 to 0.44867, saving model to /content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.4205 - accuracy: 0.8713 - balanced_acc: 0.8704 - val_loss: 0.7388 - val_accuracy: 0.7306 - val_balanced_acc: 0.4487 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.4144 - accuracy: 0.8747 - balanced_acc: 0.8751\n",
            "Epoch 17: val_balanced_acc did not improve from 0.44867\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.4150 - accuracy: 0.8745 - balanced_acc: 0.8743 - val_loss: 0.7454 - val_accuracy: 0.7047 - val_balanced_acc: 0.4345 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "211/219 [===========================>..] - ETA: 0s - loss: 0.4060 - accuracy: 0.8757 - balanced_acc: 0.8760\n",
            "Epoch 18: val_balanced_acc did not improve from 0.44867\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.4066 - accuracy: 0.8753 - balanced_acc: 0.8759 - val_loss: 0.7257 - val_accuracy: 0.7306 - val_balanced_acc: 0.4438 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "206/219 [===========================>..] - ETA: 0s - loss: 0.3968 - accuracy: 0.8801 - balanced_acc: 0.8786\n",
            "Epoch 19: val_balanced_acc improved from 0.44867 to 0.46028, saving model to /content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3969 - accuracy: 0.8805 - balanced_acc: 0.8791 - val_loss: 0.7586 - val_accuracy: 0.7358 - val_balanced_acc: 0.4603 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.3943 - accuracy: 0.8791 - balanced_acc: 0.8772\n",
            "Epoch 20: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3927 - accuracy: 0.8798 - balanced_acc: 0.8775 - val_loss: 0.7332 - val_accuracy: 0.7306 - val_balanced_acc: 0.4200 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.3807 - accuracy: 0.8858 - balanced_acc: 0.8860\n",
            "Epoch 21: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3794 - accuracy: 0.8865 - balanced_acc: 0.8867 - val_loss: 0.7196 - val_accuracy: 0.7202 - val_balanced_acc: 0.4352 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.3749 - accuracy: 0.8881 - balanced_acc: 0.8868\n",
            "Epoch 22: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3748 - accuracy: 0.8880 - balanced_acc: 0.8872 - val_loss: 0.7266 - val_accuracy: 0.7254 - val_balanced_acc: 0.4409 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "205/219 [===========================>..] - ETA: 0s - loss: 0.3732 - accuracy: 0.8864 - balanced_acc: 0.8886\n",
            "Epoch 23: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3711 - accuracy: 0.8882 - balanced_acc: 0.8902 - val_loss: 0.7075 - val_accuracy: 0.7202 - val_balanced_acc: 0.4256 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.3581 - accuracy: 0.8932 - balanced_acc: 0.8922\n",
            "Epoch 24: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3578 - accuracy: 0.8935 - balanced_acc: 0.8924 - val_loss: 0.7143 - val_accuracy: 0.7358 - val_balanced_acc: 0.4196 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "211/219 [===========================>..] - ETA: 0s - loss: 0.3554 - accuracy: 0.8933 - balanced_acc: 0.8942\n",
            "Epoch 25: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3562 - accuracy: 0.8929 - balanced_acc: 0.8940 - val_loss: 0.7219 - val_accuracy: 0.7098 - val_balanced_acc: 0.4264 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "217/219 [============================>.] - ETA: 0s - loss: 0.3468 - accuracy: 0.8965 - balanced_acc: 0.8970\n",
            "Epoch 26: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3467 - accuracy: 0.8966 - balanced_acc: 0.8970 - val_loss: 0.7223 - val_accuracy: 0.7358 - val_balanced_acc: 0.4189 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "214/219 [============================>.] - ETA: 0s - loss: 0.3433 - accuracy: 0.8972 - balanced_acc: 0.8979\n",
            "Epoch 27: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3427 - accuracy: 0.8972 - balanced_acc: 0.8978 - val_loss: 0.7190 - val_accuracy: 0.7306 - val_balanced_acc: 0.4153 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.3359 - accuracy: 0.9039 - balanced_acc: 0.9048\n",
            "Epoch 28: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3355 - accuracy: 0.9037 - balanced_acc: 0.9049 - val_loss: 0.7158 - val_accuracy: 0.7409 - val_balanced_acc: 0.4198 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.3289 - accuracy: 0.9045 - balanced_acc: 0.9045\n",
            "Epoch 29: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3298 - accuracy: 0.9042 - balanced_acc: 0.9045 - val_loss: 0.7206 - val_accuracy: 0.7409 - val_balanced_acc: 0.4578 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "211/219 [===========================>..] - ETA: 0s - loss: 0.3255 - accuracy: 0.9061 - balanced_acc: 0.9080\n",
            "Epoch 30: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3263 - accuracy: 0.9057 - balanced_acc: 0.9072 - val_loss: 0.6625 - val_accuracy: 0.7565 - val_balanced_acc: 0.4107 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "207/219 [===========================>..] - ETA: 0s - loss: 0.3210 - accuracy: 0.9074 - balanced_acc: 0.9092\n",
            "Epoch 31: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3211 - accuracy: 0.9067 - balanced_acc: 0.9080 - val_loss: 0.6877 - val_accuracy: 0.7513 - val_balanced_acc: 0.4529 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "212/219 [============================>.] - ETA: 0s - loss: 0.3134 - accuracy: 0.9098 - balanced_acc: 0.9115\n",
            "Epoch 32: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3133 - accuracy: 0.9097 - balanced_acc: 0.9113 - val_loss: 0.6998 - val_accuracy: 0.7513 - val_balanced_acc: 0.4278 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "206/219 [===========================>..] - ETA: 0s - loss: 0.3085 - accuracy: 0.9118 - balanced_acc: 0.9143\n",
            "Epoch 33: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3083 - accuracy: 0.9117 - balanced_acc: 0.9138 - val_loss: 0.6809 - val_accuracy: 0.7565 - val_balanced_acc: 0.4222 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "212/219 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.9137 - balanced_acc: 0.9130\n",
            "Epoch 34: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.3009 - accuracy: 0.9139 - balanced_acc: 0.9132 - val_loss: 0.7228 - val_accuracy: 0.7513 - val_balanced_acc: 0.4278 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2998 - accuracy: 0.9152 - balanced_acc: 0.9171\n",
            "Epoch 35: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2997 - accuracy: 0.9155 - balanced_acc: 0.9170 - val_loss: 0.7578 - val_accuracy: 0.7150 - val_balanced_acc: 0.4121 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "206/219 [===========================>..] - ETA: 0s - loss: 0.2968 - accuracy: 0.9178 - balanced_acc: 0.9182\n",
            "Epoch 36: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2966 - accuracy: 0.9176 - balanced_acc: 0.9177 - val_loss: 0.6938 - val_accuracy: 0.7461 - val_balanced_acc: 0.4535 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.2889 - accuracy: 0.9194 - balanced_acc: 0.9195\n",
            "Epoch 37: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2882 - accuracy: 0.9199 - balanced_acc: 0.9201 - val_loss: 0.6736 - val_accuracy: 0.7565 - val_balanced_acc: 0.4260 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "207/219 [===========================>..] - ETA: 0s - loss: 0.2830 - accuracy: 0.9210 - balanced_acc: 0.9198\n",
            "Epoch 38: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2819 - accuracy: 0.9210 - balanced_acc: 0.9199 - val_loss: 0.7180 - val_accuracy: 0.7202 - val_balanced_acc: 0.4409 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2814 - accuracy: 0.9209 - balanced_acc: 0.9211\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 39: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2817 - accuracy: 0.9206 - balanced_acc: 0.9208 - val_loss: 0.7305 - val_accuracy: 0.7254 - val_balanced_acc: 0.4499 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2763 - accuracy: 0.9234 - balanced_acc: 0.9236\n",
            "Epoch 40: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2765 - accuracy: 0.9236 - balanced_acc: 0.9239 - val_loss: 0.7086 - val_accuracy: 0.7358 - val_balanced_acc: 0.4359 - lr: 5.0000e-04\n",
            "Epoch 41/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2765 - accuracy: 0.9251 - balanced_acc: 0.9252\n",
            "Epoch 41: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2765 - accuracy: 0.9251 - balanced_acc: 0.9252 - val_loss: 0.7109 - val_accuracy: 0.7461 - val_balanced_acc: 0.4535 - lr: 5.0000e-04\n",
            "Epoch 42/100\n",
            "211/219 [===========================>..] - ETA: 0s - loss: 0.2667 - accuracy: 0.9267 - balanced_acc: 0.9263\n",
            "Epoch 42: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2671 - accuracy: 0.9265 - balanced_acc: 0.9264 - val_loss: 0.6948 - val_accuracy: 0.7565 - val_balanced_acc: 0.4251 - lr: 5.0000e-04\n",
            "Epoch 43/100\n",
            "214/219 [============================>.] - ETA: 0s - loss: 0.2685 - accuracy: 0.9268 - balanced_acc: 0.9259\n",
            "Epoch 43: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2687 - accuracy: 0.9267 - balanced_acc: 0.9260 - val_loss: 0.7209 - val_accuracy: 0.7202 - val_balanced_acc: 0.4428 - lr: 5.0000e-04\n",
            "Epoch 44/100\n",
            "207/219 [===========================>..] - ETA: 0s - loss: 0.2670 - accuracy: 0.9264 - balanced_acc: 0.9266\n",
            "Epoch 44: val_balanced_acc did not improve from 0.46028\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2656 - accuracy: 0.9269 - balanced_acc: 0.9267 - val_loss: 0.7128 - val_accuracy: 0.7513 - val_balanced_acc: 0.4216 - lr: 5.0000e-04\n",
            "Epoch 45/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2661 - accuracy: 0.9291 - balanced_acc: 0.9290\n",
            "Epoch 45: val_balanced_acc improved from 0.46028 to 0.46168, saving model to /content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5\n",
            "219/219 [==============================] - 2s 7ms/step - loss: 0.2658 - accuracy: 0.9291 - balanced_acc: 0.9290 - val_loss: 0.6787 - val_accuracy: 0.7668 - val_balanced_acc: 0.4617 - lr: 5.0000e-04\n",
            "Epoch 46/100\n",
            "212/219 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9281 - balanced_acc: 0.9289\n",
            "Epoch 46: val_balanced_acc did not improve from 0.46168\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2603 - accuracy: 0.9280 - balanced_acc: 0.9288 - val_loss: 0.6862 - val_accuracy: 0.7565 - val_balanced_acc: 0.4555 - lr: 5.0000e-04\n",
            "Epoch 47/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.2619 - accuracy: 0.9296 - balanced_acc: 0.9306\n",
            "Epoch 47: val_balanced_acc did not improve from 0.46168\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2628 - accuracy: 0.9288 - balanced_acc: 0.9296 - val_loss: 0.7079 - val_accuracy: 0.7461 - val_balanced_acc: 0.4178 - lr: 5.0000e-04\n",
            "Epoch 48/100\n",
            "212/219 [============================>.] - ETA: 0s - loss: 0.2564 - accuracy: 0.9334 - balanced_acc: 0.9337\n",
            "Epoch 48: val_balanced_acc did not improve from 0.46168\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2566 - accuracy: 0.9331 - balanced_acc: 0.9334 - val_loss: 0.6959 - val_accuracy: 0.7565 - val_balanced_acc: 0.4555 - lr: 5.0000e-04\n",
            "Epoch 49/100\n",
            "207/219 [===========================>..] - ETA: 0s - loss: 0.2558 - accuracy: 0.9314 - balanced_acc: 0.9321\n",
            "Epoch 49: val_balanced_acc improved from 0.46168 to 0.46441, saving model to /content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2573 - accuracy: 0.9310 - balanced_acc: 0.9314 - val_loss: 0.6931 - val_accuracy: 0.7617 - val_balanced_acc: 0.4644 - lr: 5.0000e-04\n",
            "Epoch 50/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.2514 - accuracy: 0.9337 - balanced_acc: 0.9331\n",
            "Epoch 50: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2515 - accuracy: 0.9338 - balanced_acc: 0.9332 - val_loss: 0.7147 - val_accuracy: 0.7409 - val_balanced_acc: 0.4161 - lr: 5.0000e-04\n",
            "Epoch 51/100\n",
            "212/219 [============================>.] - ETA: 0s - loss: 0.2553 - accuracy: 0.9318 - balanced_acc: 0.9317\n",
            "Epoch 51: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2544 - accuracy: 0.9328 - balanced_acc: 0.9326 - val_loss: 0.7080 - val_accuracy: 0.7409 - val_balanced_acc: 0.4207 - lr: 5.0000e-04\n",
            "Epoch 52/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.2541 - accuracy: 0.9297 - balanced_acc: 0.9306\n",
            "Epoch 52: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2531 - accuracy: 0.9306 - balanced_acc: 0.9313 - val_loss: 0.7070 - val_accuracy: 0.7409 - val_balanced_acc: 0.4165 - lr: 5.0000e-04\n",
            "Epoch 53/100\n",
            "205/219 [===========================>..] - ETA: 0s - loss: 0.2505 - accuracy: 0.9323 - balanced_acc: 0.9322\n",
            "Epoch 53: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2504 - accuracy: 0.9323 - balanced_acc: 0.9322 - val_loss: 0.6947 - val_accuracy: 0.7461 - val_balanced_acc: 0.4616 - lr: 5.0000e-04\n",
            "Epoch 54/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.2427 - accuracy: 0.9380 - balanced_acc: 0.9389\n",
            "Epoch 54: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2439 - accuracy: 0.9370 - balanced_acc: 0.9380 - val_loss: 0.7142 - val_accuracy: 0.7461 - val_balanced_acc: 0.4145 - lr: 5.0000e-04\n",
            "Epoch 55/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2473 - accuracy: 0.9326 - balanced_acc: 0.9327\n",
            "Epoch 55: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2476 - accuracy: 0.9325 - balanced_acc: 0.9326 - val_loss: 0.6908 - val_accuracy: 0.7565 - val_balanced_acc: 0.4272 - lr: 5.0000e-04\n",
            "Epoch 56/100\n",
            "213/219 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.9344 - balanced_acc: 0.9345\n",
            "Epoch 56: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2430 - accuracy: 0.9350 - balanced_acc: 0.9346 - val_loss: 0.6959 - val_accuracy: 0.7565 - val_balanced_acc: 0.4281 - lr: 5.0000e-04\n",
            "Epoch 57/100\n",
            "211/219 [===========================>..] - ETA: 0s - loss: 0.2444 - accuracy: 0.9361 - balanced_acc: 0.9350\n",
            "Epoch 57: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2444 - accuracy: 0.9358 - balanced_acc: 0.9345 - val_loss: 0.6871 - val_accuracy: 0.7617 - val_balanced_acc: 0.4243 - lr: 5.0000e-04\n",
            "Epoch 58/100\n",
            "205/219 [===========================>..] - ETA: 0s - loss: 0.2347 - accuracy: 0.9387 - balanced_acc: 0.9386\n",
            "Epoch 58: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2371 - accuracy: 0.9375 - balanced_acc: 0.9369 - val_loss: 0.7027 - val_accuracy: 0.7461 - val_balanced_acc: 0.4130 - lr: 5.0000e-04\n",
            "Epoch 59/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2397 - accuracy: 0.9364 - balanced_acc: 0.9368\n",
            "Epoch 59: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2392 - accuracy: 0.9370 - balanced_acc: 0.9374 - val_loss: 0.7020 - val_accuracy: 0.7668 - val_balanced_acc: 0.4617 - lr: 5.0000e-04\n",
            "Epoch 60/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.2377 - accuracy: 0.9364 - balanced_acc: 0.9375\n",
            "Epoch 60: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2369 - accuracy: 0.9368 - balanced_acc: 0.9378 - val_loss: 0.6657 - val_accuracy: 0.7565 - val_balanced_acc: 0.3924 - lr: 5.0000e-04\n",
            "Epoch 61/100\n",
            "214/219 [============================>.] - ETA: 0s - loss: 0.2350 - accuracy: 0.9398 - balanced_acc: 0.9404\n",
            "Epoch 61: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2347 - accuracy: 0.9401 - balanced_acc: 0.9407 - val_loss: 0.7084 - val_accuracy: 0.7513 - val_balanced_acc: 0.4546 - lr: 5.0000e-04\n",
            "Epoch 62/100\n",
            "213/219 [============================>.] - ETA: 0s - loss: 0.2348 - accuracy: 0.9384 - balanced_acc: 0.9384\n",
            "Epoch 62: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 5ms/step - loss: 0.2345 - accuracy: 0.9385 - balanced_acc: 0.9385 - val_loss: 0.6856 - val_accuracy: 0.7617 - val_balanced_acc: 0.4566 - lr: 5.0000e-04\n",
            "Epoch 63/100\n",
            "214/219 [============================>.] - ETA: 0s - loss: 0.2350 - accuracy: 0.9379 - balanced_acc: 0.9374\n",
            "Epoch 63: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 5ms/step - loss: 0.2353 - accuracy: 0.9377 - balanced_acc: 0.9372 - val_loss: 0.7094 - val_accuracy: 0.7306 - val_balanced_acc: 0.4104 - lr: 5.0000e-04\n",
            "Epoch 64/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2264 - accuracy: 0.9424 - balanced_acc: 0.9424\n",
            "Epoch 64: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 5ms/step - loss: 0.2274 - accuracy: 0.9419 - balanced_acc: 0.9415 - val_loss: 0.6748 - val_accuracy: 0.7565 - val_balanced_acc: 0.4483 - lr: 5.0000e-04\n",
            "Epoch 65/100\n",
            "205/219 [===========================>..] - ETA: 0s - loss: 0.2307 - accuracy: 0.9395 - balanced_acc: 0.9391\n",
            "Epoch 65: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2301 - accuracy: 0.9403 - balanced_acc: 0.9398 - val_loss: 0.6904 - val_accuracy: 0.7513 - val_balanced_acc: 0.4181 - lr: 5.0000e-04\n",
            "Epoch 66/100\n",
            "206/219 [===========================>..] - ETA: 0s - loss: 0.2269 - accuracy: 0.9398 - balanced_acc: 0.9404\n",
            "Epoch 66: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2257 - accuracy: 0.9404 - balanced_acc: 0.9410 - val_loss: 0.6921 - val_accuracy: 0.7461 - val_balanced_acc: 0.4145 - lr: 5.0000e-04\n",
            "Epoch 67/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 0.9427 - balanced_acc: 0.9431\n",
            "Epoch 67: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2274 - accuracy: 0.9427 - balanced_acc: 0.9431 - val_loss: 0.6825 - val_accuracy: 0.7617 - val_balanced_acc: 0.4200 - lr: 5.0000e-04\n",
            "Epoch 68/100\n",
            "211/219 [===========================>..] - ETA: 0s - loss: 0.2211 - accuracy: 0.9443 - balanced_acc: 0.9440\n",
            "Epoch 68: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2230 - accuracy: 0.9432 - balanced_acc: 0.9431 - val_loss: 0.6692 - val_accuracy: 0.7668 - val_balanced_acc: 0.4064 - lr: 5.0000e-04\n",
            "Epoch 69/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.2200 - accuracy: 0.9436 - balanced_acc: 0.9415\n",
            "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 69: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2199 - accuracy: 0.9438 - balanced_acc: 0.9418 - val_loss: 0.6815 - val_accuracy: 0.7565 - val_balanced_acc: 0.4192 - lr: 5.0000e-04\n",
            "Epoch 70/100\n",
            "206/219 [===========================>..] - ETA: 0s - loss: 0.2229 - accuracy: 0.9439 - balanced_acc: 0.9440\n",
            "Epoch 70: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2223 - accuracy: 0.9441 - balanced_acc: 0.9443 - val_loss: 0.6910 - val_accuracy: 0.7565 - val_balanced_acc: 0.4192 - lr: 2.5000e-04\n",
            "Epoch 71/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2184 - accuracy: 0.9453 - balanced_acc: 0.9443\n",
            "Epoch 71: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2181 - accuracy: 0.9451 - balanced_acc: 0.9443 - val_loss: 0.6803 - val_accuracy: 0.7565 - val_balanced_acc: 0.4192 - lr: 2.5000e-04\n",
            "Epoch 72/100\n",
            "207/219 [===========================>..] - ETA: 0s - loss: 0.2204 - accuracy: 0.9438 - balanced_acc: 0.9445\n",
            "Epoch 72: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2203 - accuracy: 0.9438 - balanced_acc: 0.9445 - val_loss: 0.6958 - val_accuracy: 0.7565 - val_balanced_acc: 0.4230 - lr: 2.5000e-04\n",
            "Epoch 73/100\n",
            "204/219 [==========================>...] - ETA: 0s - loss: 0.2166 - accuracy: 0.9458 - balanced_acc: 0.9452\n",
            "Epoch 73: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2160 - accuracy: 0.9466 - balanced_acc: 0.9458 - val_loss: 0.6828 - val_accuracy: 0.7565 - val_balanced_acc: 0.3967 - lr: 2.5000e-04\n",
            "Epoch 74/100\n",
            "212/219 [============================>.] - ETA: 0s - loss: 0.2175 - accuracy: 0.9438 - balanced_acc: 0.9453\n",
            "Epoch 74: val_balanced_acc did not improve from 0.46441\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2170 - accuracy: 0.9442 - balanced_acc: 0.9457 - val_loss: 0.6976 - val_accuracy: 0.7565 - val_balanced_acc: 0.3967 - lr: 2.5000e-04\n",
            "Epoch 75/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.2138 - accuracy: 0.9460 - balanced_acc: 0.9454\n",
            "Epoch 75: val_balanced_acc improved from 0.46441 to 0.46467, saving model to /content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5\n",
            "219/219 [==============================] - 2s 8ms/step - loss: 0.2155 - accuracy: 0.9453 - balanced_acc: 0.9447 - val_loss: 0.6835 - val_accuracy: 0.7617 - val_balanced_acc: 0.4647 - lr: 2.5000e-04\n",
            "Epoch 76/100\n",
            "207/219 [===========================>..] - ETA: 0s - loss: 0.2155 - accuracy: 0.9463 - balanced_acc: 0.9460\n",
            "Epoch 76: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2156 - accuracy: 0.9461 - balanced_acc: 0.9457 - val_loss: 0.6900 - val_accuracy: 0.7565 - val_balanced_acc: 0.4192 - lr: 2.5000e-04\n",
            "Epoch 77/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2186 - accuracy: 0.9457 - balanced_acc: 0.9460\n",
            "Epoch 77: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2185 - accuracy: 0.9455 - balanced_acc: 0.9455 - val_loss: 0.6824 - val_accuracy: 0.7565 - val_balanced_acc: 0.3967 - lr: 2.5000e-04\n",
            "Epoch 78/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.2103 - accuracy: 0.9479 - balanced_acc: 0.9477\n",
            "Epoch 78: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2120 - accuracy: 0.9473 - balanced_acc: 0.9471 - val_loss: 0.6785 - val_accuracy: 0.7565 - val_balanced_acc: 0.3967 - lr: 2.5000e-04\n",
            "Epoch 79/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2143 - accuracy: 0.9458 - balanced_acc: 0.9470\n",
            "Epoch 79: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2137 - accuracy: 0.9459 - balanced_acc: 0.9470 - val_loss: 0.7005 - val_accuracy: 0.7513 - val_balanced_acc: 0.4181 - lr: 2.5000e-04\n",
            "Epoch 80/100\n",
            "207/219 [===========================>..] - ETA: 0s - loss: 0.2130 - accuracy: 0.9464 - balanced_acc: 0.9470\n",
            "Epoch 80: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2121 - accuracy: 0.9470 - balanced_acc: 0.9477 - val_loss: 0.6866 - val_accuracy: 0.7513 - val_balanced_acc: 0.4141 - lr: 2.5000e-04\n",
            "Epoch 81/100\n",
            "205/219 [===========================>..] - ETA: 0s - loss: 0.2113 - accuracy: 0.9477 - balanced_acc: 0.9494\n",
            "Epoch 81: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2110 - accuracy: 0.9478 - balanced_acc: 0.9496 - val_loss: 0.7002 - val_accuracy: 0.7306 - val_balanced_acc: 0.4185 - lr: 2.5000e-04\n",
            "Epoch 82/100\n",
            "218/219 [============================>.] - ETA: 0s - loss: 0.2101 - accuracy: 0.9488 - balanced_acc: 0.9474\n",
            "Epoch 82: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2104 - accuracy: 0.9486 - balanced_acc: 0.9473 - val_loss: 0.6897 - val_accuracy: 0.7513 - val_balanced_acc: 0.4141 - lr: 2.5000e-04\n",
            "Epoch 83/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2169 - accuracy: 0.9453 - balanced_acc: 0.9460\n",
            "Epoch 83: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2164 - accuracy: 0.9453 - balanced_acc: 0.9463 - val_loss: 0.6915 - val_accuracy: 0.7565 - val_balanced_acc: 0.4192 - lr: 2.5000e-04\n",
            "Epoch 84/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2081 - accuracy: 0.9473 - balanced_acc: 0.9478\n",
            "Epoch 84: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2085 - accuracy: 0.9471 - balanced_acc: 0.9479 - val_loss: 0.6960 - val_accuracy: 0.7513 - val_balanced_acc: 0.4141 - lr: 2.5000e-04\n",
            "Epoch 85/100\n",
            "218/219 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9485 - balanced_acc: 0.9485\n",
            "Epoch 85: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2057 - accuracy: 0.9487 - balanced_acc: 0.9488 - val_loss: 0.7005 - val_accuracy: 0.7513 - val_balanced_acc: 0.4183 - lr: 2.5000e-04\n",
            "Epoch 86/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.2109 - accuracy: 0.9474 - balanced_acc: 0.9486\n",
            "Epoch 86: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2114 - accuracy: 0.9476 - balanced_acc: 0.9484 - val_loss: 0.6834 - val_accuracy: 0.7565 - val_balanced_acc: 0.4192 - lr: 2.5000e-04\n",
            "Epoch 87/100\n",
            "206/219 [===========================>..] - ETA: 0s - loss: 0.2065 - accuracy: 0.9495 - balanced_acc: 0.9499\n",
            "Epoch 87: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2065 - accuracy: 0.9494 - balanced_acc: 0.9497 - val_loss: 0.6819 - val_accuracy: 0.7565 - val_balanced_acc: 0.4230 - lr: 2.5000e-04\n",
            "Epoch 88/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2062 - accuracy: 0.9475 - balanced_acc: 0.9478\n",
            "Epoch 88: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2063 - accuracy: 0.9473 - balanced_acc: 0.9477 - val_loss: 0.6966 - val_accuracy: 0.7617 - val_balanced_acc: 0.4056 - lr: 2.5000e-04\n",
            "Epoch 89/100\n",
            "209/219 [===========================>..] - ETA: 0s - loss: 0.2040 - accuracy: 0.9481 - balanced_acc: 0.9494\n",
            "Epoch 89: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2044 - accuracy: 0.9485 - balanced_acc: 0.9496 - val_loss: 0.7137 - val_accuracy: 0.7358 - val_balanced_acc: 0.4113 - lr: 2.5000e-04\n",
            "Epoch 90/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.2070 - accuracy: 0.9489 - balanced_acc: 0.9481\n",
            "Epoch 90: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2064 - accuracy: 0.9490 - balanced_acc: 0.9482 - val_loss: 0.6923 - val_accuracy: 0.7565 - val_balanced_acc: 0.4230 - lr: 2.5000e-04\n",
            "Epoch 91/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.2059 - accuracy: 0.9495 - balanced_acc: 0.9488\n",
            "Epoch 91: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2065 - accuracy: 0.9485 - balanced_acc: 0.9478 - val_loss: 0.6948 - val_accuracy: 0.7513 - val_balanced_acc: 0.4222 - lr: 2.5000e-04\n",
            "Epoch 92/100\n",
            "205/219 [===========================>..] - ETA: 0s - loss: 0.2023 - accuracy: 0.9490 - balanced_acc: 0.9480\n",
            "Epoch 92: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2018 - accuracy: 0.9492 - balanced_acc: 0.9482 - val_loss: 0.6898 - val_accuracy: 0.7565 - val_balanced_acc: 0.4230 - lr: 2.5000e-04\n",
            "Epoch 93/100\n",
            "218/219 [============================>.] - ETA: 0s - loss: 0.2057 - accuracy: 0.9498 - balanced_acc: 0.9502\n",
            "Epoch 93: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2056 - accuracy: 0.9499 - balanced_acc: 0.9503 - val_loss: 0.6894 - val_accuracy: 0.7513 - val_balanced_acc: 0.4222 - lr: 2.5000e-04\n",
            "Epoch 94/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.1993 - accuracy: 0.9506 - balanced_acc: 0.9498\n",
            "Epoch 94: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2000 - accuracy: 0.9501 - balanced_acc: 0.9494 - val_loss: 0.6972 - val_accuracy: 0.7461 - val_balanced_acc: 0.4213 - lr: 2.5000e-04\n",
            "Epoch 95/100\n",
            "210/219 [===========================>..] - ETA: 0s - loss: 0.2017 - accuracy: 0.9503 - balanced_acc: 0.9503\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 95: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2029 - accuracy: 0.9495 - balanced_acc: 0.9497 - val_loss: 0.6895 - val_accuracy: 0.7513 - val_balanced_acc: 0.4141 - lr: 2.5000e-04\n",
            "Epoch 96/100\n",
            "208/219 [===========================>..] - ETA: 0s - loss: 0.2028 - accuracy: 0.9496 - balanced_acc: 0.9494\n",
            "Epoch 96: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2034 - accuracy: 0.9497 - balanced_acc: 0.9495 - val_loss: 0.6880 - val_accuracy: 0.7461 - val_balanced_acc: 0.4132 - lr: 1.2500e-04\n",
            "Epoch 97/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.9500 - balanced_acc: 0.9502\n",
            "Epoch 97: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2015 - accuracy: 0.9500 - balanced_acc: 0.9502 - val_loss: 0.6926 - val_accuracy: 0.7513 - val_balanced_acc: 0.4141 - lr: 1.2500e-04\n",
            "Epoch 98/100\n",
            "213/219 [============================>.] - ETA: 0s - loss: 0.1988 - accuracy: 0.9508 - balanced_acc: 0.9501\n",
            "Epoch 98: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.1992 - accuracy: 0.9503 - balanced_acc: 0.9497 - val_loss: 0.6875 - val_accuracy: 0.7513 - val_balanced_acc: 0.4141 - lr: 1.2500e-04\n",
            "Epoch 99/100\n",
            "211/219 [===========================>..] - ETA: 0s - loss: 0.1999 - accuracy: 0.9519 - balanced_acc: 0.9510\n",
            "Epoch 99: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.2000 - accuracy: 0.9516 - balanced_acc: 0.9507 - val_loss: 0.6830 - val_accuracy: 0.7513 - val_balanced_acc: 0.4141 - lr: 1.2500e-04\n",
            "Epoch 100/100\n",
            "205/219 [===========================>..] - ETA: 0s - loss: 0.1972 - accuracy: 0.9520 - balanced_acc: 0.9514\n",
            "Epoch 100: val_balanced_acc did not improve from 0.46467\n",
            "219/219 [==============================] - 1s 4ms/step - loss: 0.1967 - accuracy: 0.9520 - balanced_acc: 0.9518 - val_loss: 0.6793 - val_accuracy: 0.7513 - val_balanced_acc: 0.4141 - lr: 1.2500e-04\n"
          ]
        }
      ],
      "source": [
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/last_model_no.h5'\n",
        "mc1 = ModelCheckpoint(best_model_fpath, monitor='val_balanced_acc', mode='max', verbose=1, save_best_only=True)\n",
        "model2.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst = model2.fit(X_train_fm_ov, y_train_ov, epochs=100, batch_size=BATCH_SIZE, validation_data=(X_val_fm, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train_fm_ov.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8XhlbWn--8Or",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a5608582-6869-41e8-dc3e-f5fd03468aa8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save last model\n",
        "model2.save(last_model_fpath)"
      ],
      "metadata": {
        "id": "u-x0SENPGmm9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_model = load_model(last_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = last_model.predict(X_train_fm_ov)\n",
        "y_val_pred = last_model.predict(X_val_fm)\n",
        "\n",
        "#print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-e3ZaeeG1Bf",
        "outputId": "2ee8596a-a3fc-4474-e7a0-b822204123b8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.9514811394473254\n",
            "balanced accuracy on training 0.9514811394473254\n",
            "accuracy on validation 0.7512953367875648\n",
            "balanced accuracy on validation 0.539990120216601\n",
            "Score on val data:  (0.5163944851444852, 0.539990120216601, 0.5232838460980782, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = load_model(best_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = best_model.predict(X_train_fm_ov)\n",
        "y_val_pred = best_model.predict(X_val_fm)\n",
        "\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ3baQLsHLat",
        "outputId": "0bd3b56c-3648-4933-a260-9550706d526b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.9451587696242097\n",
            "balanced accuracy on training 0.9451587696242097\n",
            "accuracy on validation 0.7616580310880829\n",
            "balanced accuracy on validation 0.6896499841621793\n",
            "Score on val data:  (0.5665220111032918, 0.6896499841621793, 0.6005201453622202, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRGeofw-8tK"
      },
      "source": [
        "#Load ISIC 2018 Challange Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l3P7IjyLuZGY"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val = load_isic2018_dataset(train_under_frac = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhLZis8MQIhk",
        "outputId": "1886e70e-4ad7-4dc0-ba76-460d846a94e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[208, 119, 134],\n",
              "         [206, 120, 133],\n",
              "         [207, 122, 134],\n",
              "         ...,\n",
              "         [212, 130, 146],\n",
              "         [211, 129, 146],\n",
              "         [211, 128, 145]],\n",
              "\n",
              "        [[204, 113, 130],\n",
              "         [207, 119, 135],\n",
              "         [211, 122, 139],\n",
              "         ...,\n",
              "         [210, 132, 148],\n",
              "         [209, 130, 144],\n",
              "         [210, 128, 142]],\n",
              "\n",
              "        [[204, 113, 131],\n",
              "         [207, 117, 134],\n",
              "         [211, 121, 138],\n",
              "         ...,\n",
              "         [212, 131, 147],\n",
              "         [212, 130, 146],\n",
              "         [209, 126, 143]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[189, 119, 120],\n",
              "         [191, 122, 124],\n",
              "         [193, 124, 126],\n",
              "         ...,\n",
              "         [191, 126, 127],\n",
              "         [191, 125, 128],\n",
              "         [187, 121, 127]],\n",
              "\n",
              "        [[191, 119, 121],\n",
              "         [190, 119, 121],\n",
              "         [192, 120, 125],\n",
              "         ...,\n",
              "         [189, 123, 128],\n",
              "         [191, 123, 130],\n",
              "         [190, 124, 129]],\n",
              "\n",
              "        [[191, 116, 118],\n",
              "         [190, 114, 119],\n",
              "         [191, 116, 121],\n",
              "         ...,\n",
              "         [190, 121, 125],\n",
              "         [190, 122, 127],\n",
              "         [191, 124, 131]]],\n",
              "\n",
              "\n",
              "       [[[ 71,  34,  38],\n",
              "         [ 72,  32,  39],\n",
              "         [ 71,  32,  38],\n",
              "         ...,\n",
              "         [ 78,  29,  33],\n",
              "         [ 75,  30,  32],\n",
              "         [ 77,  30,  32]],\n",
              "\n",
              "        [[ 71,  32,  36],\n",
              "         [ 72,  33,  37],\n",
              "         [ 72,  32,  38],\n",
              "         ...,\n",
              "         [ 78,  30,  35],\n",
              "         [ 76,  30,  32],\n",
              "         [ 76,  30,  33]],\n",
              "\n",
              "        [[ 73,  33,  37],\n",
              "         [ 70,  32,  38],\n",
              "         [ 70,  31,  37],\n",
              "         ...,\n",
              "         [ 76,  29,  35],\n",
              "         [ 76,  31,  33],\n",
              "         [ 77,  31,  33]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 77,  34,  44],\n",
              "         [ 80,  35,  43],\n",
              "         [ 80,  34,  40],\n",
              "         ...,\n",
              "         [ 73,  31,  42],\n",
              "         [ 73,  32,  43],\n",
              "         [ 70,  32,  42]],\n",
              "\n",
              "        [[ 76,  34,  41],\n",
              "         [ 77,  35,  42],\n",
              "         [ 77,  34,  40],\n",
              "         ...,\n",
              "         [ 72,  32,  43],\n",
              "         [ 72,  32,  44],\n",
              "         [ 71,  32,  42]],\n",
              "\n",
              "        [[ 77,  34,  39],\n",
              "         [ 76,  34,  39],\n",
              "         [ 75,  34,  39],\n",
              "         ...,\n",
              "         [ 72,  32,  45],\n",
              "         [ 72,  31,  46],\n",
              "         [ 72,  32,  42]]],\n",
              "\n",
              "\n",
              "       [[[244, 156, 175],\n",
              "         [244, 157, 175],\n",
              "         [245, 158, 177],\n",
              "         ...,\n",
              "         [239, 164, 175],\n",
              "         [241, 164, 175],\n",
              "         [241, 164, 174]],\n",
              "\n",
              "        [[246, 159, 181],\n",
              "         [245, 158, 180],\n",
              "         [245, 158, 179],\n",
              "         ...,\n",
              "         [241, 165, 175],\n",
              "         [241, 165, 177],\n",
              "         [241, 163, 176]],\n",
              "\n",
              "        [[247, 161, 180],\n",
              "         [246, 161, 179],\n",
              "         [245, 161, 180],\n",
              "         ...,\n",
              "         [241, 165, 176],\n",
              "         [242, 164, 177],\n",
              "         [240, 163, 177]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[222, 144, 150],\n",
              "         [224, 146, 151],\n",
              "         [224, 149, 152],\n",
              "         ...,\n",
              "         [224, 156, 153],\n",
              "         [227, 159, 159],\n",
              "         [225, 158, 157]],\n",
              "\n",
              "        [[219, 144, 149],\n",
              "         [222, 146, 151],\n",
              "         [226, 148, 153],\n",
              "         ...,\n",
              "         [226, 157, 156],\n",
              "         [224, 160, 155],\n",
              "         [223, 158, 153]],\n",
              "\n",
              "        [[221, 148, 151],\n",
              "         [223, 147, 150],\n",
              "         [224, 147, 152],\n",
              "         ...,\n",
              "         [226, 159, 155],\n",
              "         [224, 159, 157],\n",
              "         [223, 157, 154]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[159, 145, 164],\n",
              "         [160, 145, 165],\n",
              "         [158, 147, 165],\n",
              "         ...,\n",
              "         [157, 144, 157],\n",
              "         [158, 143, 160],\n",
              "         [158, 148, 158]],\n",
              "\n",
              "        [[158, 147, 169],\n",
              "         [158, 147, 169],\n",
              "         [158, 147, 166],\n",
              "         ...,\n",
              "         [156, 148, 159],\n",
              "         [156, 153, 163],\n",
              "         [158, 152, 161]],\n",
              "\n",
              "        [[157, 147, 167],\n",
              "         [157, 149, 169],\n",
              "         [158, 149, 168],\n",
              "         ...,\n",
              "         [160, 152, 165],\n",
              "         [159, 154, 167],\n",
              "         [157, 154, 166]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[184, 175, 195],\n",
              "         [183, 175, 193],\n",
              "         [184, 170, 191],\n",
              "         ...,\n",
              "         [170, 165, 179],\n",
              "         [170, 163, 178],\n",
              "         [170, 164, 177]],\n",
              "\n",
              "        [[185, 175, 193],\n",
              "         [183, 174, 193],\n",
              "         [185, 172, 193],\n",
              "         ...,\n",
              "         [169, 163, 176],\n",
              "         [169, 161, 176],\n",
              "         [169, 160, 176]],\n",
              "\n",
              "        [[187, 173, 194],\n",
              "         [189, 175, 193],\n",
              "         [189, 178, 198],\n",
              "         ...,\n",
              "         [169, 161, 174],\n",
              "         [169, 160, 176],\n",
              "         [168, 159, 173]]],\n",
              "\n",
              "\n",
              "       [[[ 76,  49,  53],\n",
              "         [ 78,  54,  57],\n",
              "         [ 80,  60,  58],\n",
              "         ...,\n",
              "         [ 77,  54,  48],\n",
              "         [ 77,  48,  48],\n",
              "         [ 73,  45,  46]],\n",
              "\n",
              "        [[ 77,  54,  51],\n",
              "         [ 83,  56,  59],\n",
              "         [ 85,  60,  57],\n",
              "         ...,\n",
              "         [ 80,  55,  45],\n",
              "         [ 78,  49,  48],\n",
              "         [ 75,  47,  45]],\n",
              "\n",
              "        [[ 78,  57,  57],\n",
              "         [ 83,  60,  61],\n",
              "         [ 88,  62,  64],\n",
              "         ...,\n",
              "         [ 82,  57,  49],\n",
              "         [ 80,  52,  50],\n",
              "         [ 76,  49,  49]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[105,  75,  74],\n",
              "         [107,  77,  80],\n",
              "         [108,  79,  78],\n",
              "         ...,\n",
              "         [ 96,  67,  70],\n",
              "         [ 93,  63,  68],\n",
              "         [ 91,  62,  66]],\n",
              "\n",
              "        [[104,  75,  74],\n",
              "         [107,  76,  77],\n",
              "         [109,  78,  78],\n",
              "         ...,\n",
              "         [ 94,  64,  69],\n",
              "         [ 92,  64,  67],\n",
              "         [ 90,  63,  65]],\n",
              "\n",
              "        [[101,  74,  71],\n",
              "         [105,  76,  80],\n",
              "         [107,  78,  81],\n",
              "         ...,\n",
              "         [ 93,  63,  65],\n",
              "         [ 91,  64,  66],\n",
              "         [ 87,  64,  65]]],\n",
              "\n",
              "\n",
              "       [[[186, 167, 182],\n",
              "         [190, 171, 184],\n",
              "         [191, 172, 185],\n",
              "         ...,\n",
              "         [156, 138, 159],\n",
              "         [151, 136, 159],\n",
              "         [148, 133, 158]],\n",
              "\n",
              "        [[186, 166, 185],\n",
              "         [190, 172, 187],\n",
              "         [195, 174, 188],\n",
              "         ...,\n",
              "         [158, 139, 160],\n",
              "         [154, 138, 164],\n",
              "         [150, 136, 160]],\n",
              "\n",
              "        [[189, 171, 189],\n",
              "         [192, 172, 190],\n",
              "         [192, 175, 192],\n",
              "         ...,\n",
              "         [158, 143, 163],\n",
              "         [156, 143, 165],\n",
              "         [151, 139, 162]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[199, 182, 199],\n",
              "         [201, 183, 198],\n",
              "         [202, 186, 197],\n",
              "         ...,\n",
              "         [159, 137, 150],\n",
              "         [157, 135, 145],\n",
              "         [154, 128, 141]],\n",
              "\n",
              "        [[196, 179, 193],\n",
              "         [199, 182, 195],\n",
              "         [202, 185, 197],\n",
              "         ...,\n",
              "         [158, 136, 148],\n",
              "         [153, 133, 143],\n",
              "         [152, 127, 139]],\n",
              "\n",
              "        [[193, 173, 188],\n",
              "         [196, 178, 192],\n",
              "         [203, 181, 195],\n",
              "         ...,\n",
              "         [160, 134, 148],\n",
              "         [154, 129, 141],\n",
              "         [149, 127, 137]]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsMdwJjBQLoV",
        "outputId": "4342f742-5698-482b-c74a-ea2f186891c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[ 35,  24,  31],\n",
              "         [ 43,  31,  38],\n",
              "         [ 53,  41,  48],\n",
              "         ...,\n",
              "         [ 51,  36,  36],\n",
              "         [ 52,  37,  37],\n",
              "         [ 54,  39,  39]],\n",
              "\n",
              "        [[ 40,  28,  36],\n",
              "         [ 49,  37,  43],\n",
              "         [ 60,  46,  50],\n",
              "         ...,\n",
              "         [ 51,  35,  37],\n",
              "         [ 52,  38,  38],\n",
              "         [ 53,  38,  39]],\n",
              "\n",
              "        [[ 46,  34,  40],\n",
              "         [ 56,  42,  47],\n",
              "         [ 65,  52,  55],\n",
              "         ...,\n",
              "         [ 52,  34,  37],\n",
              "         [ 51,  37,  37],\n",
              "         [ 52,  38,  38]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 54,  33,  39],\n",
              "         [ 52,  32,  37],\n",
              "         [ 52,  33,  39],\n",
              "         ...,\n",
              "         [ 22,  13,  18],\n",
              "         [ 22,  12,  16],\n",
              "         [ 22,  13,  15]],\n",
              "\n",
              "        [[ 55,  34,  40],\n",
              "         [ 54,  34,  39],\n",
              "         [ 53,  33,  39],\n",
              "         ...,\n",
              "         [ 22,  13,  16],\n",
              "         [ 21,  12,  15],\n",
              "         [ 22,  13,  16]],\n",
              "\n",
              "        [[ 56,  36,  43],\n",
              "         [ 54,  36,  42],\n",
              "         [ 54,  33,  38],\n",
              "         ...,\n",
              "         [ 22,  13,  15],\n",
              "         [ 21,  12,  15],\n",
              "         [ 22,  13,  16]]],\n",
              "\n",
              "\n",
              "       [[[208, 129, 124],\n",
              "         [210, 134, 133],\n",
              "         [214, 141, 144],\n",
              "         ...,\n",
              "         [207, 143, 143],\n",
              "         [209, 141, 144],\n",
              "         [211, 139, 145]],\n",
              "\n",
              "        [[208, 132, 126],\n",
              "         [211, 136, 134],\n",
              "         [214, 142, 140],\n",
              "         ...,\n",
              "         [210, 145, 145],\n",
              "         [208, 140, 142],\n",
              "         [210, 139, 143]],\n",
              "\n",
              "        [[208, 134, 133],\n",
              "         [213, 138, 135],\n",
              "         [215, 143, 139],\n",
              "         ...,\n",
              "         [208, 142, 145],\n",
              "         [207, 141, 142],\n",
              "         [208, 138, 139]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[193, 129, 111],\n",
              "         [189, 120, 101],\n",
              "         [186, 117,  95],\n",
              "         ...,\n",
              "         [188, 124, 117],\n",
              "         [187, 127, 117],\n",
              "         [187, 126, 117]],\n",
              "\n",
              "        [[193, 127, 110],\n",
              "         [191, 121, 103],\n",
              "         [188, 115,  95],\n",
              "         ...,\n",
              "         [186, 124, 115],\n",
              "         [183, 126, 114],\n",
              "         [185, 125, 115]],\n",
              "\n",
              "        [[193, 126, 110],\n",
              "         [189, 121, 104],\n",
              "         [187, 118, 100],\n",
              "         ...,\n",
              "         [187, 124, 115],\n",
              "         [184, 124, 113],\n",
              "         [185, 124, 113]]],\n",
              "\n",
              "\n",
              "       [[[170, 127, 126],\n",
              "         [173, 130, 130],\n",
              "         [175, 140, 139],\n",
              "         ...,\n",
              "         [172, 127, 121],\n",
              "         [173, 127, 121],\n",
              "         [175, 129, 125]],\n",
              "\n",
              "        [[173, 133, 132],\n",
              "         [175, 136, 136],\n",
              "         [177, 142, 144],\n",
              "         ...,\n",
              "         [171, 127, 124],\n",
              "         [173, 127, 122],\n",
              "         [175, 129, 125]],\n",
              "\n",
              "        [[176, 140, 133],\n",
              "         [176, 139, 133],\n",
              "         [178, 143, 145],\n",
              "         ...,\n",
              "         [173, 129, 129],\n",
              "         [173, 131, 130],\n",
              "         [175, 134, 133]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[181, 141, 134],\n",
              "         [187, 148, 143],\n",
              "         [189, 150, 145],\n",
              "         ...,\n",
              "         [169, 131, 140],\n",
              "         [165, 126, 130],\n",
              "         [166, 120, 124]],\n",
              "\n",
              "        [[183, 146, 141],\n",
              "         [189, 155, 149],\n",
              "         [192, 156, 153],\n",
              "         ...,\n",
              "         [169, 130, 139],\n",
              "         [164, 125, 130],\n",
              "         [168, 123, 125]],\n",
              "\n",
              "        [[185, 151, 149],\n",
              "         [190, 157, 154],\n",
              "         [193, 163, 161],\n",
              "         ...,\n",
              "         [166, 127, 136],\n",
              "         [163, 122, 128],\n",
              "         [166, 123, 124]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[182, 181, 200],\n",
              "         [182, 183, 204],\n",
              "         [184, 184, 205],\n",
              "         ...,\n",
              "         [186, 184, 199],\n",
              "         [185, 186, 199],\n",
              "         [186, 186, 201]],\n",
              "\n",
              "        [[182, 183, 200],\n",
              "         [181, 183, 202],\n",
              "         [184, 185, 204],\n",
              "         ...,\n",
              "         [186, 187, 199],\n",
              "         [187, 185, 200],\n",
              "         [188, 185, 200]],\n",
              "\n",
              "        [[182, 185, 203],\n",
              "         [183, 185, 204],\n",
              "         [183, 185, 204],\n",
              "         ...,\n",
              "         [187, 186, 198],\n",
              "         [186, 185, 200],\n",
              "         [187, 186, 200]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[185, 186, 200],\n",
              "         [185, 188, 201],\n",
              "         [182, 186, 199],\n",
              "         ...,\n",
              "         [191, 190, 207],\n",
              "         [190, 189, 206],\n",
              "         [190, 190, 208]],\n",
              "\n",
              "        [[184, 186, 199],\n",
              "         [184, 186, 198],\n",
              "         [183, 185, 198],\n",
              "         ...,\n",
              "         [190, 188, 205],\n",
              "         [192, 188, 206],\n",
              "         [191, 188, 207]],\n",
              "\n",
              "        [[184, 185, 199],\n",
              "         [183, 184, 197],\n",
              "         [182, 185, 198],\n",
              "         ...,\n",
              "         [188, 187, 205],\n",
              "         [190, 189, 205],\n",
              "         [190, 191, 205]]],\n",
              "\n",
              "\n",
              "       [[[150, 127, 149],\n",
              "         [150, 126, 151],\n",
              "         [150, 126, 151],\n",
              "         ...,\n",
              "         [162, 139, 165],\n",
              "         [159, 140, 165],\n",
              "         [158, 142, 164]],\n",
              "\n",
              "        [[150, 127, 150],\n",
              "         [150, 126, 153],\n",
              "         [151, 126, 152],\n",
              "         ...,\n",
              "         [160, 139, 165],\n",
              "         [160, 138, 166],\n",
              "         [158, 140, 164]],\n",
              "\n",
              "        [[151, 126, 152],\n",
              "         [150, 127, 152],\n",
              "         [151, 127, 152],\n",
              "         ...,\n",
              "         [159, 138, 165],\n",
              "         [160, 136, 164],\n",
              "         [159, 136, 164]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[147, 121, 151],\n",
              "         [146, 121, 150],\n",
              "         [144, 120, 150],\n",
              "         ...,\n",
              "         [158, 135, 156],\n",
              "         [159, 135, 157],\n",
              "         [157, 135, 158]],\n",
              "\n",
              "        [[142, 116, 150],\n",
              "         [145, 119, 149],\n",
              "         [143, 120, 149],\n",
              "         ...,\n",
              "         [159, 135, 157],\n",
              "         [157, 135, 156],\n",
              "         [156, 133, 156]],\n",
              "\n",
              "        [[142, 113, 149],\n",
              "         [145, 117, 149],\n",
              "         [143, 119, 151],\n",
              "         ...,\n",
              "         [159, 136, 158],\n",
              "         [158, 134, 156],\n",
              "         [157, 134, 156]]],\n",
              "\n",
              "\n",
              "       [[[167, 108,  95],\n",
              "         [178, 115,  99],\n",
              "         [181, 121, 108],\n",
              "         ...,\n",
              "         [185, 127, 117],\n",
              "         [182, 123, 112],\n",
              "         [181, 120, 113]],\n",
              "\n",
              "        [[190, 135, 120],\n",
              "         [198, 134, 114],\n",
              "         [199, 132, 111],\n",
              "         ...,\n",
              "         [204, 140, 126],\n",
              "         [203, 137, 122],\n",
              "         [201, 135, 119]],\n",
              "\n",
              "        [[193, 139, 127],\n",
              "         [204, 143, 128],\n",
              "         [203, 140, 122],\n",
              "         ...,\n",
              "         [200, 143, 128],\n",
              "         [203, 142, 127],\n",
              "         [205, 140, 124]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[203, 158, 144],\n",
              "         [217, 170, 151],\n",
              "         [216, 168, 149],\n",
              "         ...,\n",
              "         [215, 167, 161],\n",
              "         [214, 167, 161],\n",
              "         [214, 164, 157]],\n",
              "\n",
              "        [[204, 157, 142],\n",
              "         [217, 165, 146],\n",
              "         [216, 165, 144],\n",
              "         ...,\n",
              "         [214, 167, 161],\n",
              "         [214, 165, 160],\n",
              "         [213, 164, 156]],\n",
              "\n",
              "        [[202, 157, 141],\n",
              "         [216, 164, 146],\n",
              "         [215, 164, 143],\n",
              "         ...,\n",
              "         [212, 164, 159],\n",
              "         [213, 164, 158],\n",
              "         [211, 162, 153]]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = preprocess_image_input(X_train, the_arch)\n",
        "X_val = preprocess_image_input(X_val, the_arch)"
      ],
      "metadata": {
        "id": "XT95XFaHQD6d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiHvdJzuT7ZI",
        "outputId": "ca876e56-f31a-4a79-d8af-477c7bd4e0f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[ 30.060997  ,   2.2210007 ,  84.32      ],\n",
              "         [ 29.060997  ,   3.2210007 ,  82.32      ],\n",
              "         [ 30.060997  ,   5.2210007 ,  83.32      ],\n",
              "         ...,\n",
              "         [ 42.060997  ,  13.221001  ,  88.32      ],\n",
              "         [ 42.060997  ,  12.221001  ,  87.32      ],\n",
              "         [ 41.060997  ,  11.221001  ,  87.32      ]],\n",
              "\n",
              "        [[ 26.060997  ,  -3.7789993 ,  80.32      ],\n",
              "         [ 31.060997  ,   2.2210007 ,  83.32      ],\n",
              "         [ 35.060997  ,   5.2210007 ,  87.32      ],\n",
              "         ...,\n",
              "         [ 44.060997  ,  15.221001  ,  86.32      ],\n",
              "         [ 40.060997  ,  13.221001  ,  85.32      ],\n",
              "         [ 38.060997  ,  11.221001  ,  86.32      ]],\n",
              "\n",
              "        [[ 27.060997  ,  -3.7789993 ,  80.32      ],\n",
              "         [ 30.060997  ,   0.22100067,  83.32      ],\n",
              "         [ 34.060997  ,   4.2210007 ,  87.32      ],\n",
              "         ...,\n",
              "         [ 43.060997  ,  14.221001  ,  88.32      ],\n",
              "         [ 42.060997  ,  13.221001  ,  88.32      ],\n",
              "         [ 39.060997  ,   9.221001  ,  85.32      ]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 16.060997  ,   2.2210007 ,  65.32      ],\n",
              "         [ 20.060997  ,   5.2210007 ,  67.32      ],\n",
              "         [ 22.060997  ,   7.2210007 ,  69.32      ],\n",
              "         ...,\n",
              "         [ 23.060997  ,   9.221001  ,  67.32      ],\n",
              "         [ 24.060997  ,   8.221001  ,  67.32      ],\n",
              "         [ 23.060997  ,   4.2210007 ,  63.32      ]],\n",
              "\n",
              "        [[ 17.060997  ,   2.2210007 ,  67.32      ],\n",
              "         [ 17.060997  ,   2.2210007 ,  66.32      ],\n",
              "         [ 21.060997  ,   3.2210007 ,  68.32      ],\n",
              "         ...,\n",
              "         [ 24.060997  ,   6.2210007 ,  65.32      ],\n",
              "         [ 26.060997  ,   6.2210007 ,  67.32      ],\n",
              "         [ 25.060997  ,   7.2210007 ,  66.32      ]],\n",
              "\n",
              "        [[ 14.060997  ,  -0.7789993 ,  67.32      ],\n",
              "         [ 15.060997  ,  -2.7789993 ,  66.32      ],\n",
              "         [ 17.060997  ,  -0.7789993 ,  67.32      ],\n",
              "         ...,\n",
              "         [ 21.060997  ,   4.2210007 ,  66.32      ],\n",
              "         [ 23.060997  ,   5.2210007 ,  66.32      ],\n",
              "         [ 27.060997  ,   7.2210007 ,  67.32      ]]],\n",
              "\n",
              "\n",
              "       [[[-65.939     , -82.779     , -52.68      ],\n",
              "         [-64.939     , -84.779     , -51.68      ],\n",
              "         [-65.939     , -84.779     , -52.68      ],\n",
              "         ...,\n",
              "         [-70.939     , -87.779     , -45.68      ],\n",
              "         [-71.939     , -86.779     , -48.68      ],\n",
              "         [-71.939     , -86.779     , -46.68      ]],\n",
              "\n",
              "        [[-67.939     , -84.779     , -52.68      ],\n",
              "         [-66.939     , -83.779     , -51.68      ],\n",
              "         [-65.939     , -84.779     , -51.68      ],\n",
              "         ...,\n",
              "         [-68.939     , -86.779     , -45.68      ],\n",
              "         [-71.939     , -86.779     , -47.68      ],\n",
              "         [-70.939     , -86.779     , -47.68      ]],\n",
              "\n",
              "        [[-66.939     , -83.779     , -50.68      ],\n",
              "         [-65.939     , -84.779     , -53.68      ],\n",
              "         [-66.939     , -85.779     , -53.68      ],\n",
              "         ...,\n",
              "         [-68.939     , -87.779     , -47.68      ],\n",
              "         [-70.939     , -85.779     , -47.68      ],\n",
              "         [-70.939     , -85.779     , -46.68      ]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-59.939003  , -82.779     , -46.68      ],\n",
              "         [-60.939003  , -81.779     , -43.68      ],\n",
              "         [-63.939003  , -82.779     , -43.68      ],\n",
              "         ...,\n",
              "         [-61.939003  , -85.779     , -50.68      ],\n",
              "         [-60.939003  , -84.779     , -50.68      ],\n",
              "         [-61.939003  , -84.779     , -53.68      ]],\n",
              "\n",
              "        [[-62.939003  , -82.779     , -47.68      ],\n",
              "         [-61.939003  , -81.779     , -46.68      ],\n",
              "         [-63.939003  , -82.779     , -46.68      ],\n",
              "         ...,\n",
              "         [-60.939003  , -84.779     , -51.68      ],\n",
              "         [-59.939003  , -84.779     , -51.68      ],\n",
              "         [-61.939003  , -84.779     , -52.68      ]],\n",
              "\n",
              "        [[-64.939     , -82.779     , -46.68      ],\n",
              "         [-64.939     , -82.779     , -47.68      ],\n",
              "         [-64.939     , -82.779     , -48.68      ],\n",
              "         ...,\n",
              "         [-58.939003  , -84.779     , -51.68      ],\n",
              "         [-57.939003  , -85.779     , -51.68      ],\n",
              "         [-61.939003  , -84.779     , -51.68      ]]],\n",
              "\n",
              "\n",
              "       [[[ 71.061     ,  39.221     , 120.32      ],\n",
              "         [ 71.061     ,  40.221     , 120.32      ],\n",
              "         [ 73.061     ,  41.221     , 121.32      ],\n",
              "         ...,\n",
              "         [ 71.061     ,  47.221     , 115.32      ],\n",
              "         [ 71.061     ,  47.221     , 117.32      ],\n",
              "         [ 70.061     ,  47.221     , 117.32      ]],\n",
              "\n",
              "        [[ 77.061     ,  42.221     , 122.32      ],\n",
              "         [ 76.061     ,  41.221     , 121.32      ],\n",
              "         [ 75.061     ,  41.221     , 121.32      ],\n",
              "         ...,\n",
              "         [ 71.061     ,  48.221     , 117.32      ],\n",
              "         [ 73.061     ,  48.221     , 117.32      ],\n",
              "         [ 72.061     ,  46.221     , 117.32      ]],\n",
              "\n",
              "        [[ 76.061     ,  44.221     , 123.32      ],\n",
              "         [ 75.061     ,  44.221     , 122.32      ],\n",
              "         [ 76.061     ,  44.221     , 121.32      ],\n",
              "         ...,\n",
              "         [ 72.061     ,  48.221     , 117.32      ],\n",
              "         [ 73.061     ,  47.221     , 118.32      ],\n",
              "         [ 73.061     ,  46.221     , 116.32      ]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 46.060997  ,  27.221     ,  98.32      ],\n",
              "         [ 47.060997  ,  29.221     , 100.32      ],\n",
              "         [ 48.060997  ,  32.221     , 100.32      ],\n",
              "         ...,\n",
              "         [ 49.060997  ,  39.221     , 100.32      ],\n",
              "         [ 55.060997  ,  42.221     , 103.32      ],\n",
              "         [ 53.060997  ,  41.221     , 101.32      ]],\n",
              "\n",
              "        [[ 45.060997  ,  27.221     ,  95.32      ],\n",
              "         [ 47.060997  ,  29.221     ,  98.32      ],\n",
              "         [ 49.060997  ,  31.221     , 102.32      ],\n",
              "         ...,\n",
              "         [ 52.060997  ,  40.221     , 102.32      ],\n",
              "         [ 51.060997  ,  43.221     , 100.32      ],\n",
              "         [ 49.060997  ,  41.221     ,  99.32      ]],\n",
              "\n",
              "        [[ 47.060997  ,  31.221     ,  97.32      ],\n",
              "         [ 46.060997  ,  30.221     ,  99.32      ],\n",
              "         [ 48.060997  ,  30.221     , 100.32      ],\n",
              "         ...,\n",
              "         [ 51.060997  ,  42.221     , 102.32      ],\n",
              "         [ 53.060997  ,  42.221     , 100.32      ],\n",
              "         [ 50.060997  ,  40.221     ,  99.32      ]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[ 60.060997  ,  28.221     ,  35.32      ],\n",
              "         [ 61.060997  ,  28.221     ,  36.32      ],\n",
              "         [ 61.060997  ,  30.221     ,  34.32      ],\n",
              "         ...,\n",
              "         [ 53.060997  ,  27.221     ,  33.32      ],\n",
              "         [ 56.060997  ,  26.221     ,  34.32      ],\n",
              "         [ 54.060997  ,  31.221     ,  34.32      ]],\n",
              "\n",
              "        [[ 65.061     ,  30.221     ,  34.32      ],\n",
              "         [ 65.061     ,  30.221     ,  34.32      ],\n",
              "         [ 62.060997  ,  30.221     ,  34.32      ],\n",
              "         ...,\n",
              "         [ 55.060997  ,  31.221     ,  32.32      ],\n",
              "         [ 59.060997  ,  36.221     ,  32.32      ],\n",
              "         [ 57.060997  ,  35.221     ,  34.32      ]],\n",
              "\n",
              "        [[ 63.060997  ,  30.221     ,  33.32      ],\n",
              "         [ 65.061     ,  32.221     ,  33.32      ],\n",
              "         [ 64.061     ,  32.221     ,  34.32      ],\n",
              "         ...,\n",
              "         [ 61.060997  ,  35.221     ,  36.32      ],\n",
              "         [ 63.060997  ,  37.221     ,  35.32      ],\n",
              "         [ 62.060997  ,  37.221     ,  33.32      ]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 91.061     ,  58.221     ,  60.32      ],\n",
              "         [ 89.061     ,  58.221     ,  59.32      ],\n",
              "         [ 87.061     ,  53.221     ,  60.32      ],\n",
              "         ...,\n",
              "         [ 75.061     ,  48.221     ,  46.32      ],\n",
              "         [ 74.061     ,  46.221     ,  46.32      ],\n",
              "         [ 73.061     ,  47.221     ,  46.32      ]],\n",
              "\n",
              "        [[ 89.061     ,  58.221     ,  61.32      ],\n",
              "         [ 89.061     ,  57.221     ,  59.32      ],\n",
              "         [ 89.061     ,  55.221     ,  61.32      ],\n",
              "         ...,\n",
              "         [ 72.061     ,  46.221     ,  45.32      ],\n",
              "         [ 72.061     ,  44.221     ,  45.32      ],\n",
              "         [ 72.061     ,  43.221     ,  45.32      ]],\n",
              "\n",
              "        [[ 90.061     ,  56.221     ,  63.32      ],\n",
              "         [ 89.061     ,  58.221     ,  65.32      ],\n",
              "         [ 94.061     ,  61.221     ,  65.32      ],\n",
              "         ...,\n",
              "         [ 70.061     ,  44.221     ,  45.32      ],\n",
              "         [ 72.061     ,  43.221     ,  45.32      ],\n",
              "         [ 69.061     ,  42.221     ,  44.32      ]]],\n",
              "\n",
              "\n",
              "       [[[-50.939003  , -67.779     , -47.68      ],\n",
              "         [-46.939003  , -62.779     , -45.68      ],\n",
              "         [-45.939003  , -56.779     , -43.68      ],\n",
              "         ...,\n",
              "         [-55.939003  , -62.779     , -46.68      ],\n",
              "         [-55.939003  , -68.779     , -46.68      ],\n",
              "         [-57.939003  , -71.779     , -50.68      ]],\n",
              "\n",
              "        [[-52.939003  , -62.779     , -46.68      ],\n",
              "         [-44.939003  , -60.779     , -40.68      ],\n",
              "         [-46.939003  , -56.779     , -38.68      ],\n",
              "         ...,\n",
              "         [-58.939003  , -61.779     , -43.68      ],\n",
              "         [-55.939003  , -67.779     , -45.68      ],\n",
              "         [-58.939003  , -69.779     , -48.68      ]],\n",
              "\n",
              "        [[-46.939003  , -59.779     , -45.68      ],\n",
              "         [-42.939003  , -56.779     , -40.68      ],\n",
              "         [-39.939003  , -54.779     , -35.68      ],\n",
              "         ...,\n",
              "         [-54.939003  , -59.779     , -41.68      ],\n",
              "         [-53.939003  , -64.779     , -43.68      ],\n",
              "         [-54.939003  , -67.779     , -47.68      ]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-29.939003  , -41.779     , -18.68      ],\n",
              "         [-23.939003  , -39.779     , -16.68      ],\n",
              "         [-25.939003  , -37.779     , -15.68      ],\n",
              "         ...,\n",
              "         [-33.939003  , -49.779     , -27.68      ],\n",
              "         [-35.939003  , -53.779     , -30.68      ],\n",
              "         [-37.939003  , -54.779     , -32.68      ]],\n",
              "\n",
              "        [[-29.939003  , -41.779     , -19.68      ],\n",
              "         [-26.939003  , -40.779     , -16.68      ],\n",
              "         [-25.939003  , -38.779     , -14.68      ],\n",
              "         ...,\n",
              "         [-34.939003  , -52.779     , -29.68      ],\n",
              "         [-36.939003  , -52.779     , -31.68      ],\n",
              "         [-38.939003  , -53.779     , -33.68      ]],\n",
              "\n",
              "        [[-32.939003  , -42.779     , -22.68      ],\n",
              "         [-23.939003  , -40.779     , -18.68      ],\n",
              "         [-22.939003  , -38.779     , -16.68      ],\n",
              "         ...,\n",
              "         [-38.939003  , -53.779     , -30.68      ],\n",
              "         [-37.939003  , -52.779     , -32.68      ],\n",
              "         [-38.939003  , -52.779     , -36.68      ]]],\n",
              "\n",
              "\n",
              "       [[[ 78.061     ,  50.221     ,  62.32      ],\n",
              "         [ 80.061     ,  54.221     ,  66.32      ],\n",
              "         [ 81.061     ,  55.221     ,  67.32      ],\n",
              "         ...,\n",
              "         [ 55.060997  ,  21.221     ,  32.32      ],\n",
              "         [ 55.060997  ,  19.221     ,  27.32      ],\n",
              "         [ 54.060997  ,  16.221     ,  24.32      ]],\n",
              "\n",
              "        [[ 81.061     ,  49.221     ,  62.32      ],\n",
              "         [ 83.061     ,  55.221     ,  66.32      ],\n",
              "         [ 84.061     ,  57.221     ,  71.32      ],\n",
              "         ...,\n",
              "         [ 56.060997  ,  22.221     ,  34.32      ],\n",
              "         [ 60.060997  ,  21.221     ,  30.32      ],\n",
              "         [ 56.060997  ,  19.221     ,  26.32      ]],\n",
              "\n",
              "        [[ 85.061     ,  54.221     ,  65.32      ],\n",
              "         [ 86.061     ,  55.221     ,  68.32      ],\n",
              "         [ 88.061     ,  58.221     ,  68.32      ],\n",
              "         ...,\n",
              "         [ 59.060997  ,  26.221     ,  34.32      ],\n",
              "         [ 61.060997  ,  26.221     ,  32.32      ],\n",
              "         [ 58.060997  ,  22.221     ,  27.32      ]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 95.061     ,  65.221     ,  75.32      ],\n",
              "         [ 94.061     ,  66.221     ,  77.32      ],\n",
              "         [ 93.061     ,  69.221     ,  78.32      ],\n",
              "         ...,\n",
              "         [ 46.060997  ,  20.221     ,  35.32      ],\n",
              "         [ 41.060997  ,  18.221     ,  33.32      ],\n",
              "         [ 37.060997  ,  11.221001  ,  30.32      ]],\n",
              "\n",
              "        [[ 89.061     ,  62.221     ,  72.32      ],\n",
              "         [ 91.061     ,  65.221     ,  75.32      ],\n",
              "         [ 93.061     ,  68.221     ,  78.32      ],\n",
              "         ...,\n",
              "         [ 44.060997  ,  19.221     ,  34.32      ],\n",
              "         [ 39.060997  ,  16.221     ,  29.32      ],\n",
              "         [ 35.060997  ,  10.221001  ,  28.32      ]],\n",
              "\n",
              "        [[ 84.061     ,  56.221     ,  69.32      ],\n",
              "         [ 88.061     ,  61.221     ,  72.32      ],\n",
              "         [ 91.061     ,  64.221     ,  79.32      ],\n",
              "         ...,\n",
              "         [ 44.060997  ,  17.221     ,  36.32      ],\n",
              "         [ 37.060997  ,  12.221001  ,  30.32      ],\n",
              "         [ 33.060997  ,  10.221001  ,  25.32      ]]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAvkZGGOT9B2",
        "outputId": "993a4059-8224-437d-d988-d8a9a9ba384e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[-7.2939003e+01, -9.2778999e+01, -8.8680000e+01],\n",
              "         [-6.5939003e+01, -8.5778999e+01, -8.0680000e+01],\n",
              "         [-5.5939003e+01, -7.5778999e+01, -7.0680000e+01],\n",
              "         ...,\n",
              "         [-6.7939003e+01, -8.0778999e+01, -7.2680000e+01],\n",
              "         [-6.6939003e+01, -7.9778999e+01, -7.1680000e+01],\n",
              "         [-6.4939003e+01, -7.7778999e+01, -6.9680000e+01]],\n",
              "\n",
              "        [[-6.7939003e+01, -8.8778999e+01, -8.3680000e+01],\n",
              "         [-6.0939003e+01, -7.9778999e+01, -7.4680000e+01],\n",
              "         [-5.3939003e+01, -7.0778999e+01, -6.3680000e+01],\n",
              "         ...,\n",
              "         [-6.6939003e+01, -8.1778999e+01, -7.2680000e+01],\n",
              "         [-6.5939003e+01, -7.8778999e+01, -7.1680000e+01],\n",
              "         [-6.4939003e+01, -7.8778999e+01, -7.0680000e+01]],\n",
              "\n",
              "        [[-6.3939003e+01, -8.2778999e+01, -7.7680000e+01],\n",
              "         [-5.6939003e+01, -7.4778999e+01, -6.7680000e+01],\n",
              "         [-4.8939003e+01, -6.4778999e+01, -5.8680000e+01],\n",
              "         ...,\n",
              "         [-6.6939003e+01, -8.2778999e+01, -7.1680000e+01],\n",
              "         [-6.6939003e+01, -7.9778999e+01, -7.2680000e+01],\n",
              "         [-6.5939003e+01, -7.8778999e+01, -7.1680000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-6.4939003e+01, -8.3778999e+01, -6.9680000e+01],\n",
              "         [-6.6939003e+01, -8.4778999e+01, -7.1680000e+01],\n",
              "         [-6.4939003e+01, -8.3778999e+01, -7.1680000e+01],\n",
              "         ...,\n",
              "         [-8.5939003e+01, -1.0377900e+02, -1.0168000e+02],\n",
              "         [-8.7939003e+01, -1.0477900e+02, -1.0168000e+02],\n",
              "         [-8.8939003e+01, -1.0377900e+02, -1.0168000e+02]],\n",
              "\n",
              "        [[-6.3939003e+01, -8.2778999e+01, -6.8680000e+01],\n",
              "         [-6.4939003e+01, -8.2778999e+01, -6.9680000e+01],\n",
              "         [-6.4939003e+01, -8.3778999e+01, -7.0680000e+01],\n",
              "         ...,\n",
              "         [-8.7939003e+01, -1.0377900e+02, -1.0168000e+02],\n",
              "         [-8.8939003e+01, -1.0477900e+02, -1.0268000e+02],\n",
              "         [-8.7939003e+01, -1.0377900e+02, -1.0168000e+02]],\n",
              "\n",
              "        [[-6.0939003e+01, -8.0778999e+01, -6.7680000e+01],\n",
              "         [-6.1939003e+01, -8.0778999e+01, -6.9680000e+01],\n",
              "         [-6.5939003e+01, -8.3778999e+01, -6.9680000e+01],\n",
              "         ...,\n",
              "         [-8.8939003e+01, -1.0377900e+02, -1.0168000e+02],\n",
              "         [-8.8939003e+01, -1.0477900e+02, -1.0268000e+02],\n",
              "         [-8.7939003e+01, -1.0377900e+02, -1.0168000e+02]]],\n",
              "\n",
              "\n",
              "       [[[ 2.0060997e+01,  1.2221001e+01,  8.4320000e+01],\n",
              "         [ 2.9060997e+01,  1.7221001e+01,  8.6320000e+01],\n",
              "         [ 4.0060997e+01,  2.4221001e+01,  9.0320000e+01],\n",
              "         ...,\n",
              "         [ 3.9060997e+01,  2.6221001e+01,  8.3320000e+01],\n",
              "         [ 4.0060997e+01,  2.4221001e+01,  8.5320000e+01],\n",
              "         [ 4.1060997e+01,  2.2221001e+01,  8.7320000e+01]],\n",
              "\n",
              "        [[ 2.2060997e+01,  1.5221001e+01,  8.4320000e+01],\n",
              "         [ 3.0060997e+01,  1.9221001e+01,  8.7320000e+01],\n",
              "         [ 3.6060997e+01,  2.5221001e+01,  9.0320000e+01],\n",
              "         ...,\n",
              "         [ 4.1060997e+01,  2.8221001e+01,  8.6320000e+01],\n",
              "         [ 3.8060997e+01,  2.3221001e+01,  8.4320000e+01],\n",
              "         [ 3.9060997e+01,  2.2221001e+01,  8.6320000e+01]],\n",
              "\n",
              "        [[ 2.9060997e+01,  1.7221001e+01,  8.4320000e+01],\n",
              "         [ 3.1060997e+01,  2.1221001e+01,  8.9320000e+01],\n",
              "         [ 3.5060997e+01,  2.6221001e+01,  9.1320000e+01],\n",
              "         ...,\n",
              "         [ 4.1060997e+01,  2.5221001e+01,  8.4320000e+01],\n",
              "         [ 3.8060997e+01,  2.4221001e+01,  8.3320000e+01],\n",
              "         [ 3.5060997e+01,  2.1221001e+01,  8.4320000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 7.0609970e+00,  1.2221001e+01,  6.9320000e+01],\n",
              "         [-2.9390030e+00,  3.2210007e+00,  6.5320000e+01],\n",
              "         [-8.9390030e+00,  2.2100067e-01,  6.2320000e+01],\n",
              "         ...,\n",
              "         [ 1.3060997e+01,  7.2210007e+00,  6.4320000e+01],\n",
              "         [ 1.3060997e+01,  1.0221001e+01,  6.3320000e+01],\n",
              "         [ 1.3060997e+01,  9.2210007e+00,  6.3320000e+01]],\n",
              "\n",
              "        [[ 6.0609970e+00,  1.0221001e+01,  6.9320000e+01],\n",
              "         [-9.3900299e-01,  4.2210007e+00,  6.7320000e+01],\n",
              "         [-8.9390030e+00, -1.7789993e+00,  6.4320000e+01],\n",
              "         ...,\n",
              "         [ 1.1060997e+01,  7.2210007e+00,  6.2320000e+01],\n",
              "         [ 1.0060997e+01,  9.2210007e+00,  5.9320000e+01],\n",
              "         [ 1.1060997e+01,  8.2210007e+00,  6.1320000e+01]],\n",
              "\n",
              "        [[ 6.0609970e+00,  9.2210007e+00,  6.9320000e+01],\n",
              "         [ 6.0997009e-02,  4.2210007e+00,  6.5320000e+01],\n",
              "         [-3.9390030e+00,  1.2210007e+00,  6.3320000e+01],\n",
              "         ...,\n",
              "         [ 1.1060997e+01,  7.2210007e+00,  6.3320000e+01],\n",
              "         [ 9.0609970e+00,  7.2210007e+00,  6.0320000e+01],\n",
              "         [ 9.0609970e+00,  7.2210007e+00,  6.1320000e+01]]],\n",
              "\n",
              "\n",
              "       [[[ 2.2060997e+01,  1.0221001e+01,  4.6320000e+01],\n",
              "         [ 2.6060997e+01,  1.3221001e+01,  4.9320000e+01],\n",
              "         [ 3.5060997e+01,  2.3221001e+01,  5.1320000e+01],\n",
              "         ...,\n",
              "         [ 1.7060997e+01,  1.0221001e+01,  4.8320000e+01],\n",
              "         [ 1.7060997e+01,  1.0221001e+01,  4.9320000e+01],\n",
              "         [ 2.1060997e+01,  1.2221001e+01,  5.1320000e+01]],\n",
              "\n",
              "        [[ 2.8060997e+01,  1.6221001e+01,  4.9320000e+01],\n",
              "         [ 3.2060997e+01,  1.9221001e+01,  5.1320000e+01],\n",
              "         [ 4.0060997e+01,  2.5221001e+01,  5.3320000e+01],\n",
              "         ...,\n",
              "         [ 2.0060997e+01,  1.0221001e+01,  4.7320000e+01],\n",
              "         [ 1.8060997e+01,  1.0221001e+01,  4.9320000e+01],\n",
              "         [ 2.1060997e+01,  1.2221001e+01,  5.1320000e+01]],\n",
              "\n",
              "        [[ 2.9060997e+01,  2.3221001e+01,  5.2320000e+01],\n",
              "         [ 2.9060997e+01,  2.2221001e+01,  5.2320000e+01],\n",
              "         [ 4.1060997e+01,  2.6221001e+01,  5.4320000e+01],\n",
              "         ...,\n",
              "         [ 2.5060997e+01,  1.2221001e+01,  4.9320000e+01],\n",
              "         [ 2.6060997e+01,  1.4221001e+01,  4.9320000e+01],\n",
              "         [ 2.9060997e+01,  1.7221001e+01,  5.1320000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 3.0060997e+01,  2.4221001e+01,  5.7320000e+01],\n",
              "         [ 3.9060997e+01,  3.1221001e+01,  6.3320000e+01],\n",
              "         [ 4.1060997e+01,  3.3221001e+01,  6.5320000e+01],\n",
              "         ...,\n",
              "         [ 3.6060997e+01,  1.4221001e+01,  4.5320000e+01],\n",
              "         [ 2.6060997e+01,  9.2210007e+00,  4.1320000e+01],\n",
              "         [ 2.0060997e+01,  3.2210007e+00,  4.2320000e+01]],\n",
              "\n",
              "        [[ 3.7060997e+01,  2.9221001e+01,  5.9320000e+01],\n",
              "         [ 4.5060997e+01,  3.8221001e+01,  6.5320000e+01],\n",
              "         [ 4.9060997e+01,  3.9221001e+01,  6.8320000e+01],\n",
              "         ...,\n",
              "         [ 3.5060997e+01,  1.3221001e+01,  4.5320000e+01],\n",
              "         [ 2.6060997e+01,  8.2210007e+00,  4.0320000e+01],\n",
              "         [ 2.1060997e+01,  6.2210007e+00,  4.4320000e+01]],\n",
              "\n",
              "        [[ 4.5060997e+01,  3.4221001e+01,  6.1320000e+01],\n",
              "         [ 5.0060997e+01,  4.0221001e+01,  6.6320000e+01],\n",
              "         [ 5.7060997e+01,  4.6221001e+01,  6.9320000e+01],\n",
              "         ...,\n",
              "         [ 3.2060997e+01,  1.0221001e+01,  4.2320000e+01],\n",
              "         [ 2.4060997e+01,  5.2210007e+00,  3.9320000e+01],\n",
              "         [ 2.0060997e+01,  6.2210007e+00,  4.2320000e+01]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[ 9.6060997e+01,  6.4221001e+01,  5.8320000e+01],\n",
              "         [ 1.0006100e+02,  6.6221001e+01,  5.8320000e+01],\n",
              "         [ 1.0106100e+02,  6.7221001e+01,  6.0320000e+01],\n",
              "         ...,\n",
              "         [ 9.5060997e+01,  6.7221001e+01,  6.2320000e+01],\n",
              "         [ 9.5060997e+01,  6.9221001e+01,  6.1320000e+01],\n",
              "         [ 9.7060997e+01,  6.9221001e+01,  6.2320000e+01]],\n",
              "\n",
              "        [[ 9.6060997e+01,  6.6221001e+01,  5.8320000e+01],\n",
              "         [ 9.8060997e+01,  6.6221001e+01,  5.7320000e+01],\n",
              "         [ 1.0006100e+02,  6.8221001e+01,  6.0320000e+01],\n",
              "         ...,\n",
              "         [ 9.5060997e+01,  7.0221001e+01,  6.2320000e+01],\n",
              "         [ 9.6060997e+01,  6.8221001e+01,  6.3320000e+01],\n",
              "         [ 9.6060997e+01,  6.8221001e+01,  6.4320000e+01]],\n",
              "\n",
              "        [[ 9.9060997e+01,  6.8221001e+01,  5.8320000e+01],\n",
              "         [ 1.0006100e+02,  6.8221001e+01,  5.9320000e+01],\n",
              "         [ 1.0006100e+02,  6.8221001e+01,  5.9320000e+01],\n",
              "         ...,\n",
              "         [ 9.4060997e+01,  6.9221001e+01,  6.3320000e+01],\n",
              "         [ 9.6060997e+01,  6.8221001e+01,  6.2320000e+01],\n",
              "         [ 9.6060997e+01,  6.9221001e+01,  6.3320000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 9.6060997e+01,  6.9221001e+01,  6.1320000e+01],\n",
              "         [ 9.7060997e+01,  7.1221001e+01,  6.1320000e+01],\n",
              "         [ 9.5060997e+01,  6.9221001e+01,  5.8320000e+01],\n",
              "         ...,\n",
              "         [ 1.0306100e+02,  7.3221001e+01,  6.7320000e+01],\n",
              "         [ 1.0206100e+02,  7.2221001e+01,  6.6320000e+01],\n",
              "         [ 1.0406100e+02,  7.3221001e+01,  6.6320000e+01]],\n",
              "\n",
              "        [[ 9.5060997e+01,  6.9221001e+01,  6.0320000e+01],\n",
              "         [ 9.4060997e+01,  6.9221001e+01,  6.0320000e+01],\n",
              "         [ 9.4060997e+01,  6.8221001e+01,  5.9320000e+01],\n",
              "         ...,\n",
              "         [ 1.0106100e+02,  7.1221001e+01,  6.6320000e+01],\n",
              "         [ 1.0206100e+02,  7.1221001e+01,  6.8320000e+01],\n",
              "         [ 1.0306100e+02,  7.1221001e+01,  6.7320000e+01]],\n",
              "\n",
              "        [[ 9.5060997e+01,  6.8221001e+01,  6.0320000e+01],\n",
              "         [ 9.3060997e+01,  6.7221001e+01,  5.9320000e+01],\n",
              "         [ 9.4060997e+01,  6.8221001e+01,  5.8320000e+01],\n",
              "         ...,\n",
              "         [ 1.0106100e+02,  7.0221001e+01,  6.4320000e+01],\n",
              "         [ 1.0106100e+02,  7.2221001e+01,  6.6320000e+01],\n",
              "         [ 1.0106100e+02,  7.4221001e+01,  6.6320000e+01]]],\n",
              "\n",
              "\n",
              "       [[[ 4.5060997e+01,  1.0221001e+01,  2.6320000e+01],\n",
              "         [ 4.7060997e+01,  9.2210007e+00,  2.6320000e+01],\n",
              "         [ 4.7060997e+01,  9.2210007e+00,  2.6320000e+01],\n",
              "         ...,\n",
              "         [ 6.1060997e+01,  2.2221001e+01,  3.8320000e+01],\n",
              "         [ 6.1060997e+01,  2.3221001e+01,  3.5320000e+01],\n",
              "         [ 6.0060997e+01,  2.5221001e+01,  3.4320000e+01]],\n",
              "\n",
              "        [[ 4.6060997e+01,  1.0221001e+01,  2.6320000e+01],\n",
              "         [ 4.9060997e+01,  9.2210007e+00,  2.6320000e+01],\n",
              "         [ 4.8060997e+01,  9.2210007e+00,  2.7320000e+01],\n",
              "         ...,\n",
              "         [ 6.1060997e+01,  2.2221001e+01,  3.6320000e+01],\n",
              "         [ 6.2060997e+01,  2.1221001e+01,  3.6320000e+01],\n",
              "         [ 6.0060997e+01,  2.3221001e+01,  3.4320000e+01]],\n",
              "\n",
              "        [[ 4.8060997e+01,  9.2210007e+00,  2.7320000e+01],\n",
              "         [ 4.8060997e+01,  1.0221001e+01,  2.6320000e+01],\n",
              "         [ 4.8060997e+01,  1.0221001e+01,  2.7320000e+01],\n",
              "         ...,\n",
              "         [ 6.1060997e+01,  2.1221001e+01,  3.5320000e+01],\n",
              "         [ 6.0060997e+01,  1.9221001e+01,  3.6320000e+01],\n",
              "         [ 6.0060997e+01,  1.9221001e+01,  3.5320000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 4.7060997e+01,  4.2210007e+00,  2.3320000e+01],\n",
              "         [ 4.6060997e+01,  4.2210007e+00,  2.2320000e+01],\n",
              "         [ 4.6060997e+01,  3.2210007e+00,  2.0320000e+01],\n",
              "         ...,\n",
              "         [ 5.2060997e+01,  1.8221001e+01,  3.4320000e+01],\n",
              "         [ 5.3060997e+01,  1.8221001e+01,  3.5320000e+01],\n",
              "         [ 5.4060997e+01,  1.8221001e+01,  3.3320000e+01]],\n",
              "\n",
              "        [[ 4.6060997e+01, -7.7899933e-01,  1.8320000e+01],\n",
              "         [ 4.5060997e+01,  2.2210007e+00,  2.1320000e+01],\n",
              "         [ 4.5060997e+01,  3.2210007e+00,  1.9320000e+01],\n",
              "         ...,\n",
              "         [ 5.3060997e+01,  1.8221001e+01,  3.5320000e+01],\n",
              "         [ 5.2060997e+01,  1.8221001e+01,  3.3320000e+01],\n",
              "         [ 5.2060997e+01,  1.6221001e+01,  3.2320000e+01]],\n",
              "\n",
              "        [[ 4.5060997e+01, -3.7789993e+00,  1.8320000e+01],\n",
              "         [ 4.5060997e+01,  2.2100067e-01,  2.1320000e+01],\n",
              "         [ 4.7060997e+01,  2.2210007e+00,  1.9320000e+01],\n",
              "         ...,\n",
              "         [ 5.4060997e+01,  1.9221001e+01,  3.5320000e+01],\n",
              "         [ 5.2060997e+01,  1.7221001e+01,  3.4320000e+01],\n",
              "         [ 5.2060997e+01,  1.7221001e+01,  3.3320000e+01]]],\n",
              "\n",
              "\n",
              "       [[[-8.9390030e+00, -8.7789993e+00,  4.3320000e+01],\n",
              "         [-4.9390030e+00, -1.7789993e+00,  5.4320000e+01],\n",
              "         [ 4.0609970e+00,  4.2210007e+00,  5.7320000e+01],\n",
              "         ...,\n",
              "         [ 1.3060997e+01,  1.0221001e+01,  6.1320000e+01],\n",
              "         [ 8.0609970e+00,  6.2210007e+00,  5.8320000e+01],\n",
              "         [ 9.0609970e+00,  3.2210007e+00,  5.7320000e+01]],\n",
              "\n",
              "        [[ 1.6060997e+01,  1.8221001e+01,  6.6320000e+01],\n",
              "         [ 1.0060997e+01,  1.7221001e+01,  7.4320000e+01],\n",
              "         [ 7.0609970e+00,  1.5221001e+01,  7.5320000e+01],\n",
              "         ...,\n",
              "         [ 2.2060997e+01,  2.3221001e+01,  8.0320000e+01],\n",
              "         [ 1.8060997e+01,  2.0221001e+01,  7.9320000e+01],\n",
              "         [ 1.5060997e+01,  1.8221001e+01,  7.7320000e+01]],\n",
              "\n",
              "        [[ 2.3060997e+01,  2.2221001e+01,  6.9320000e+01],\n",
              "         [ 2.4060997e+01,  2.6221001e+01,  8.0320000e+01],\n",
              "         [ 1.8060997e+01,  2.3221001e+01,  7.9320000e+01],\n",
              "         ...,\n",
              "         [ 2.4060997e+01,  2.6221001e+01,  7.6320000e+01],\n",
              "         [ 2.3060997e+01,  2.5221001e+01,  7.9320000e+01],\n",
              "         [ 2.0060997e+01,  2.3221001e+01,  8.1320000e+01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 4.0060997e+01,  4.1221001e+01,  7.9320000e+01],\n",
              "         [ 4.7060997e+01,  5.3221001e+01,  9.3320000e+01],\n",
              "         [ 4.5060997e+01,  5.1221001e+01,  9.2320000e+01],\n",
              "         ...,\n",
              "         [ 5.7060997e+01,  5.0221001e+01,  9.1320000e+01],\n",
              "         [ 5.7060997e+01,  5.0221001e+01,  9.0320000e+01],\n",
              "         [ 5.3060997e+01,  4.7221001e+01,  9.0320000e+01]],\n",
              "\n",
              "        [[ 3.8060997e+01,  4.0221001e+01,  8.0320000e+01],\n",
              "         [ 4.2060997e+01,  4.8221001e+01,  9.3320000e+01],\n",
              "         [ 4.0060997e+01,  4.8221001e+01,  9.2320000e+01],\n",
              "         ...,\n",
              "         [ 5.7060997e+01,  5.0221001e+01,  9.0320000e+01],\n",
              "         [ 5.6060997e+01,  4.8221001e+01,  9.0320000e+01],\n",
              "         [ 5.2060997e+01,  4.7221001e+01,  8.9320000e+01]],\n",
              "\n",
              "        [[ 3.7060997e+01,  4.0221001e+01,  7.8320000e+01],\n",
              "         [ 4.2060997e+01,  4.7221001e+01,  9.2320000e+01],\n",
              "         [ 3.9060997e+01,  4.7221001e+01,  9.1320000e+01],\n",
              "         ...,\n",
              "         [ 5.5060997e+01,  4.7221001e+01,  8.8320000e+01],\n",
              "         [ 5.4060997e+01,  4.7221001e+01,  8.9320000e+01],\n",
              "         [ 4.9060997e+01,  4.5221001e+01,  8.7320000e+01]]]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IncA-_o_n5w"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = y_train.value_counts(ascending=True)\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKMKSb4Bkym"
      },
      "source": [
        "Plot 3 images per label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnVuqbFBW3K"
      },
      "outputs": [],
      "source": [
        "def plot_images_per_label(df, label, cols: int, size: tuple):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=cols, figsize=size)\n",
        "\n",
        "    cntMax = cols\n",
        "    cntCur = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if(y_train == label and cntCur < cntMax):\n",
        "            axs[cntCur].imshow(plt.imread(df.FilePaths[index]))\n",
        "            axs[cntCur].set_title(df.Labels[index])\n",
        "\n",
        "            cntCur += 1\n",
        "        else:\n",
        "            if(cntCur >= cntMax):\n",
        "                break\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# unique labels\n",
        "labels = sorted(df1['y_train'].unique())\n",
        "for label in range(7):\n",
        "    plot_images_per_label(df1, label, 3, (12,9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asV1O58Lrq-R"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img = Image.fromarray(X_train[0], 'RGB')\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRKKrNacAZtl"
      },
      "source": [
        "Drop duplicate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERwfyPDHP-zC"
      },
      "outputs": [],
      "source": [
        "#df_group = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_LesionGroupings.csv') \n",
        "#df_train = df_train.set_index('image').join(df_group.set_index('image'))\n",
        "#df_train = df_train.drop_duplicates(subset=['lesion_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBXx28B9yGu"
      },
      "source": [
        "#DeepSMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmX_Uqbmj-tN"
      },
      "outputs": [],
      "source": [
        "from numpy import moveaxis\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "max_el = np.inf\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 3#1    # number of channels in the input data \n",
        "args['n_z'] = 600 #300     # number of dimensions in latent space. \n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 300       # how many epochs to run for\n",
        "args['batch_size'] = 100   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = True       # train networks if True, else load networks from\n",
        "args['patience'] = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NydOdPMajEfT"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        \n",
        "        # convolutional filters, work excellent with image data\n",
        "        # [(WK+2P)/S]+1\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AvgPool2d(7, stride=7),\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),# 16\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False), # 8\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),# 4\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 0, bias=False),#14\n",
        "            nn.BatchNorm2d(self.dim_h * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 2**3 * 7 * 7),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        # H_out = (H_in1)*stride[0]  2padding[0] + dilation[0](kernel_size[0]1) + output_padding[0] + 1\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4), #10\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4), #13\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, self.dim_h, 4),# 16\n",
        "            nn.BatchNorm2d(self.dim_h),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h, 3, 4, 2, 1),# 32\n",
        "            nn.UpsamplingBilinear2d(scale_factor=7),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 2**3, 7, 7)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def biased_get_class(X, y, c):\n",
        "    \n",
        "    xbeg = X[y == c]\n",
        "    ybeg = y[y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "    n_neigh = 5\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "def DeepSMOTE_train(X_train, y_train, one_hot = False):\n",
        "  from torch.utils.data import TensorDataset\n",
        "  import os\n",
        "\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #X_train = X_train.astype('float32') / 255.\n",
        "  \n",
        "  batch_size = args['batch_size']\n",
        "  patience = args['patience']\n",
        "  encoder = Encoder(args)\n",
        "  decoder = Decoder(args)\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  print(device)\n",
        "  decoder = decoder.to(device)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "  #decoder loss function\n",
        "  criterion = nn.MSELoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  num_workers = 0\n",
        "\n",
        "  #torch.Tensor returns float so if want long then use torch.tensor\n",
        "  tensor_x = torch.from_numpy(X_train.copy())#torch.Tensor(X_train)\n",
        "  tensor_y = torch.tensor(y_train,dtype=torch.long)\n",
        "  mnist_bal = TensorDataset(tensor_x,tensor_y) \n",
        "  train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
        "      batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
        "\n",
        "  best_loss = np.inf\n",
        "\n",
        "  enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
        "  dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
        "\n",
        "  for epoch in range(args['epochs']):\n",
        "      train_loss = 0.0\n",
        "      tmse_loss = 0.0\n",
        "      tdiscr_loss = 0.0\n",
        "      # train for one epoch -- set nets to train mode\n",
        "      encoder.train()\n",
        "      decoder.train()\n",
        "  \n",
        "      for images,labs in train_loader:\n",
        "      \n",
        "          # zero gradients for each batch\n",
        "          encoder.zero_grad()\n",
        "          decoder.zero_grad()\n",
        "          images, labs = images.to(device), labs.to(device)\n",
        "          labsn = labs.detach().cpu().numpy()\n",
        "#            print('images shape', images.shape)\n",
        "          # run images\n",
        "          z_hat = encoder(images)\n",
        "#            print('images shape after encoding', z_hat.shape)\n",
        "      \n",
        "          x_hat = decoder(z_hat) #decoder outputs tanh\n",
        "#            print('images shape after decoding', x_hat.shape)\n",
        "          mse = criterion(x_hat,images)\n",
        "                  \n",
        "          resx = []\n",
        "          resy = []\n",
        "      \n",
        "          tc = np.random.choice(num_classes,1)\n",
        "          #tc = 9\n",
        "          xbeg = X_train[y_train == tc]\n",
        "          ybeg = y_train[y_train == tc] \n",
        "          xlen = len(xbeg)\n",
        "          nsamp = min(xlen, 100)\n",
        "          ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
        "          xclass = xbeg[ind]\n",
        "          yclass = ybeg[ind]\n",
        "      \n",
        "          xclen = len(xclass)\n",
        "          xcminus = np.arange(1,xclen)\n",
        "          \n",
        "          xcplus = np.append(xcminus,0)\n",
        "          xcnew = (xclass[[xcplus],:])\n",
        "          xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
        "      \n",
        "          xcnew = torch.Tensor(xcnew)\n",
        "          xcnew = xcnew.to(device)\n",
        "      \n",
        "          #encode xclass to feature space\n",
        "          xclass = torch.Tensor(xclass)\n",
        "          xclass = xclass.to(device)\n",
        "          xclass = encoder(xclass)\n",
        "      \n",
        "          xclass = xclass.detach().cpu().numpy()\n",
        "      \n",
        "          xc_enc = (xclass[[xcplus],:])\n",
        "          xc_enc = np.squeeze(xc_enc)\n",
        "      \n",
        "          xc_enc = torch.Tensor(xc_enc)\n",
        "          xc_enc = xc_enc.to(device)\n",
        "          \n",
        "          ximg = decoder(xc_enc)\n",
        "          \n",
        "          mse2 = criterion(ximg,xcnew)\n",
        "      \n",
        "          comb_loss = mse2 + mse\n",
        "          comb_loss.backward()\n",
        "      \n",
        "          enc_optim.step()\n",
        "          dec_optim.step()\n",
        "      \n",
        "          train_loss += comb_loss.item()*images.size(0)\n",
        "          tmse_loss += mse.item()*images.size(0)\n",
        "          tdiscr_loss += mse2.item()*images.size(0)\n",
        "\n",
        "      train_loss = train_loss/len(train_loader)\n",
        "      tmse_loss = tmse_loss/len(train_loader)\n",
        "      tdiscr_loss = tdiscr_loss/len(train_loader)\n",
        "      print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
        "              train_loss,tmse_loss,tdiscr_loss))\n",
        "      \n",
        "  \n",
        "  \n",
        "      #store the best encoder and decoder models\n",
        "      #here, /crs5 is a reference to 5 way cross validation, but is not\n",
        "      #necessary for illustration purposes\n",
        "      if train_loss < best_loss:\n",
        "          print('Saving..')\n",
        "          patience = args['patience']\n",
        "          path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_enc.pth'\n",
        "          path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_dec.pth'\n",
        "        \n",
        "          torch.save(encoder.state_dict(), path_enc)\n",
        "          torch.save(decoder.state_dict(), path_dec)\n",
        "  \n",
        "          best_loss = train_loss\n",
        "      else:\n",
        "          patience = patience - 1\n",
        "\n",
        "      if patience == 0:\n",
        "          print('Out of patience. \\n')\n",
        "          break\n",
        "\n",
        "def DeepSMOTE_Data(X_train, y_train, one_hot = False):\n",
        "  batch_size = args['batch_size']\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #Generate artificial images\n",
        "  import torch\n",
        "  np.printoptions(precision=5,suppress=True)\n",
        "\n",
        "  #path on the computer where the models are stored\n",
        "  modpth = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/'\n",
        "\n",
        "  path_enc = modpth + '/bst_enc.pth'\n",
        "  path_dec = modpth + '/bst_dec.pth'\n",
        "  \n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  encoder = Encoder(args)\n",
        "  encoder.load_state_dict(torch.load(path_enc), strict=False)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  decoder = Decoder(args)\n",
        "  decoder.load_state_dict(torch.load(path_dec), strict=False)\n",
        "  decoder = decoder.to(device)\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  resx = []\n",
        "  resy = []\n",
        "  \n",
        "  counter = Counter(y_train)\n",
        "  counter = sorted(counter.items())\n",
        "  counter = [value for _, value in counter]\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "      #encode xclass to feature space\n",
        "      xclass = torch.Tensor(xclass)\n",
        "      xclass = xclass.to(device)\n",
        "      xclass = encoder(xclass)\n",
        "          \n",
        "      xclass = xclass.detach().cpu().numpy()\n",
        "      n = np.max(counter) - counter[i]\n",
        "      if n == 0:\n",
        "        continue\n",
        "#        resx2 = []\n",
        "#        resy2 = []\n",
        "#        for j in range(batch_size, n+batch_size+1, batch_size):\n",
        "#          if j <= n:\n",
        "#            batch_size_max = batch_size\n",
        "#          elif n % batch_size != 0:\n",
        "#            batch_size_max = n%batch_size\n",
        "#          else:\n",
        "#            break\n",
        "#          xsamp, ysamp = G_SM(xclass,yclass,batch_size_max,i)\n",
        "      xsamp, ysamp = G_SM(xclass,yclass,n,i)\n",
        "      ysamp = np.array(ysamp)\n",
        "  \n",
        "      \"\"\"to generate samples for resnet\"\"\"   \n",
        "      xsamp = torch.Tensor(xsamp)\n",
        "      xsamp = xsamp.to(device)\n",
        "      ximg = decoder(xsamp)\n",
        "\n",
        "      ximn = ximg.detach().cpu().numpy()\n",
        "#        resx2.append(ximn)\n",
        "#        resy2.append(ysamp)\n",
        "#        \n",
        "#        resx2 = np.vstack(resx2)\n",
        "#        resy2 = np.hstack(resy2)\n",
        "      resx.append(ximn)\n",
        "      resy.append(ysamp)\n",
        "  \n",
        "  resx1 = np.vstack(resx)\n",
        "  resy1 = np.hstack(resy)\n",
        "  resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  X_train = X_train.reshape(X_train.shape[0],-1)\n",
        "  X_train = np.vstack((resx1,X_train))\n",
        "  y_train = np.hstack((resy1,y_train))\n",
        "  y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, 3, IMAGE_W, IMAGE_H)\n",
        "  X_train = moveaxis(X_train, 1, 3)\n",
        "  X_train = X_train * max_el\n",
        "  return X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jrJ33lUDkCM"
      },
      "source": [
        "#Split dataset to train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6qneWL_Bs2U"
      },
      "outputs": [],
      "source": [
        "# stratified train and rem (20%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Remaining Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kef4r_zxjgk"
      },
      "outputs": [],
      "source": [
        "#Data Augmentation\n",
        "dataaugment = ImageDataGenerator(\n",
        "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "        shear_range = 10) \n",
        "\n",
        "dataaugment.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PgksTFkOAq"
      },
      "source": [
        "#Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr1jnSM7yzJc"
      },
      "outputs": [],
      "source": [
        "limit = 171\n",
        "for layer in model.layers[:limit]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[limit:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "optimizer_SGD = SGD(learning_rate=0.0001, momentum=0.9)\n",
        "model.compile(optimizer = optimizer_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst2 = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,\n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1aAQBmiy0K"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst2.history['balanced_acc'])\n",
        "plt.plot(hst2.history['val_balanced_acc'])\n",
        "plt.title('model balance_acc after tunning')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "E_x4c0_DTkaa",
        "BE9FCWBe8deT",
        "iDRWiTnO0MGh",
        "eaK4zbtoaAaC",
        "3K908bbiYwbS",
        "UswA0co2y1wl",
        "LfcFpsBwM0d4",
        "cNBXx28B9yGu",
        "0jrJ33lUDkCM",
        "B2PgksTFkOAq"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}