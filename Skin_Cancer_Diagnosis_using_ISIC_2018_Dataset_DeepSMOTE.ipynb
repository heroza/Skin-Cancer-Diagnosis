{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/Skin-Cancer-Diagnosis/blob/main/Skin_Cancer_Diagnosis_using_ISIC_2018_Dataset_DeepSMOTE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUusDE1Z9TNb"
      },
      "source": [
        "Prepare the dataset. \n",
        "Currently, we use skin cancer ISIC dataset from Kaggle https://www.kaggle.com/datasets/nodoubttome/skin-cancer9-classesisic\n",
        "\n",
        "Tutorial for how to load Kaggle dataset can be found in https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eus_4tUgfEk9",
        "outputId": "52516ddc-f61e-415e-d318-dca2d7f6d264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRGeofw-8tK"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR2MJBYq-oiB",
        "outputId": "e3a4b9e7-81b4-4cbb-efab-d24896069d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "\n",
        "!pip install imbalanced-learn\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3mnEebdJH6Ex"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv') \n",
        "df_val = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv') \n",
        "num_classes = 7\n",
        "#df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aFSe3uekK67v"
      },
      "outputs": [],
      "source": [
        "#decode one hot label\n",
        "df_train[\"Labels\"] = (df_train.iloc[:, 1:]).idxmax(axis=1)\n",
        "df_val[\"Labels\"] = (df_val.iloc[:, 1:]).idxmax(axis=1)\n",
        "\n",
        "#drop one-hot column\n",
        "df_train = df_train.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "df_val = df_val.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "\n",
        "#make filepaths of the image\n",
        "dir_train = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_Input/'\n",
        "dir_val = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_Input/'\n",
        "df_train['FilePaths'] = dir_train + df_train['image'] + '.jpg'\n",
        "df_val['FilePaths'] = dir_val + df_val['image'] + '.jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38f3dgvyBqFM"
      },
      "source": [
        "Label Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "2IncA-_o_n5w",
        "outputId": "d0d3e234-90a8-4343-a9b3-1058e1ad18ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0, 'DF'),\n",
              " Text(0, 0, 'VASC'),\n",
              " Text(0, 0, 'AKIEC'),\n",
              " Text(0, 0, 'BCC'),\n",
              " Text(0, 0, 'BKL'),\n",
              " Text(0, 0, 'MEL'),\n",
              " Text(0, 0, 'NV')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = df_train['Labels'].value_counts(ascending=True)\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKMKSb4Bkym"
      },
      "source": [
        "Plot 3 images per label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jdnVuqbFBW3K"
      },
      "outputs": [],
      "source": [
        "def plot_images_per_label(df, label, cols: int, size: tuple):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=cols, figsize=size)\n",
        "\n",
        "    cntMax = cols\n",
        "    cntCur = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if(row['Labels'] == label and cntCur < cntMax):\n",
        "            axs[cntCur].imshow(plt.imread(df.FilePaths[index]))\n",
        "            axs[cntCur].set_title(df.Labels[index])\n",
        "\n",
        "            cntCur += 1\n",
        "        else:\n",
        "            if(cntCur >= cntMax):\n",
        "                break\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# unique labels\n",
        "labels = sorted(df_train['Labels'].unique())\n",
        "#for label in labels:\n",
        "#    plot_images_per_label(df_train, label, 3, (12,9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRKKrNacAZtl"
      },
      "source": [
        "Drop duplicate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ERwfyPDHP-zC"
      },
      "outputs": [],
      "source": [
        "#df_group = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_LesionGroupings.csv') \n",
        "#df_train = df_train.set_index('image').join(df_group.set_index('image'))\n",
        "#df_train = df_train.drop_duplicates(subset=['lesion_id'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h35T8vKRVV1Y"
      },
      "source": [
        "Manual undersampling majority class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BeldhlTdVQlT"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.drop(df_train[df_train['Labels'] == 'NV'].sample(frac=.7).index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKjC59JOB_6d"
      },
      "source": [
        "Prepare X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9-c7Xghg4SB4"
      },
      "outputs": [],
      "source": [
        "# input image size\n",
        "IMAGE_W = 224\n",
        "IMAGE_H = 224\n",
        "IMG_SIZE = (IMAGE_W,IMAGE_H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jyCpXnlFoQK"
      },
      "outputs": [],
      "source": [
        "#TIME CONSUMING OPERATION\n",
        "#from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "#X = []\n",
        "#for img in df['FilePaths']:\n",
        "    #img_arr = load_img(img, target_size=IMG_SIZE)\n",
        "#    with load_img(img, target_size=IMG_SIZE) as img_arr:\n",
        "#      X.append(img_to_array(img_arr))\n",
        "\n",
        "#X = np.array(X)\n",
        "df_train['image_px'] = df_train['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "df_val['image_px'] = df_val['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZyZMydSgvZo"
      },
      "outputs": [],
      "source": [
        "X_train = np.asarray(df_train['image_px'].tolist())\n",
        "X_val = np.asarray(df_val['image_px'].tolist())\n",
        "print(np.array(X_train).shape)\n",
        "print(np.array(X_val).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqYLmicGAjZz"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(df_train['Labels'].values)\n",
        "\n",
        "# summarize class distribution\n",
        "from collections import Counter\n",
        "counter = Counter(y_train)\n",
        "print(counter)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEuVIGc3g859"
      },
      "outputs": [],
      "source": [
        "y_val = np.array(df_val['Labels'].values)\n",
        "print(Counter(y_val))\n",
        "print(y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfvEVGIQhIr2"
      },
      "outputs": [],
      "source": [
        "#label encoding\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_val = label_encoder.fit_transform(y_val)\n",
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZv-B-ygCD57"
      },
      "source": [
        "#SMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDskF1wjGffh"
      },
      "outputs": [],
      "source": [
        "def SMOTE_Data(X, y):\n",
        "  sm = SMOTE(random_state=42, k_neighbors=5)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X.reshape((-1, IMAGE_W * IMAGE_H * 3)), y)\n",
        "  X_resampled.reshape(-1, IMAGE_W, IMAGE_H, 3)\n",
        "  return X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brshqGvOCDJL",
        "outputId": "177db98e-72a2-4308-b875-80bb6bf6d0a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14077, 150528)\n",
            "(14077,)\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = SMOTE_Data(X_train, y_train) #beware of the actual parameter\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GfOtcbV5vVZ",
        "outputId": "1a24cec2-e284-4588-e427-a54eb387651d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({5: 2011, 4: 2011, 2: 2011, 3: 2011, 0: 2011, 1: 2011, 6: 2011})\n"
          ]
        }
      ],
      "source": [
        "counter = Counter(y_train)\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl-nmACZOZpg",
        "outputId": "e6b51afe-0a14-4312-c5b0-9927ea4bd2f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape:  (14077, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.reshape(-1, IMAGE_W, IMAGE_H, 3)\n",
        "print('X_train shape: ',X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBXx28B9yGu"
      },
      "source": [
        "#DeepSMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAxUkXy8ueYG"
      },
      "outputs": [],
      "source": [
        "#Normalization\n",
        "#X_train_mean = np.mean(X_train)\n",
        "#X_train_std = np.std(X_train)\n",
        "\n",
        "#X_train = (X_train - X_train_mean)/X_train_std\n",
        "#X_val = (X_val - X_train_mean)/X_train_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-Xqj-WQ90L_"
      },
      "outputs": [],
      "source": [
        "#optional\n",
        "#X=X_train\n",
        "#y=y_train\n",
        "\n",
        "from numpy import moveaxis\n",
        "X_train = moveaxis(X_train, 3, 1)\n",
        "X_train = X_train.astype('float32') / 255.\n",
        "#dec_x = X_train \n",
        "#dec_y = y\n",
        "\n",
        "#create counter for encoder\n",
        "counter = sorted(counter.items())\n",
        "counter = [value for _, value in counter]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kMMmX7r-fV5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(torch.version.cuda) #10.1\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 3#1    # number of channels in the input data \n",
        "\n",
        "args['n_z'] = 600 #300     # number of dimensions in latent space. \n",
        "\n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 300       # how many epochs to run for\n",
        "args['batch_size'] = 6   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = False       # train networks if True, else load networks from\n",
        "\n",
        "args['patience'] = 20\n",
        "\n",
        "## create encoder model and decoder model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        \n",
        "        # convolutional filters, work excellent with image data\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),# 112\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False), # 56\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),# 28\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),#14\n",
        "            nn.BatchNorm2d(self.dim_h * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 8, self.dim_h * 16, 4, 2, 1, bias=False),#7\n",
        "            nn.BatchNorm2d(self.dim_h * 16),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 16, self.dim_h * 32, 7, 1, 0, bias=False),#1\n",
        "            nn.BatchNorm2d(self.dim_h * 32), # 40 X 8 = 320\n",
        "            nn.LeakyReLU(0.2, inplace=True) )#,\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 5), self.n_z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 2**5 * 7 * 7),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        # H_out ​= (H_in​−1)*stride[0] − 2×padding[0] + dilation[0]×(kernel_size[0]−1) + output_padding[0] + 1\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 32, self.dim_h * 16, 4, 2, 1), # 14\n",
        "            nn.BatchNorm2d(self.dim_h * 16),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 16, self.dim_h * 8, 4, 2, 1), # 28\n",
        "            nn.BatchNorm2d(self.dim_h * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4, 2, 1), #56\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4, 2, 1), #112\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, 3, 4, stride=2, padding=1),# 224\n",
        "            #nn.Sigmoid())\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('dec')\n",
        "        #print('input ',x.size())\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 2**5, 7, 7)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"functions to create SMOTE images\"\"\"\n",
        "\n",
        "def biased_get_class(X, y, c):\n",
        "    \n",
        "    xbeg = X[y == c]\n",
        "    ybeg = y[y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "\n",
        "    # determining the number of samples to generate\n",
        "    #n_to_sample = 10 \n",
        "\n",
        "    # fitting the model\n",
        "    n_neigh = 5\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "#xsamp, ysamp = SM(xclass,yclass)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHtLISURIMTg",
        "outputId": "800fca77-386c-4779-b28d-968170e1c0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Epoch: 0 \tTrain Loss: 0.775552 \tmse loss: 0.407762 \tmse2 loss: 0.367790\n",
            "Saving..\n",
            "Epoch: 1 \tTrain Loss: 0.211056 \tmse loss: 0.111666 \tmse2 loss: 0.099390\n",
            "Saving..\n",
            "Epoch: 2 \tTrain Loss: 0.153950 \tmse loss: 0.078980 \tmse2 loss: 0.074970\n",
            "Saving..\n",
            "Epoch: 3 \tTrain Loss: 0.137521 \tmse loss: 0.070455 \tmse2 loss: 0.067066\n",
            "Saving..\n",
            "Epoch: 4 \tTrain Loss: 0.123724 \tmse loss: 0.064348 \tmse2 loss: 0.059376\n",
            "Saving..\n",
            "Epoch: 5 \tTrain Loss: 0.114418 \tmse loss: 0.059063 \tmse2 loss: 0.055354\n",
            "Saving..\n",
            "Epoch: 6 \tTrain Loss: 0.110404 \tmse loss: 0.057641 \tmse2 loss: 0.052763\n",
            "Saving..\n",
            "Epoch: 7 \tTrain Loss: 0.099953 \tmse loss: 0.053611 \tmse2 loss: 0.046341\n",
            "Saving..\n",
            "Epoch: 8 \tTrain Loss: 0.094897 \tmse loss: 0.051286 \tmse2 loss: 0.043611\n",
            "Saving..\n",
            "Epoch: 9 \tTrain Loss: 0.090954 \tmse loss: 0.050273 \tmse2 loss: 0.040682\n",
            "Saving..\n",
            "Epoch: 10 \tTrain Loss: 0.087089 \tmse loss: 0.048824 \tmse2 loss: 0.038265\n",
            "Saving..\n",
            "Epoch: 11 \tTrain Loss: 0.082592 \tmse loss: 0.046604 \tmse2 loss: 0.035989\n",
            "Saving..\n",
            "Epoch: 12 \tTrain Loss: 0.078564 \tmse loss: 0.045056 \tmse2 loss: 0.033508\n",
            "Saving..\n",
            "Epoch: 13 \tTrain Loss: 0.072842 \tmse loss: 0.041858 \tmse2 loss: 0.030984\n",
            "Saving..\n",
            "Epoch: 14 \tTrain Loss: 0.069838 \tmse loss: 0.040502 \tmse2 loss: 0.029336\n",
            "Saving..\n",
            "Epoch: 15 \tTrain Loss: 0.068549 \tmse loss: 0.040186 \tmse2 loss: 0.028363\n",
            "Saving..\n",
            "Epoch: 16 \tTrain Loss: 0.063513 \tmse loss: 0.037157 \tmse2 loss: 0.026356\n",
            "Saving..\n",
            "Epoch: 17 \tTrain Loss: 0.060590 \tmse loss: 0.035664 \tmse2 loss: 0.024926\n",
            "Saving..\n",
            "Epoch: 18 \tTrain Loss: 0.059045 \tmse loss: 0.035046 \tmse2 loss: 0.023999\n",
            "Saving..\n",
            "Epoch: 19 \tTrain Loss: 0.056783 \tmse loss: 0.033961 \tmse2 loss: 0.022822\n",
            "Saving..\n",
            "Epoch: 20 \tTrain Loss: 0.054568 \tmse loss: 0.033137 \tmse2 loss: 0.021430\n",
            "Saving..\n",
            "Epoch: 21 \tTrain Loss: 0.053124 \tmse loss: 0.031838 \tmse2 loss: 0.021286\n",
            "Saving..\n",
            "Epoch: 22 \tTrain Loss: 0.050431 \tmse loss: 0.030542 \tmse2 loss: 0.019890\n",
            "Saving..\n",
            "Epoch: 23 \tTrain Loss: 0.050202 \tmse loss: 0.030749 \tmse2 loss: 0.019454\n",
            "Saving..\n",
            "Epoch: 24 \tTrain Loss: 0.047643 \tmse loss: 0.028750 \tmse2 loss: 0.018893\n",
            "Saving..\n",
            "Epoch: 25 \tTrain Loss: 0.045406 \tmse loss: 0.027763 \tmse2 loss: 0.017643\n",
            "Saving..\n",
            "Epoch: 26 \tTrain Loss: 0.043981 \tmse loss: 0.026757 \tmse2 loss: 0.017225\n",
            "Saving..\n",
            "Epoch: 27 \tTrain Loss: 0.042864 \tmse loss: 0.026297 \tmse2 loss: 0.016567\n",
            "Saving..\n",
            "Epoch: 28 \tTrain Loss: 0.041682 \tmse loss: 0.025595 \tmse2 loss: 0.016086\n",
            "Saving..\n",
            "Epoch: 29 \tTrain Loss: 0.041499 \tmse loss: 0.025642 \tmse2 loss: 0.015857\n",
            "Saving..\n",
            "Epoch: 30 \tTrain Loss: 0.037969 \tmse loss: 0.023497 \tmse2 loss: 0.014472\n",
            "Saving..\n",
            "Epoch: 31 \tTrain Loss: 0.037807 \tmse loss: 0.023243 \tmse2 loss: 0.014563\n",
            "Saving..\n",
            "Epoch: 32 \tTrain Loss: 0.037300 \tmse loss: 0.023138 \tmse2 loss: 0.014162\n",
            "Saving..\n",
            "Epoch: 33 \tTrain Loss: 0.036372 \tmse loss: 0.022326 \tmse2 loss: 0.014046\n",
            "Saving..\n",
            "Epoch: 34 \tTrain Loss: 0.035522 \tmse loss: 0.021885 \tmse2 loss: 0.013637\n",
            "Saving..\n",
            "Epoch: 35 \tTrain Loss: 0.034779 \tmse loss: 0.021506 \tmse2 loss: 0.013273\n",
            "Saving..\n",
            "Epoch: 36 \tTrain Loss: 0.032102 \tmse loss: 0.020080 \tmse2 loss: 0.012022\n",
            "Saving..\n",
            "Epoch: 37 \tTrain Loss: 0.034198 \tmse loss: 0.021344 \tmse2 loss: 0.012854\n",
            "Epoch: 38 \tTrain Loss: 0.032470 \tmse loss: 0.020265 \tmse2 loss: 0.012206\n",
            "Epoch: 39 \tTrain Loss: 0.030947 \tmse loss: 0.019350 \tmse2 loss: 0.011597\n",
            "Saving..\n",
            "Epoch: 40 \tTrain Loss: 0.029693 \tmse loss: 0.018529 \tmse2 loss: 0.011164\n",
            "Saving..\n",
            "Epoch: 41 \tTrain Loss: 0.029748 \tmse loss: 0.018592 \tmse2 loss: 0.011156\n",
            "Epoch: 42 \tTrain Loss: 0.029334 \tmse loss: 0.018120 \tmse2 loss: 0.011214\n",
            "Saving..\n",
            "Epoch: 43 \tTrain Loss: 0.028952 \tmse loss: 0.017944 \tmse2 loss: 0.011008\n",
            "Saving..\n",
            "Epoch: 44 \tTrain Loss: 0.027600 \tmse loss: 0.017173 \tmse2 loss: 0.010427\n",
            "Saving..\n",
            "Epoch: 45 \tTrain Loss: 0.027604 \tmse loss: 0.017348 \tmse2 loss: 0.010256\n",
            "Epoch: 46 \tTrain Loss: 0.026385 \tmse loss: 0.016375 \tmse2 loss: 0.010010\n",
            "Saving..\n",
            "Epoch: 47 \tTrain Loss: 0.026282 \tmse loss: 0.016437 \tmse2 loss: 0.009844\n",
            "Saving..\n",
            "Epoch: 48 \tTrain Loss: 0.025948 \tmse loss: 0.016188 \tmse2 loss: 0.009760\n",
            "Saving..\n",
            "Epoch: 49 \tTrain Loss: 0.026479 \tmse loss: 0.016379 \tmse2 loss: 0.010100\n",
            "Epoch: 50 \tTrain Loss: 0.024849 \tmse loss: 0.015670 \tmse2 loss: 0.009179\n",
            "Saving..\n",
            "Epoch: 51 \tTrain Loss: 0.023903 \tmse loss: 0.014810 \tmse2 loss: 0.009093\n",
            "Saving..\n",
            "Epoch: 52 \tTrain Loss: 0.024073 \tmse loss: 0.015048 \tmse2 loss: 0.009025\n",
            "Epoch: 53 \tTrain Loss: 0.023138 \tmse loss: 0.014388 \tmse2 loss: 0.008750\n",
            "Saving..\n",
            "Epoch: 54 \tTrain Loss: 0.023135 \tmse loss: 0.014332 \tmse2 loss: 0.008804\n",
            "Saving..\n",
            "Epoch: 55 \tTrain Loss: 0.022108 \tmse loss: 0.013746 \tmse2 loss: 0.008362\n",
            "Saving..\n",
            "Epoch: 56 \tTrain Loss: 0.021995 \tmse loss: 0.013694 \tmse2 loss: 0.008301\n",
            "Saving..\n",
            "Epoch: 57 \tTrain Loss: 0.022043 \tmse loss: 0.013755 \tmse2 loss: 0.008287\n",
            "Epoch: 58 \tTrain Loss: 0.022110 \tmse loss: 0.013756 \tmse2 loss: 0.008354\n",
            "Epoch: 59 \tTrain Loss: 0.021382 \tmse loss: 0.013284 \tmse2 loss: 0.008098\n",
            "Saving..\n",
            "Epoch: 60 \tTrain Loss: 0.020553 \tmse loss: 0.012730 \tmse2 loss: 0.007823\n",
            "Saving..\n",
            "Epoch: 61 \tTrain Loss: 0.020526 \tmse loss: 0.012938 \tmse2 loss: 0.007588\n",
            "Saving..\n",
            "Epoch: 62 \tTrain Loss: 0.020712 \tmse loss: 0.012915 \tmse2 loss: 0.007797\n",
            "Epoch: 63 \tTrain Loss: 0.020426 \tmse loss: 0.012755 \tmse2 loss: 0.007670\n",
            "Saving..\n",
            "Epoch: 64 \tTrain Loss: 0.020276 \tmse loss: 0.012629 \tmse2 loss: 0.007647\n",
            "Saving..\n",
            "Epoch: 65 \tTrain Loss: 0.019778 \tmse loss: 0.012275 \tmse2 loss: 0.007502\n",
            "Saving..\n",
            "Epoch: 66 \tTrain Loss: 0.019596 \tmse loss: 0.012174 \tmse2 loss: 0.007422\n",
            "Saving..\n",
            "Epoch: 67 \tTrain Loss: 0.019395 \tmse loss: 0.012196 \tmse2 loss: 0.007199\n",
            "Saving..\n",
            "Epoch: 68 \tTrain Loss: 0.018925 \tmse loss: 0.011811 \tmse2 loss: 0.007114\n",
            "Saving..\n",
            "Epoch: 69 \tTrain Loss: 0.018632 \tmse loss: 0.011571 \tmse2 loss: 0.007062\n",
            "Saving..\n",
            "Epoch: 70 \tTrain Loss: 0.018705 \tmse loss: 0.011617 \tmse2 loss: 0.007088\n",
            "Epoch: 71 \tTrain Loss: 0.018341 \tmse loss: 0.011462 \tmse2 loss: 0.006879\n",
            "Saving..\n",
            "Epoch: 72 \tTrain Loss: 0.017906 \tmse loss: 0.011042 \tmse2 loss: 0.006865\n",
            "Saving..\n",
            "Epoch: 73 \tTrain Loss: 0.017702 \tmse loss: 0.011033 \tmse2 loss: 0.006669\n",
            "Saving..\n",
            "Epoch: 74 \tTrain Loss: 0.018381 \tmse loss: 0.011357 \tmse2 loss: 0.007023\n",
            "Epoch: 75 \tTrain Loss: 0.017592 \tmse loss: 0.010941 \tmse2 loss: 0.006651\n",
            "Saving..\n",
            "Epoch: 76 \tTrain Loss: 0.017088 \tmse loss: 0.010751 \tmse2 loss: 0.006337\n",
            "Saving..\n",
            "Epoch: 77 \tTrain Loss: 0.016920 \tmse loss: 0.010665 \tmse2 loss: 0.006255\n",
            "Saving..\n",
            "Epoch: 78 \tTrain Loss: 0.016727 \tmse loss: 0.010370 \tmse2 loss: 0.006357\n",
            "Saving..\n",
            "Epoch: 79 \tTrain Loss: 0.016654 \tmse loss: 0.010397 \tmse2 loss: 0.006258\n",
            "Saving..\n",
            "Epoch: 80 \tTrain Loss: 0.016630 \tmse loss: 0.010345 \tmse2 loss: 0.006285\n",
            "Saving..\n",
            "Epoch: 81 \tTrain Loss: 0.016535 \tmse loss: 0.010201 \tmse2 loss: 0.006334\n",
            "Saving..\n",
            "Epoch: 82 \tTrain Loss: 0.016445 \tmse loss: 0.010208 \tmse2 loss: 0.006237\n",
            "Saving..\n",
            "Epoch: 83 \tTrain Loss: 0.016364 \tmse loss: 0.010111 \tmse2 loss: 0.006253\n",
            "Saving..\n",
            "Epoch: 84 \tTrain Loss: 0.016294 \tmse loss: 0.010059 \tmse2 loss: 0.006235\n",
            "Saving..\n",
            "Epoch: 85 \tTrain Loss: 0.015848 \tmse loss: 0.009855 \tmse2 loss: 0.005993\n",
            "Saving..\n",
            "Epoch: 86 \tTrain Loss: 0.016745 \tmse loss: 0.010353 \tmse2 loss: 0.006392\n",
            "Epoch: 87 \tTrain Loss: 0.015603 \tmse loss: 0.009604 \tmse2 loss: 0.005998\n",
            "Saving..\n",
            "Epoch: 88 \tTrain Loss: 0.015266 \tmse loss: 0.009542 \tmse2 loss: 0.005723\n",
            "Saving..\n",
            "Epoch: 89 \tTrain Loss: 0.015433 \tmse loss: 0.009594 \tmse2 loss: 0.005838\n",
            "Epoch: 90 \tTrain Loss: 0.015653 \tmse loss: 0.009714 \tmse2 loss: 0.005938\n",
            "Epoch: 91 \tTrain Loss: 0.015037 \tmse loss: 0.009359 \tmse2 loss: 0.005678\n",
            "Saving..\n",
            "Epoch: 92 \tTrain Loss: 0.015083 \tmse loss: 0.009401 \tmse2 loss: 0.005682\n",
            "Epoch: 93 \tTrain Loss: 0.014839 \tmse loss: 0.009240 \tmse2 loss: 0.005599\n",
            "Saving..\n",
            "Epoch: 94 \tTrain Loss: 0.014491 \tmse loss: 0.009060 \tmse2 loss: 0.005431\n",
            "Saving..\n",
            "Epoch: 95 \tTrain Loss: 0.014343 \tmse loss: 0.008973 \tmse2 loss: 0.005370\n",
            "Saving..\n",
            "Epoch: 96 \tTrain Loss: 0.014413 \tmse loss: 0.008961 \tmse2 loss: 0.005452\n",
            "Epoch: 97 \tTrain Loss: 0.014514 \tmse loss: 0.009017 \tmse2 loss: 0.005496\n",
            "Epoch: 98 \tTrain Loss: 0.014498 \tmse loss: 0.009009 \tmse2 loss: 0.005489\n",
            "Epoch: 99 \tTrain Loss: 0.014384 \tmse loss: 0.009049 \tmse2 loss: 0.005334\n",
            "Epoch: 100 \tTrain Loss: 0.014102 \tmse loss: 0.008739 \tmse2 loss: 0.005363\n",
            "Saving..\n",
            "Epoch: 101 \tTrain Loss: 0.014110 \tmse loss: 0.008784 \tmse2 loss: 0.005326\n",
            "Epoch: 102 \tTrain Loss: 0.014105 \tmse loss: 0.008818 \tmse2 loss: 0.005287\n",
            "Epoch: 103 \tTrain Loss: 0.013822 \tmse loss: 0.008612 \tmse2 loss: 0.005210\n",
            "Saving..\n",
            "Epoch: 104 \tTrain Loss: 0.013302 \tmse loss: 0.008301 \tmse2 loss: 0.005001\n",
            "Saving..\n",
            "Epoch: 105 \tTrain Loss: 0.013501 \tmse loss: 0.008379 \tmse2 loss: 0.005122\n",
            "Epoch: 106 \tTrain Loss: 0.013352 \tmse loss: 0.008297 \tmse2 loss: 0.005056\n",
            "Epoch: 107 \tTrain Loss: 0.013359 \tmse loss: 0.008263 \tmse2 loss: 0.005096\n",
            "Epoch: 108 \tTrain Loss: 0.013193 \tmse loss: 0.008218 \tmse2 loss: 0.004975\n",
            "Saving..\n",
            "Epoch: 109 \tTrain Loss: 0.013153 \tmse loss: 0.008191 \tmse2 loss: 0.004962\n",
            "Saving..\n",
            "Epoch: 110 \tTrain Loss: 0.013338 \tmse loss: 0.008248 \tmse2 loss: 0.005089\n",
            "Epoch: 111 \tTrain Loss: 0.013237 \tmse loss: 0.008225 \tmse2 loss: 0.005012\n",
            "Epoch: 112 \tTrain Loss: 0.013311 \tmse loss: 0.008270 \tmse2 loss: 0.005040\n",
            "Epoch: 113 \tTrain Loss: 0.012905 \tmse loss: 0.007964 \tmse2 loss: 0.004941\n",
            "Saving..\n",
            "Epoch: 114 \tTrain Loss: 0.012795 \tmse loss: 0.008010 \tmse2 loss: 0.004785\n",
            "Saving..\n",
            "Epoch: 115 \tTrain Loss: 0.013091 \tmse loss: 0.008113 \tmse2 loss: 0.004978\n",
            "Epoch: 116 \tTrain Loss: 0.012867 \tmse loss: 0.007992 \tmse2 loss: 0.004875\n",
            "Epoch: 117 \tTrain Loss: 0.012668 \tmse loss: 0.007955 \tmse2 loss: 0.004713\n",
            "Saving..\n",
            "Epoch: 118 \tTrain Loss: 0.012603 \tmse loss: 0.007866 \tmse2 loss: 0.004738\n",
            "Saving..\n",
            "Epoch: 119 \tTrain Loss: 0.012430 \tmse loss: 0.007753 \tmse2 loss: 0.004677\n",
            "Saving..\n",
            "Epoch: 120 \tTrain Loss: 0.012518 \tmse loss: 0.007812 \tmse2 loss: 0.004707\n",
            "Epoch: 121 \tTrain Loss: 0.012137 \tmse loss: 0.007592 \tmse2 loss: 0.004546\n",
            "Saving..\n",
            "Epoch: 122 \tTrain Loss: 0.012269 \tmse loss: 0.007668 \tmse2 loss: 0.004601\n",
            "Epoch: 123 \tTrain Loss: 0.011947 \tmse loss: 0.007499 \tmse2 loss: 0.004448\n",
            "Saving..\n",
            "Epoch: 124 \tTrain Loss: 0.012195 \tmse loss: 0.007512 \tmse2 loss: 0.004683\n",
            "Epoch: 125 \tTrain Loss: 0.012213 \tmse loss: 0.007572 \tmse2 loss: 0.004641\n"
          ]
        }
      ],
      "source": [
        "#Begin the training\n",
        "batch_size = args['batch_size']\n",
        "patience = args['patience']\n",
        "encoder = Encoder(args)\n",
        "decoder = Decoder(args)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "decoder = decoder.to(device)\n",
        "encoder = encoder.to(device)\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "#decoder loss function\n",
        "criterion = nn.MSELoss()\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "num_workers = 0\n",
        "\n",
        "#torch.Tensor returns float so if want long then use torch.tensor\n",
        "tensor_x = torch.Tensor(X_train)\n",
        "tensor_y = torch.tensor(y_train,dtype=torch.long)\n",
        "mnist_bal = TensorDataset(tensor_x,tensor_y) \n",
        "train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
        "    batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "t0 = time.time()\n",
        "if args['train']:\n",
        "    enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
        "    dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
        "\n",
        "    for epoch in range(args['epochs']):\n",
        "        train_loss = 0.0\n",
        "        tmse_loss = 0.0\n",
        "        tdiscr_loss = 0.0\n",
        "        # train for one epoch -- set nets to train mode\n",
        "        encoder.train()\n",
        "        decoder.train()\n",
        "    \n",
        "        for images,labs in train_loader:\n",
        "        \n",
        "            # zero gradients for each batch\n",
        "            encoder.zero_grad()\n",
        "            decoder.zero_grad()\n",
        "            images, labs = images.to(device), labs.to(device)\n",
        "            labsn = labs.detach().cpu().numpy()\n",
        "#            print('images shape', images.shape)\n",
        "            # run images\n",
        "            z_hat = encoder(images)\n",
        "#            print('images shape after encoding', z_hat.shape)\n",
        "        \n",
        "            x_hat = decoder(z_hat) #decoder outputs tanh\n",
        "#            print('images shape after decoding', x_hat.shape)\n",
        "            mse = criterion(x_hat,images)\n",
        "                    \n",
        "            resx = []\n",
        "            resy = []\n",
        "        \n",
        "            tc = np.random.choice(num_classes,1)\n",
        "            #tc = 9\n",
        "            xbeg = X_train[y_train == tc]\n",
        "            ybeg = y_train[y_train == tc] \n",
        "            xlen = len(xbeg)\n",
        "            nsamp = min(xlen, 100)\n",
        "            ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
        "            xclass = xbeg[ind]\n",
        "            yclass = ybeg[ind]\n",
        "        \n",
        "            xclen = len(xclass)\n",
        "            xcminus = np.arange(1,xclen)\n",
        "            \n",
        "            xcplus = np.append(xcminus,0)\n",
        "            xcnew = (xclass[[xcplus],:])\n",
        "            xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
        "        \n",
        "            xcnew = torch.Tensor(xcnew)\n",
        "            xcnew = xcnew.to(device)\n",
        "        \n",
        "            #encode xclass to feature space\n",
        "            xclass = torch.Tensor(xclass)\n",
        "            xclass = xclass.to(device)\n",
        "            xclass = encoder(xclass)\n",
        "        \n",
        "            xclass = xclass.detach().cpu().numpy()\n",
        "        \n",
        "            xc_enc = (xclass[[xcplus],:])\n",
        "            xc_enc = np.squeeze(xc_enc)\n",
        "        \n",
        "            xc_enc = torch.Tensor(xc_enc)\n",
        "            xc_enc = xc_enc.to(device)\n",
        "            \n",
        "            ximg = decoder(xc_enc)\n",
        "            \n",
        "            mse2 = criterion(ximg,xcnew)\n",
        "        \n",
        "            comb_loss = mse2 + mse\n",
        "            comb_loss.backward()\n",
        "        \n",
        "            enc_optim.step()\n",
        "            dec_optim.step()\n",
        "        \n",
        "            train_loss += comb_loss.item()*images.size(0)\n",
        "            tmse_loss += mse.item()*images.size(0)\n",
        "            tdiscr_loss += mse2.item()*images.size(0)\n",
        "\n",
        "        train_loss = train_loss/len(train_loader)\n",
        "        tmse_loss = tmse_loss/len(train_loader)\n",
        "        tdiscr_loss = tdiscr_loss/len(train_loader)\n",
        "        print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
        "                train_loss,tmse_loss,tdiscr_loss))\n",
        "        \n",
        "    \n",
        "    \n",
        "        #store the best encoder and decoder models\n",
        "        #here, /crs5 is a reference to 5 way cross validation, but is not\n",
        "        #necessary for illustration purposes\n",
        "        if train_loss < best_loss:\n",
        "            print('Saving..')\n",
        "            patience = args['patience']\n",
        "            path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/bst_enc.pth'\n",
        "            path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/bst_dec.pth'\n",
        "          \n",
        "            torch.save(encoder.state_dict(), path_enc)\n",
        "            torch.save(decoder.state_dict(), path_dec)\n",
        "    \n",
        "            best_loss = train_loss\n",
        "        else:\n",
        "            patience = patience - 1\n",
        "\n",
        "        if patience == 0:\n",
        "            print('Out of patience. \\n')\n",
        "            break\n",
        "    \n",
        "    \n",
        "    #in addition, store the final model (may not be the best) for\n",
        "    #informational purposes\n",
        "    path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/f_enc.pth'\n",
        "    path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/f_dec.pth'\n",
        "    print(path_enc)\n",
        "    print(path_dec)\n",
        "    torch.save(encoder.state_dict(), path_enc)\n",
        "    torch.save(decoder.state_dict(), path_dec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoqPIVKqF5Ug"
      },
      "outputs": [],
      "source": [
        "#Generate artificial images\n",
        "import torch\n",
        "np.printoptions(precision=5,suppress=True)\n",
        "\n",
        "#path on the computer where the models are stored\n",
        "modpth = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/'\n",
        "\n",
        "encf = []\n",
        "decf = []\n",
        "for p in range(1):\n",
        "    enc = modpth + '/bst_enc.pth'\n",
        "    dec = modpth + '/bst_dec.pth'\n",
        "    encf.append(enc)\n",
        "    decf.append(dec)\n",
        "\n",
        "for m in range(1):\n",
        "    print('train imgs shape ',X_train.shape) #(45000,3,32,32)\n",
        "    print('decy ',y_train.shape)\n",
        "    \n",
        "    #generate some images \n",
        "    train_on_gpu = torch.cuda.is_available()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    path_enc = encf[m]\n",
        "    path_dec = decf[m]\n",
        "\n",
        "    encoder = Encoder(args)\n",
        "    encoder.load_state_dict(torch.load(path_enc), strict=False)\n",
        "    encoder = encoder.to(device)\n",
        "\n",
        "    decoder = Decoder(args)\n",
        "    decoder.load_state_dict(torch.load(path_dec), strict=False)\n",
        "    decoder = decoder.to(device)\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    resx = []\n",
        "    resy = []\n",
        "\n",
        "    for i in [0,1,2,3,4,6]: #skip class 5 since it's max class\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "#        print(xclass.shape) #(500, 3, 32, 32)\n",
        "#        print(yclass[0]) #(500,)\n",
        "            \n",
        "        #encode xclass to feature space\n",
        "        xclass = torch.Tensor(xclass)\n",
        "        xclass = xclass.to(device)\n",
        "        xclass = encoder(xclass)\n",
        "            \n",
        "        xclass = xclass.detach().cpu().numpy()\n",
        "        n = np.max(counter) - counter[i]\n",
        "        xsamp, ysamp = G_SM(xclass,yclass,n,i)\n",
        "#        print(xsamp.shape) #(4500, 600)\n",
        "#        print(len(ysamp)) #4500\n",
        "        ysamp = np.array(ysamp)\n",
        "    \n",
        "        \"\"\"to generate samples for resnet\"\"\"   \n",
        "        xsamp = torch.Tensor(xsamp)\n",
        "        xsamp = xsamp.to(device)\n",
        "        ximg = decoder(xsamp)\n",
        "\n",
        "        ximn = ximg.detach().cpu().numpy()\n",
        "#        print(ximn.shape) \n",
        "        resx.append(ximn)\n",
        "        resy.append(ysamp)\n",
        "    \n",
        "    resx1 = np.vstack(resx)\n",
        "    resy1 = np.hstack(resy)\n",
        "#    print(resx1.shape) #(34720, 3, 32, 32)\n",
        "\n",
        "    resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "#    print(resx1.shape) #(34720, 3072)\n",
        "    \n",
        "    dec_x1 = X_train.reshape(X_train.shape[0],-1)\n",
        "#    print('decx1 ',dec_x1.shape)\n",
        "    combx = np.vstack((resx1,dec_x1))\n",
        "    comby = np.hstack((resy1,y_train))\n",
        "\n",
        "    print(combx.shape) #(45000, 3, 32, 32)\n",
        "    print(comby.shape) #(45000,)\n",
        "#    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76bpFBQcHLIY"
      },
      "outputs": [],
      "source": [
        "X_train = combx.reshape(-1, 3, IMAGE_W, IMAGE_H)\n",
        "X_train = moveaxis(X_train, 1, 3)\n",
        "print('X_train shape: ',X_train.shape)\n",
        "y_train = comby\n",
        "print(Counter(comby))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SqyvSgX8sLO"
      },
      "outputs": [],
      "source": [
        "X_train = X_train * 255\n",
        "X_train = X_train.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we4D35jnEOqs"
      },
      "outputs": [],
      "source": [
        "#de-standardization\n",
        "#X_train = (X_train * X_train_std + X_train_mean).astype(int)\n",
        "#X_val = (X_val * X_train_std + X_train_mean).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US0KkIaVlTdU"
      },
      "source": [
        "#Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a_lPCqbibaS"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.reshape(-1, 1)\n",
        "y_val = y_val.reshape(-1, 1)\n",
        "print('y_train shape: ',y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja5ZmgbCvDw5"
      },
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "y_val = to_categorical(y_val, num_classes = num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QM00erNGU32"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jrJ33lUDkCM"
      },
      "source": [
        "#Split dataset to train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6qneWL_Bs2U",
        "outputId": "22c2c4ff-5544-4dd8-b69a-af96b4b97616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data:  (9814, 32, 32, 3)\n",
            "Remaining Data:  (201, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "# stratified train and rem (20%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.02, stratify=y, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Remaining Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vai7M7WSXVY4",
        "outputId": "b9bc1199-a518-4a8b-9d6e-4d6038efbafe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Data:  (416, 32, 32, 3)\n",
            "Val Data:  (416, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "# stratified val and test (50%) \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_rem, y_rem, test_size=0.5, stratify=y_rem, random_state=1)\n",
        "\n",
        "print('Test Data: ', X_test.shape)\n",
        "print('Val Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I15HgVuhjFlm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVVOQPNiHXHw",
        "outputId": "a1e60310-4f3b-4063-8465-4d7120d225c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data:  (2015, 32, 32, 3)\n",
            "Test Data:  (224, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "#optional\n",
        "# stratified train and test (10%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Test Data: ', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_oaUYYgJNV7",
        "outputId": "ca2f393a-0599-4cfc-b330-45d723f2bb06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data:  (3369, 32, 32, 3)\n",
            "Val Data:  (375, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "#optional\n",
        "# stratified train and val (10%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, stratify=y, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Val Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZklPzWxCCtTW"
      },
      "source": [
        "Create and compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jFShRvTHnqi"
      },
      "outputs": [],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kef4r_zxjgk"
      },
      "outputs": [],
      "source": [
        "#Data Augmentation\n",
        "dataaugment = ImageDataGenerator(\n",
        "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "        shear_range = 10) \n",
        "\n",
        "dataaugment.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7sLC2svMuJ"
      },
      "source": [
        "# Classification\n",
        "Model from https://github.com/AnasBrital98/CNN-From-Scratch/tree/master/Inception-V3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8eRZiucdYnP"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 32\n",
        "optimizer = Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "#USe TF.data\n",
        "training_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "validation_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "\n",
        "autotune = tf.data.AUTOTUNE\n",
        "train_data_batches = training_data.shuffle(buffer_size=40000).batch(32).prefetch(buffer_size=autotune)\n",
        "valid_data_batches = validation_data.shuffle(buffer_size=10000).batch(32).prefetch(buffer_size=autotune)\n",
        "\n",
        "#Data augmentation\n",
        "rescale_layer = tf.keras.Sequential([layers.experimental.preprocessing.Rescaling(1./255)])\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2), \n",
        "  layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3)),\n",
        "  layers.experimental.preprocessing.RandomTranslation(0.3, 0.3, fill_mode='reflect', interpolation='bilinear',)\n",
        "])\n",
        "\n",
        "def balanced_acc(y_true, y_pred):\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import numpy as np\n",
        "    from keras import backend as K\n",
        "\n",
        "    tensor1 = tf.math.argmax(y_true, axis=1)\n",
        "    tensor2 = tf.math.argmax(y_pred, axis=1)\n",
        "\n",
        "    cm = tf.math.confusion_matrix(tensor1, tensor2)\n",
        "    \n",
        "    diag = tf.linalg.tensor_diag_part (cm)\n",
        "    tpfn = tf.cast(K.sum(cm, axis = 1), tf.float32) + K.epsilon()\n",
        "    recall = tf.divide(tf.cast(diag, tf.float32),tpfn)\n",
        "    balanced_acc = K.mean(recall)\n",
        "    return balanced_acc\n",
        "\n",
        "#Callbacks\n",
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/best_model_SMOTE.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/last_model_SMOTE.h5'\n",
        "mc = ModelCheckpoint(best_model_fpath, monitor='val_balanced_acc', mode='max', verbose=1, save_best_only=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=10, verbose=0, factor=0.5, min_lr=0.00001)\n",
        "early_stopping_monitor = EarlyStopping(patience=30,monitor='val_balanced_acc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwwLiXUSG0IZ"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "input_shape = X_train.shape[1:]\n",
        "input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "x = data_augmentation(input_tensor)\n",
        "#x = rescale_layer(x)\n",
        "\n",
        "base_model = ResNet50(input_shape=input_shape, weights='imagenet', include_top=False)\n",
        "x = base_model(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(7, activation='softmax')(x)\n",
        "model = Model(inputs=input_tensor, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "#hst = model.fit(dataaugment.flow(X_train,y_train, batch_size=BATCH_SIZE),\n",
        "hst = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,      \n",
        "                    #steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXnW3lmCgln3"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model balanced accuracy')\n",
        "plt.ylabel('Balanced Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PgksTFkOAq"
      },
      "source": [
        "#Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr1jnSM7yzJc"
      },
      "outputs": [],
      "source": [
        "limit = 171\n",
        "for layer in model.layers[:limit]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[limit:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "optimizer_SGD = SGD(learning_rate=0.0001, momentum=0.9)\n",
        "model.compile(optimizer = optimizer_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst2 = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,\n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPz8NH1Oylv9"
      },
      "outputs": [],
      "source": [
        "#save last model\n",
        "\n",
        "#model.save(last_model_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS3ewyxO_anU"
      },
      "outputs": [],
      "source": [
        "#last_model = load_model(last_model_fpath)\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_val_pred = model.predict(X_val)\n",
        "\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3IyWjdGG4Xq"
      },
      "outputs": [],
      "source": [
        "#best_model = load_model(best_model_fpath)\n",
        "#y_train_pred = best_model.predict(X_train)\n",
        "#y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1aAQBmiy0K"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst2.history['balanced_acc'])\n",
        "plt.plot(hst2.history['val_balanced_acc'])\n",
        "plt.title('model balance_acc after tunning')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K908bbiYwbS"
      },
      "source": [
        "#Testing\n",
        "Result from ISIC Live\n",
        "last_model: 0.506\n",
        "best_model: 0.478"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMY2yvMYxsC"
      },
      "outputs": [],
      "source": [
        "dir_test = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Test_Input/'\n",
        "filepaths = sorted( filter( lambda x: (os.path.isfile(os.path.join(dir_test, x))) and (x.endswith('.jpg')),\n",
        "                        os.listdir(dir_test) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ic95mefkpG3"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame(filepaths, columns =['image'])\n",
        "df_test['FilePaths'] = dir_test + df_test['image']\n",
        "#df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBa1TxPuY8ni"
      },
      "outputs": [],
      "source": [
        "df_test['image_px'] = df_test['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60LYAT7VsNOZ"
      },
      "outputs": [],
      "source": [
        "X_test = np.asarray(df_test['image_px'].tolist())\n",
        "print(np.array(X_test).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7ml90JZ8FK"
      },
      "source": [
        "Calculate y_pred from training and testing for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIX0AmEFNv3Y"
      },
      "outputs": [],
      "source": [
        "# predicted labels\n",
        "Y_pred2 = model.predict(X_test)\n",
        "print(\"Y_pred2\", Y_pred2.shape)\n",
        "# rounded labels\n",
        "y_pred2 = np.argmax(Y_pred2, axis=1)\n",
        "print(\"y_pred2\", y_pred2.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oeArO5CtxGb"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame(Y_pred2, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "df_pred['image'] = df_test['FilePaths'].map(lambda x: x.replace(dir_test, '').replace('.jpg', ''))\n",
        "df_pred = df_pred[['image', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOnjc3RJ0e4T"
      },
      "outputs": [],
      "source": [
        "df_pred.set_index(\"image\", inplace = True)\n",
        "df_pred.to_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/response_DeepSMOTEOversampling_last.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK4zbtoaAaC"
      },
      "source": [
        "#Confusion Metric on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4nEpmkZaTZC"
      },
      "outputs": [],
      "source": [
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1))\n",
        "\n",
        "print(cf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtvW3YeaLlC"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(cf_matrix / cf_matrix.sum(axis=1, keepdims=True), annot=True, \n",
        "            cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix \\n');\n",
        "ax.set_xlabel('\\nPredicted')\n",
        "ax.set_ylabel('Actual ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "ax.yaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,3)\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-1yjWGeKs7"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = df_val['Labels'].value_counts().sort_index()\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uZv-B-ygCD57",
        "cNBXx28B9yGu",
        "0jrJ33lUDkCM",
        "3K908bbiYwbS"
      ],
      "machine_shape": "hm",
      "name": "Skin Cancer Diagnosis using ISIC 2018 Dataset.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}