{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepSMOTE_MNIST.ipynb",
      "provenance": [],
      "mount_file_id": "1CQrhlCFW6aAmo09ETBwDS0Edqxv0TMOh",
      "authorship_tag": "ABX9TyOGPP94QfDDzTX+oqqQAGaT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/Skin-Cancer-Diagnosis/blob/main/DeepSMOTE_Skin%20Cancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRER55BUida6",
        "outputId": "c1988ace-8bf0-4af7-e9f5-c04f4c3dfe65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.3\n",
            "['/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/trn_img/0_trn_img.txt']\n",
            "['/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/trn_lab/0_trn_lab.txt']\n",
            "\n",
            "0\n",
            "cuda\n",
            "/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/trn_img/0_trn_img.txt\n",
            "/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/trn_lab/0_trn_lab.txt\n",
            "train imgs before reshape  (9000, 784)\n",
            "train labels  (9000,)\n",
            "Counter({0.0: 4000, 1.0: 2000, 2.0: 1000, 3.0: 750, 4.0: 500, 5.0: 350, 6.0: 200, 7.0: 100, 8.0: 60, 9.0: 40})\n",
            "train imgs after reshape  (9000, 1, 28, 28)\n",
            "Epoch: 0 \tTrain Loss: 65.889007 \tmse loss: 30.335068 \tmse2 loss: 35.553939\n",
            "Saving..\n",
            "Epoch: 1 \tTrain Loss: 22.285015 \tmse loss: 8.309343 \tmse2 loss: 13.975672\n",
            "Saving..\n",
            "Epoch: 2 \tTrain Loss: 11.938509 \tmse loss: 4.595525 \tmse2 loss: 7.342984\n",
            "Saving..\n",
            "Epoch: 3 \tTrain Loss: 7.579986 \tmse loss: 3.268280 \tmse2 loss: 4.311706\n",
            "Saving..\n",
            "Epoch: 4 \tTrain Loss: 5.710235 \tmse loss: 2.643303 \tmse2 loss: 3.066932\n",
            "Saving..\n",
            "Epoch: 5 \tTrain Loss: 4.854219 \tmse loss: 2.287051 \tmse2 loss: 2.567167\n",
            "Saving..\n",
            "Epoch: 6 \tTrain Loss: 4.134795 \tmse loss: 1.953546 \tmse2 loss: 2.181249\n",
            "Saving..\n",
            "Epoch: 7 \tTrain Loss: 3.339756 \tmse loss: 1.669245 \tmse2 loss: 1.670511\n",
            "Saving..\n",
            "Epoch: 8 \tTrain Loss: 2.949975 \tmse loss: 1.497342 \tmse2 loss: 1.452634\n",
            "Saving..\n",
            "Epoch: 9 \tTrain Loss: 2.676707 \tmse loss: 1.387233 \tmse2 loss: 1.289473\n",
            "Saving..\n",
            "Epoch: 10 \tTrain Loss: 2.342943 \tmse loss: 1.276258 \tmse2 loss: 1.066685\n",
            "Saving..\n",
            "Epoch: 11 \tTrain Loss: 2.249182 \tmse loss: 1.191936 \tmse2 loss: 1.057245\n",
            "Saving..\n",
            "Epoch: 12 \tTrain Loss: 2.057350 \tmse loss: 1.097022 \tmse2 loss: 0.960328\n",
            "Saving..\n",
            "Epoch: 13 \tTrain Loss: 1.931177 \tmse loss: 1.040295 \tmse2 loss: 0.890882\n",
            "Saving..\n",
            "Epoch: 14 \tTrain Loss: 1.749622 \tmse loss: 0.953448 \tmse2 loss: 0.796174\n",
            "Saving..\n",
            "Epoch: 15 \tTrain Loss: 1.736591 \tmse loss: 0.918431 \tmse2 loss: 0.818160\n",
            "Saving..\n",
            "Epoch: 16 \tTrain Loss: 1.628240 \tmse loss: 0.872983 \tmse2 loss: 0.755257\n",
            "Saving..\n",
            "Epoch: 17 \tTrain Loss: 1.495123 \tmse loss: 0.826873 \tmse2 loss: 0.668250\n",
            "Saving..\n",
            "Epoch: 18 \tTrain Loss: 1.432310 \tmse loss: 0.795672 \tmse2 loss: 0.636637\n",
            "Saving..\n",
            "Epoch: 19 \tTrain Loss: 1.383734 \tmse loss: 0.753163 \tmse2 loss: 0.630571\n",
            "Saving..\n",
            "Epoch: 20 \tTrain Loss: 1.362143 \tmse loss: 0.737529 \tmse2 loss: 0.624614\n",
            "Saving..\n",
            "Epoch: 21 \tTrain Loss: 1.277392 \tmse loss: 0.693060 \tmse2 loss: 0.584332\n",
            "Saving..\n",
            "Epoch: 22 \tTrain Loss: 1.207404 \tmse loss: 0.669169 \tmse2 loss: 0.538235\n",
            "Saving..\n",
            "Epoch: 23 \tTrain Loss: 1.182730 \tmse loss: 0.643041 \tmse2 loss: 0.539689\n",
            "Saving..\n",
            "Epoch: 24 \tTrain Loss: 1.219070 \tmse loss: 0.643471 \tmse2 loss: 0.575599\n",
            "Epoch: 25 \tTrain Loss: 1.147808 \tmse loss: 0.616063 \tmse2 loss: 0.531745\n",
            "Saving..\n",
            "Epoch: 26 \tTrain Loss: 0.991900 \tmse loss: 0.570708 \tmse2 loss: 0.421191\n",
            "Saving..\n",
            "Epoch: 27 \tTrain Loss: 0.993126 \tmse loss: 0.537518 \tmse2 loss: 0.455608\n",
            "Epoch: 28 \tTrain Loss: 0.958138 \tmse loss: 0.541583 \tmse2 loss: 0.416554\n",
            "Saving..\n",
            "Epoch: 29 \tTrain Loss: 0.989295 \tmse loss: 0.539463 \tmse2 loss: 0.449833\n",
            "Epoch: 30 \tTrain Loss: 0.907218 \tmse loss: 0.504913 \tmse2 loss: 0.402305\n",
            "Saving..\n",
            "Epoch: 31 \tTrain Loss: 0.962050 \tmse loss: 0.524935 \tmse2 loss: 0.437115\n",
            "Epoch: 32 \tTrain Loss: 0.892578 \tmse loss: 0.489996 \tmse2 loss: 0.402582\n",
            "Saving..\n",
            "Epoch: 33 \tTrain Loss: 0.873477 \tmse loss: 0.484193 \tmse2 loss: 0.389284\n",
            "Saving..\n",
            "Epoch: 34 \tTrain Loss: 0.854800 \tmse loss: 0.464253 \tmse2 loss: 0.390546\n",
            "Saving..\n",
            "Epoch: 35 \tTrain Loss: 0.880914 \tmse loss: 0.483874 \tmse2 loss: 0.397040\n",
            "Epoch: 36 \tTrain Loss: 0.771057 \tmse loss: 0.445938 \tmse2 loss: 0.325119\n",
            "Saving..\n",
            "Epoch: 37 \tTrain Loss: 0.816115 \tmse loss: 0.451547 \tmse2 loss: 0.364567\n",
            "Epoch: 38 \tTrain Loss: 0.817504 \tmse loss: 0.433693 \tmse2 loss: 0.383811\n",
            "Epoch: 39 \tTrain Loss: 0.757865 \tmse loss: 0.421583 \tmse2 loss: 0.336283\n",
            "Saving..\n",
            "Epoch: 40 \tTrain Loss: 0.714909 \tmse loss: 0.396048 \tmse2 loss: 0.318860\n",
            "Saving..\n",
            "Epoch: 41 \tTrain Loss: 0.682509 \tmse loss: 0.393500 \tmse2 loss: 0.289009\n",
            "Saving..\n",
            "Epoch: 42 \tTrain Loss: 0.685129 \tmse loss: 0.394395 \tmse2 loss: 0.290734\n",
            "Epoch: 43 \tTrain Loss: 0.701159 \tmse loss: 0.392168 \tmse2 loss: 0.308991\n",
            "Epoch: 44 \tTrain Loss: 0.698188 \tmse loss: 0.394584 \tmse2 loss: 0.303604\n",
            "Epoch: 45 \tTrain Loss: 0.691640 \tmse loss: 0.379782 \tmse2 loss: 0.311858\n",
            "Epoch: 46 \tTrain Loss: 0.688876 \tmse loss: 0.382968 \tmse2 loss: 0.305908\n",
            "Epoch: 47 \tTrain Loss: 0.733953 \tmse loss: 0.391459 \tmse2 loss: 0.342494\n",
            "Epoch: 48 \tTrain Loss: 0.700061 \tmse loss: 0.382303 \tmse2 loss: 0.317758\n",
            "Epoch: 49 \tTrain Loss: 0.671365 \tmse loss: 0.372732 \tmse2 loss: 0.298633\n",
            "Saving..\n",
            "Epoch: 50 \tTrain Loss: 0.616321 \tmse loss: 0.349609 \tmse2 loss: 0.266712\n",
            "Saving..\n",
            "Epoch: 51 \tTrain Loss: 0.636030 \tmse loss: 0.346692 \tmse2 loss: 0.289339\n",
            "Epoch: 52 \tTrain Loss: 0.578787 \tmse loss: 0.340003 \tmse2 loss: 0.238784\n",
            "Saving..\n",
            "Epoch: 53 \tTrain Loss: 0.584549 \tmse loss: 0.340451 \tmse2 loss: 0.244097\n",
            "Epoch: 54 \tTrain Loss: 0.607404 \tmse loss: 0.338107 \tmse2 loss: 0.269296\n",
            "Epoch: 55 \tTrain Loss: 0.614135 \tmse loss: 0.336219 \tmse2 loss: 0.277916\n",
            "Epoch: 56 \tTrain Loss: 0.568013 \tmse loss: 0.322341 \tmse2 loss: 0.245672\n",
            "Saving..\n",
            "Epoch: 57 \tTrain Loss: 0.553829 \tmse loss: 0.313153 \tmse2 loss: 0.240675\n",
            "Saving..\n",
            "Epoch: 58 \tTrain Loss: 0.598446 \tmse loss: 0.320835 \tmse2 loss: 0.277611\n",
            "Epoch: 59 \tTrain Loss: 0.602365 \tmse loss: 0.334845 \tmse2 loss: 0.267520\n",
            "Epoch: 60 \tTrain Loss: 0.585458 \tmse loss: 0.322933 \tmse2 loss: 0.262525\n",
            "Epoch: 61 \tTrain Loss: 0.545680 \tmse loss: 0.300269 \tmse2 loss: 0.245411\n",
            "Saving..\n",
            "Epoch: 62 \tTrain Loss: 0.561445 \tmse loss: 0.307922 \tmse2 loss: 0.253523\n",
            "Epoch: 63 \tTrain Loss: 0.527887 \tmse loss: 0.299442 \tmse2 loss: 0.228445\n",
            "Saving..\n",
            "Epoch: 64 \tTrain Loss: 0.529482 \tmse loss: 0.296490 \tmse2 loss: 0.232992\n",
            "Epoch: 65 \tTrain Loss: 0.507435 \tmse loss: 0.293265 \tmse2 loss: 0.214169\n",
            "Saving..\n",
            "Epoch: 66 \tTrain Loss: 0.537880 \tmse loss: 0.293688 \tmse2 loss: 0.244192\n",
            "Epoch: 67 \tTrain Loss: 0.505399 \tmse loss: 0.286801 \tmse2 loss: 0.218598\n",
            "Saving..\n",
            "Epoch: 68 \tTrain Loss: 0.513644 \tmse loss: 0.277464 \tmse2 loss: 0.236180\n",
            "Epoch: 69 \tTrain Loss: 0.479392 \tmse loss: 0.283674 \tmse2 loss: 0.195718\n",
            "Saving..\n",
            "Epoch: 70 \tTrain Loss: 0.480095 \tmse loss: 0.271973 \tmse2 loss: 0.208122\n",
            "Epoch: 71 \tTrain Loss: 0.477124 \tmse loss: 0.268350 \tmse2 loss: 0.208774\n",
            "Saving..\n",
            "Epoch: 72 \tTrain Loss: 0.474728 \tmse loss: 0.269815 \tmse2 loss: 0.204913\n",
            "Saving..\n",
            "Epoch: 73 \tTrain Loss: 0.459702 \tmse loss: 0.260233 \tmse2 loss: 0.199469\n",
            "Saving..\n",
            "Epoch: 74 \tTrain Loss: 0.511804 \tmse loss: 0.280507 \tmse2 loss: 0.231297\n",
            "Epoch: 75 \tTrain Loss: 0.460258 \tmse loss: 0.263362 \tmse2 loss: 0.196896\n",
            "Epoch: 76 \tTrain Loss: 0.483385 \tmse loss: 0.273805 \tmse2 loss: 0.209581\n",
            "Epoch: 77 \tTrain Loss: 0.477150 \tmse loss: 0.263949 \tmse2 loss: 0.213201\n",
            "Epoch: 78 \tTrain Loss: 0.472799 \tmse loss: 0.267210 \tmse2 loss: 0.205589\n",
            "Epoch: 79 \tTrain Loss: 0.440196 \tmse loss: 0.249196 \tmse2 loss: 0.191000\n",
            "Saving..\n",
            "Epoch: 80 \tTrain Loss: 0.454147 \tmse loss: 0.247227 \tmse2 loss: 0.206920\n",
            "Epoch: 81 \tTrain Loss: 0.436379 \tmse loss: 0.249956 \tmse2 loss: 0.186424\n",
            "Saving..\n",
            "Epoch: 82 \tTrain Loss: 0.445587 \tmse loss: 0.250731 \tmse2 loss: 0.194856\n",
            "Epoch: 83 \tTrain Loss: 0.440990 \tmse loss: 0.240253 \tmse2 loss: 0.200737\n",
            "Epoch: 84 \tTrain Loss: 0.414676 \tmse loss: 0.233726 \tmse2 loss: 0.180950\n",
            "Saving..\n",
            "Epoch: 85 \tTrain Loss: 0.429685 \tmse loss: 0.234390 \tmse2 loss: 0.195294\n",
            "Epoch: 86 \tTrain Loss: 0.426524 \tmse loss: 0.235608 \tmse2 loss: 0.190915\n",
            "Epoch: 87 \tTrain Loss: 0.427006 \tmse loss: 0.242421 \tmse2 loss: 0.184585\n",
            "Epoch: 88 \tTrain Loss: 0.418887 \tmse loss: 0.232229 \tmse2 loss: 0.186658\n",
            "Epoch: 89 \tTrain Loss: 0.438129 \tmse loss: 0.237304 \tmse2 loss: 0.200824\n",
            "Epoch: 90 \tTrain Loss: 0.413081 \tmse loss: 0.229003 \tmse2 loss: 0.184078\n",
            "Saving..\n",
            "Epoch: 91 \tTrain Loss: 0.427150 \tmse loss: 0.239998 \tmse2 loss: 0.187152\n",
            "Epoch: 92 \tTrain Loss: 0.400903 \tmse loss: 0.228693 \tmse2 loss: 0.172209\n",
            "Saving..\n",
            "Epoch: 93 \tTrain Loss: 0.415183 \tmse loss: 0.230887 \tmse2 loss: 0.184297\n",
            "Epoch: 94 \tTrain Loss: 0.371696 \tmse loss: 0.216858 \tmse2 loss: 0.154838\n",
            "Saving..\n",
            "Epoch: 95 \tTrain Loss: 0.377766 \tmse loss: 0.214347 \tmse2 loss: 0.163419\n",
            "Epoch: 96 \tTrain Loss: 0.384007 \tmse loss: 0.218247 \tmse2 loss: 0.165760\n",
            "Epoch: 97 \tTrain Loss: 0.374594 \tmse loss: 0.210659 \tmse2 loss: 0.163935\n",
            "Epoch: 98 \tTrain Loss: 0.371356 \tmse loss: 0.213711 \tmse2 loss: 0.157644\n",
            "Saving..\n",
            "Epoch: 99 \tTrain Loss: 0.372952 \tmse loss: 0.216659 \tmse2 loss: 0.156293\n",
            "Epoch: 100 \tTrain Loss: 0.381065 \tmse loss: 0.217644 \tmse2 loss: 0.163421\n",
            "Epoch: 101 \tTrain Loss: 0.394876 \tmse loss: 0.213879 \tmse2 loss: 0.180996\n",
            "Epoch: 102 \tTrain Loss: 0.353607 \tmse loss: 0.198655 \tmse2 loss: 0.154952\n",
            "Saving..\n",
            "Epoch: 103 \tTrain Loss: 0.350772 \tmse loss: 0.199581 \tmse2 loss: 0.151192\n",
            "Saving..\n",
            "Epoch: 104 \tTrain Loss: 0.363345 \tmse loss: 0.204854 \tmse2 loss: 0.158491\n",
            "Epoch: 105 \tTrain Loss: 0.355941 \tmse loss: 0.200933 \tmse2 loss: 0.155008\n",
            "Epoch: 106 \tTrain Loss: 0.338641 \tmse loss: 0.192579 \tmse2 loss: 0.146062\n",
            "Saving..\n",
            "Epoch: 107 \tTrain Loss: 0.349112 \tmse loss: 0.196064 \tmse2 loss: 0.153047\n",
            "Epoch: 108 \tTrain Loss: 0.354004 \tmse loss: 0.199906 \tmse2 loss: 0.154097\n",
            "Epoch: 109 \tTrain Loss: 0.366291 \tmse loss: 0.200200 \tmse2 loss: 0.166092\n",
            "Epoch: 110 \tTrain Loss: 0.343817 \tmse loss: 0.200122 \tmse2 loss: 0.143695\n",
            "Epoch: 111 \tTrain Loss: 0.319178 \tmse loss: 0.189874 \tmse2 loss: 0.129304\n",
            "Saving..\n",
            "Epoch: 112 \tTrain Loss: 0.346222 \tmse loss: 0.190748 \tmse2 loss: 0.155474\n",
            "Epoch: 113 \tTrain Loss: 0.347537 \tmse loss: 0.189503 \tmse2 loss: 0.158035\n",
            "Epoch: 114 \tTrain Loss: 0.336855 \tmse loss: 0.191103 \tmse2 loss: 0.145751\n",
            "Epoch: 115 \tTrain Loss: 0.329997 \tmse loss: 0.186970 \tmse2 loss: 0.143027\n",
            "Epoch: 116 \tTrain Loss: 0.342637 \tmse loss: 0.183220 \tmse2 loss: 0.159418\n",
            "Epoch: 117 \tTrain Loss: 0.338018 \tmse loss: 0.187946 \tmse2 loss: 0.150072\n",
            "Epoch: 118 \tTrain Loss: 0.313402 \tmse loss: 0.178721 \tmse2 loss: 0.134681\n",
            "Saving..\n",
            "Epoch: 119 \tTrain Loss: 0.314719 \tmse loss: 0.176922 \tmse2 loss: 0.137797\n",
            "Epoch: 120 \tTrain Loss: 0.313860 \tmse loss: 0.175458 \tmse2 loss: 0.138403\n",
            "Epoch: 121 \tTrain Loss: 0.302280 \tmse loss: 0.173493 \tmse2 loss: 0.128787\n",
            "Saving..\n",
            "Epoch: 122 \tTrain Loss: 0.308956 \tmse loss: 0.173626 \tmse2 loss: 0.135330\n",
            "Epoch: 123 \tTrain Loss: 0.305982 \tmse loss: 0.172721 \tmse2 loss: 0.133261\n",
            "Epoch: 124 \tTrain Loss: 0.326613 \tmse loss: 0.176726 \tmse2 loss: 0.149887\n",
            "Epoch: 125 \tTrain Loss: 0.302499 \tmse loss: 0.172854 \tmse2 loss: 0.129645\n",
            "Epoch: 126 \tTrain Loss: 0.298226 \tmse loss: 0.168545 \tmse2 loss: 0.129680\n",
            "Saving..\n",
            "Epoch: 127 \tTrain Loss: 0.294769 \tmse loss: 0.166506 \tmse2 loss: 0.128263\n",
            "Saving..\n",
            "Epoch: 128 \tTrain Loss: 0.289207 \tmse loss: 0.168336 \tmse2 loss: 0.120872\n",
            "Saving..\n",
            "Epoch: 129 \tTrain Loss: 0.288256 \tmse loss: 0.163074 \tmse2 loss: 0.125181\n",
            "Saving..\n",
            "Epoch: 130 \tTrain Loss: 0.287836 \tmse loss: 0.164536 \tmse2 loss: 0.123300\n",
            "Saving..\n",
            "Epoch: 131 \tTrain Loss: 0.290289 \tmse loss: 0.165663 \tmse2 loss: 0.124626\n",
            "Epoch: 132 \tTrain Loss: 0.299794 \tmse loss: 0.164158 \tmse2 loss: 0.135636\n",
            "Epoch: 133 \tTrain Loss: 0.293322 \tmse loss: 0.161217 \tmse2 loss: 0.132104\n",
            "Epoch: 134 \tTrain Loss: 0.283194 \tmse loss: 0.157842 \tmse2 loss: 0.125352\n",
            "Saving..\n",
            "Epoch: 135 \tTrain Loss: 0.284192 \tmse loss: 0.157240 \tmse2 loss: 0.126953\n",
            "Epoch: 136 \tTrain Loss: 0.282207 \tmse loss: 0.160316 \tmse2 loss: 0.121891\n",
            "Saving..\n",
            "Epoch: 137 \tTrain Loss: 0.277196 \tmse loss: 0.153164 \tmse2 loss: 0.124031\n",
            "Saving..\n",
            "Epoch: 138 \tTrain Loss: 0.269476 \tmse loss: 0.152077 \tmse2 loss: 0.117399\n",
            "Saving..\n",
            "Epoch: 139 \tTrain Loss: 0.271389 \tmse loss: 0.148310 \tmse2 loss: 0.123079\n",
            "Epoch: 140 \tTrain Loss: 0.274279 \tmse loss: 0.152788 \tmse2 loss: 0.121491\n",
            "Epoch: 141 \tTrain Loss: 0.289336 \tmse loss: 0.156829 \tmse2 loss: 0.132508\n",
            "Epoch: 142 \tTrain Loss: 0.288639 \tmse loss: 0.150679 \tmse2 loss: 0.137960\n",
            "Epoch: 143 \tTrain Loss: 0.258371 \tmse loss: 0.148504 \tmse2 loss: 0.109867\n",
            "Saving..\n",
            "Epoch: 144 \tTrain Loss: 0.252812 \tmse loss: 0.146726 \tmse2 loss: 0.106086\n",
            "Saving..\n",
            "Epoch: 145 \tTrain Loss: 0.260586 \tmse loss: 0.148624 \tmse2 loss: 0.111962\n",
            "Epoch: 146 \tTrain Loss: 0.260593 \tmse loss: 0.146381 \tmse2 loss: 0.114212\n",
            "Epoch: 147 \tTrain Loss: 0.261596 \tmse loss: 0.146575 \tmse2 loss: 0.115021\n",
            "Epoch: 148 \tTrain Loss: 0.257192 \tmse loss: 0.147625 \tmse2 loss: 0.109567\n",
            "Epoch: 149 \tTrain Loss: 0.251682 \tmse loss: 0.141785 \tmse2 loss: 0.109897\n",
            "Saving..\n",
            "Epoch: 150 \tTrain Loss: 0.239702 \tmse loss: 0.139309 \tmse2 loss: 0.100393\n",
            "Saving..\n",
            "Epoch: 151 \tTrain Loss: 0.240197 \tmse loss: 0.138669 \tmse2 loss: 0.101528\n",
            "Epoch: 152 \tTrain Loss: 0.255241 \tmse loss: 0.140670 \tmse2 loss: 0.114571\n",
            "Epoch: 153 \tTrain Loss: 0.253453 \tmse loss: 0.141405 \tmse2 loss: 0.112048\n",
            "Epoch: 154 \tTrain Loss: 0.261185 \tmse loss: 0.144701 \tmse2 loss: 0.116484\n",
            "Epoch: 155 \tTrain Loss: 0.257224 \tmse loss: 0.144462 \tmse2 loss: 0.112761\n",
            "Epoch: 156 \tTrain Loss: 0.233913 \tmse loss: 0.136819 \tmse2 loss: 0.097094\n",
            "Saving..\n",
            "Epoch: 157 \tTrain Loss: 0.243421 \tmse loss: 0.137173 \tmse2 loss: 0.106248\n",
            "Epoch: 158 \tTrain Loss: 0.249134 \tmse loss: 0.135239 \tmse2 loss: 0.113895\n",
            "Epoch: 159 \tTrain Loss: 0.248461 \tmse loss: 0.135982 \tmse2 loss: 0.112478\n",
            "Epoch: 160 \tTrain Loss: 0.239539 \tmse loss: 0.134892 \tmse2 loss: 0.104648\n",
            "Epoch: 161 \tTrain Loss: 0.244005 \tmse loss: 0.138281 \tmse2 loss: 0.105724\n",
            "Epoch: 162 \tTrain Loss: 0.234875 \tmse loss: 0.132975 \tmse2 loss: 0.101900\n",
            "Epoch: 163 \tTrain Loss: 0.229646 \tmse loss: 0.129778 \tmse2 loss: 0.099869\n",
            "Saving..\n",
            "Epoch: 164 \tTrain Loss: 0.228152 \tmse loss: 0.130625 \tmse2 loss: 0.097527\n",
            "Saving..\n",
            "Epoch: 165 \tTrain Loss: 0.240171 \tmse loss: 0.130106 \tmse2 loss: 0.110065\n",
            "Epoch: 166 \tTrain Loss: 0.226355 \tmse loss: 0.127327 \tmse2 loss: 0.099028\n",
            "Saving..\n",
            "Epoch: 167 \tTrain Loss: 0.221593 \tmse loss: 0.125283 \tmse2 loss: 0.096311\n",
            "Saving..\n",
            "Epoch: 168 \tTrain Loss: 0.223586 \tmse loss: 0.125356 \tmse2 loss: 0.098230\n",
            "Epoch: 169 \tTrain Loss: 0.231702 \tmse loss: 0.129238 \tmse2 loss: 0.102464\n",
            "Epoch: 170 \tTrain Loss: 0.221730 \tmse loss: 0.127379 \tmse2 loss: 0.094351\n",
            "Epoch: 171 \tTrain Loss: 0.233493 \tmse loss: 0.131749 \tmse2 loss: 0.101744\n",
            "Epoch: 172 \tTrain Loss: 0.231525 \tmse loss: 0.127980 \tmse2 loss: 0.103546\n",
            "Epoch: 173 \tTrain Loss: 0.228548 \tmse loss: 0.130858 \tmse2 loss: 0.097690\n",
            "Epoch: 174 \tTrain Loss: 0.213570 \tmse loss: 0.122835 \tmse2 loss: 0.090735\n",
            "Saving..\n",
            "Epoch: 175 \tTrain Loss: 0.205296 \tmse loss: 0.119289 \tmse2 loss: 0.086008\n",
            "Saving..\n",
            "Epoch: 176 \tTrain Loss: 0.214994 \tmse loss: 0.119828 \tmse2 loss: 0.095165\n",
            "Epoch: 177 \tTrain Loss: 0.219132 \tmse loss: 0.123203 \tmse2 loss: 0.095929\n",
            "Epoch: 178 \tTrain Loss: 0.228126 \tmse loss: 0.123845 \tmse2 loss: 0.104281\n",
            "Epoch: 179 \tTrain Loss: 0.210960 \tmse loss: 0.118278 \tmse2 loss: 0.092682\n",
            "Epoch: 180 \tTrain Loss: 0.203436 \tmse loss: 0.117049 \tmse2 loss: 0.086387\n",
            "Saving..\n",
            "Epoch: 181 \tTrain Loss: 0.203354 \tmse loss: 0.115365 \tmse2 loss: 0.087989\n",
            "Saving..\n",
            "Epoch: 182 \tTrain Loss: 0.195647 \tmse loss: 0.114709 \tmse2 loss: 0.080938\n",
            "Saving..\n",
            "Epoch: 183 \tTrain Loss: 0.197603 \tmse loss: 0.112758 \tmse2 loss: 0.084844\n",
            "Epoch: 184 \tTrain Loss: 0.212276 \tmse loss: 0.115571 \tmse2 loss: 0.096705\n",
            "Epoch: 185 \tTrain Loss: 0.209667 \tmse loss: 0.116990 \tmse2 loss: 0.092677\n",
            "Epoch: 186 \tTrain Loss: 0.201060 \tmse loss: 0.115127 \tmse2 loss: 0.085934\n",
            "Epoch: 187 \tTrain Loss: 0.201717 \tmse loss: 0.117039 \tmse2 loss: 0.084678\n",
            "Epoch: 188 \tTrain Loss: 0.196589 \tmse loss: 0.114181 \tmse2 loss: 0.082408\n",
            "Epoch: 189 \tTrain Loss: 0.191350 \tmse loss: 0.106526 \tmse2 loss: 0.084824\n",
            "Saving..\n",
            "Epoch: 190 \tTrain Loss: 0.193662 \tmse loss: 0.114213 \tmse2 loss: 0.079449\n",
            "Epoch: 191 \tTrain Loss: 0.203840 \tmse loss: 0.114794 \tmse2 loss: 0.089047\n",
            "Epoch: 192 \tTrain Loss: 0.197587 \tmse loss: 0.112715 \tmse2 loss: 0.084872\n",
            "Epoch: 193 \tTrain Loss: 0.196633 \tmse loss: 0.107882 \tmse2 loss: 0.088751\n",
            "Epoch: 194 \tTrain Loss: 0.190220 \tmse loss: 0.109255 \tmse2 loss: 0.080965\n",
            "Saving..\n",
            "Epoch: 195 \tTrain Loss: 0.180729 \tmse loss: 0.106102 \tmse2 loss: 0.074627\n",
            "Saving..\n",
            "Epoch: 196 \tTrain Loss: 0.203339 \tmse loss: 0.109023 \tmse2 loss: 0.094316\n",
            "Epoch: 197 \tTrain Loss: 0.211925 \tmse loss: 0.115609 \tmse2 loss: 0.096316\n",
            "Epoch: 198 \tTrain Loss: 0.201630 \tmse loss: 0.108820 \tmse2 loss: 0.092810\n",
            "Epoch: 199 \tTrain Loss: 0.185124 \tmse loss: 0.105971 \tmse2 loss: 0.079153\n",
            "/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/models/crs5/0/f_enc.pth\n",
            "/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/models/crs5/0/f_dec.pth\n",
            "\n",
            "total time(min): 24.54\n",
            "final time(min): 24.62\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(torch.version.cuda) #10.1\n",
        "t3 = time.time()\n",
        "##############################################################################\n",
        "\"\"\"args for AE\"\"\"\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 1#3    # number of channels in the input data \n",
        "\n",
        "args['n_z'] = 300 #600     # number of dimensions in latent space. \n",
        "\n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 200       # how many epochs to run for\n",
        "args['batch_size'] = 100   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = True       # train networks if True, else load networks from\n",
        "\n",
        "args['dataset'] = 'mnist'  #'fmnist' # specify which dataset to use\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "\n",
        "\n",
        "## create encoder model and decoder model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        \n",
        "        # convolutional filters, work excellent with image data\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
        "            #nn.ReLU(True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            #nn.ReLU(True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            #nn.ReLU(True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            \n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n",
        "            \n",
        "            #3d and 32 by 32\n",
        "            #nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 1, 0, bias=False),\n",
        "            \n",
        "            nn.BatchNorm2d(self.dim_h * 8), # 40 X 8 = 320\n",
        "            #nn.ReLU(True),\n",
        "            nn.LeakyReLU(0.2, inplace=True) )#,\n",
        "            #nn.Conv2d(self.dim_h * 8, 1, 2, 1, 0, bias=False))\n",
        "            #nn.Conv2d(self.dim_h * 8, 1, 4, 1, 0, bias=False))\n",
        "        # final layer is fully connected\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('enc')\n",
        "        #print('input ',x.size()) #torch.Size([100, 3,32,32])\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        #print('aft squeeze ',x.size()) #torch.Size([128, 320])\n",
        "        #aft squeeze  torch.Size([100, 320])\n",
        "        x = self.fc(x)\n",
        "        #print('out ',x.size()) #torch.Size([128, 20])\n",
        "        #out  torch.Size([100, 300])\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 8 * 7 * 7),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, 1, 4, stride=2),\n",
        "            #nn.Sigmoid())\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('dec')\n",
        "        #print('input ',x.size())\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 8, 7, 7)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"functions to create SMOTE images\"\"\"\n",
        "\n",
        "def biased_get_class(c):\n",
        "    \n",
        "    xbeg = dec_x[dec_y == c]\n",
        "    ybeg = dec_y[dec_y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "\n",
        "    # determining the number of samples to generate\n",
        "    #n_to_sample = 10 \n",
        "\n",
        "    # fitting the model\n",
        "    n_neigh = 5 + 1\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "#xsamp, ysamp = SM(xclass,yclass)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "#NOTE: Download the training ('.../0_trn_img.txt') and label files \n",
        "# ('.../0_trn_lab.txt').  Place the files in directories (e.g., ../MNIST/trn_img/\n",
        "# and /MNIST/trn_lab/).  Originally, when the code was written, it was for 5 fold\n",
        "#cross validation and hence there were 5 files in each of the \n",
        "#directories.  Here, for illustration, we use only 1 training and 1 label\n",
        "#file (e.g., '.../0_trn_img.txt' and '.../0_trn_lab.txt').\n",
        "\n",
        "dtrnimg = '/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/trn_img/'\n",
        "dtrnlab = '/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/trn_lab/'\n",
        "\n",
        "ids = os.listdir(dtrnimg)\n",
        "idtri_f = [os.path.join(dtrnimg, image_id) for image_id in ids]\n",
        "print(idtri_f)\n",
        "\n",
        "ids = os.listdir(dtrnlab)\n",
        "idtrl_f = [os.path.join(dtrnlab, image_id) for image_id in ids]\n",
        "print(idtrl_f)\n",
        "\n",
        "#for i in range(5):\n",
        "for i in range(len(ids)):\n",
        "    print()\n",
        "    print(i)\n",
        "    encoder = Encoder(args)\n",
        "    decoder = Decoder(args)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(device)\n",
        "    decoder = decoder.to(device)\n",
        "    encoder = encoder.to(device)\n",
        "\n",
        "    train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "    #decoder loss function\n",
        "    criterion = nn.MSELoss()\n",
        "    criterion = criterion.to(device)\n",
        "    \n",
        "    trnimgfile = idtri_f[i]\n",
        "    trnlabfile = idtrl_f[i]\n",
        "    \n",
        "    print(trnimgfile)\n",
        "    print(trnlabfile)\n",
        "    dec_x = np.loadtxt(trnimgfile) \n",
        "    dec_y = np.loadtxt(trnlabfile)\n",
        "\n",
        "    print('train imgs before reshape ',dec_x.shape) \n",
        "    print('train labels ',dec_y.shape) \n",
        "    print(collections.Counter(dec_y))\n",
        "    dec_x = dec_x.reshape(dec_x.shape[0],1,28,28)   \n",
        "    print('train imgs after reshape ',dec_x.shape) \n",
        "\n",
        "    batch_size = 100\n",
        "    num_workers = 0\n",
        "\n",
        "    #torch.Tensor returns float so if want long then use torch.tensor\n",
        "    tensor_x = torch.Tensor(dec_x)\n",
        "    tensor_y = torch.tensor(dec_y,dtype=torch.long)\n",
        "    mnist_bal = TensorDataset(tensor_x,tensor_y) \n",
        "    train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
        "        batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
        "    \n",
        "    classes = ('0', '1', '2', '3', '4',\n",
        "           '5', '6', '7', '8', '9')\n",
        "\n",
        "    best_loss = np.inf\n",
        "\n",
        "    t0 = time.time()\n",
        "    if args['train']:\n",
        "        enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
        "        dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
        "    \n",
        "        for epoch in range(args['epochs']):\n",
        "            train_loss = 0.0\n",
        "            tmse_loss = 0.0\n",
        "            tdiscr_loss = 0.0\n",
        "            # train for one epoch -- set nets to train mode\n",
        "            encoder.train()\n",
        "            decoder.train()\n",
        "        \n",
        "            for images,labs in train_loader:\n",
        "            \n",
        "                # zero gradients for each batch\n",
        "                encoder.zero_grad()\n",
        "                decoder.zero_grad()\n",
        "                #print(images)\n",
        "                images, labs = images.to(device), labs.to(device)\n",
        "                #print('images ',images.size()) \n",
        "                labsn = labs.detach().cpu().numpy()\n",
        "                #print('labsn ',labsn.shape, labsn)\n",
        "            \n",
        "                # run images\n",
        "                z_hat = encoder(images)\n",
        "            \n",
        "                x_hat = decoder(z_hat) #decoder outputs tanh\n",
        "                #print('xhat ', x_hat.size())\n",
        "                #print(x_hat)\n",
        "                mse = criterion(x_hat,images)\n",
        "                #print('mse ',mse)\n",
        "                \n",
        "                       \n",
        "                resx = []\n",
        "                resy = []\n",
        "            \n",
        "                tc = np.random.choice(10,1)\n",
        "                #tc = 9\n",
        "                xbeg = dec_x[dec_y == tc]\n",
        "                ybeg = dec_y[dec_y == tc] \n",
        "                xlen = len(xbeg)\n",
        "                nsamp = min(xlen, 100)\n",
        "                ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
        "                xclass = xbeg[ind]\n",
        "                yclass = ybeg[ind]\n",
        "            \n",
        "                xclen = len(xclass)\n",
        "                #print('xclen ',xclen)\n",
        "                xcminus = np.arange(1,xclen)\n",
        "                #print('minus ',xcminus.shape,xcminus)\n",
        "                \n",
        "                xcplus = np.append(xcminus,0)\n",
        "                #print('xcplus ',xcplus)\n",
        "                xcnew = (xclass[[xcplus],:])\n",
        "                #xcnew = np.squeeze(xcnew)\n",
        "                xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
        "                #print('xcnew ',xcnew.shape)\n",
        "            \n",
        "                xcnew = torch.Tensor(xcnew)\n",
        "                xcnew = xcnew.to(device)\n",
        "            \n",
        "                #encode xclass to feature space\n",
        "                xclass = torch.Tensor(xclass)\n",
        "                xclass = xclass.to(device)\n",
        "                xclass = encoder(xclass)\n",
        "                #print('xclass ',xclass.shape) \n",
        "            \n",
        "                xclass = xclass.detach().cpu().numpy()\n",
        "            \n",
        "                xc_enc = (xclass[[xcplus],:])\n",
        "                xc_enc = np.squeeze(xc_enc)\n",
        "                #print('xc enc ',xc_enc.shape)\n",
        "            \n",
        "                xc_enc = torch.Tensor(xc_enc)\n",
        "                xc_enc = xc_enc.to(device)\n",
        "                \n",
        "                ximg = decoder(xc_enc)\n",
        "                \n",
        "                mse2 = criterion(ximg,xcnew)\n",
        "            \n",
        "                comb_loss = mse2 + mse\n",
        "                comb_loss.backward()\n",
        "            \n",
        "                enc_optim.step()\n",
        "                dec_optim.step()\n",
        "            \n",
        "                train_loss += comb_loss.item()*images.size(0)\n",
        "                tmse_loss += mse.item()*images.size(0)\n",
        "                tdiscr_loss += mse2.item()*images.size(0)\n",
        "            \n",
        "                 \n",
        "            # print avg training statistics \n",
        "            train_loss = train_loss/len(train_loader)\n",
        "            tmse_loss = tmse_loss/len(train_loader)\n",
        "            tdiscr_loss = tdiscr_loss/len(train_loader)\n",
        "            print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
        "                    train_loss,tmse_loss,tdiscr_loss))\n",
        "            \n",
        "        \n",
        "        \n",
        "            #store the best encoder and decoder models\n",
        "            #here, /crs5 is a reference to 5 way cross validation, but is not\n",
        "            #necessary for illustration purposes\n",
        "            if train_loss < best_loss:\n",
        "                print('Saving..')\n",
        "                path_enc = '/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/models/crs5/' \\\n",
        "                    + str(i) + '/bst_enc.pth'\n",
        "                path_dec = '/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/models/crs5/' \\\n",
        "                    + str(i) + '/bst_dec.pth'\n",
        "             \n",
        "                torch.save(encoder.state_dict(), path_enc)\n",
        "                torch.save(decoder.state_dict(), path_dec)\n",
        "        \n",
        "                best_loss = train_loss\n",
        "        \n",
        "        \n",
        "        #in addition, store the final model (may not be the best) for\n",
        "        #informational purposes\n",
        "        path_enc = '/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/models/crs5/' \\\n",
        "            + str(i) + '/f_enc.pth'\n",
        "        path_dec = '/content/drive/MyDrive/PHD/DeepSMOTE/MNIST/models/crs5/' \\\n",
        "            + str(i) + '/f_dec.pth'\n",
        "        print(path_enc)\n",
        "        print(path_dec)\n",
        "        torch.save(encoder.state_dict(), path_enc)\n",
        "        torch.save(decoder.state_dict(), path_dec)\n",
        "        print()\n",
        "              \n",
        "    t1 = time.time()\n",
        "    print('total time(min): {:.2f}'.format((t1 - t0)/60))             \n",
        " \n",
        "t4 = time.time()\n",
        "print('final time(min): {:.2f}'.format((t4 - t3)/60))\n"
      ]
    }
  ]
}