{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/Skin-Cancer-Diagnosis/blob/main/Skin_Cancer_Diagnosis_using_ISIC_2018_Dataset_SMOTE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUusDE1Z9TNb"
      },
      "source": [
        "Prepare the dataset. \n",
        "Currently, we use skin cancer ISIC dataset from Kaggle https://www.kaggle.com/datasets/nodoubttome/skin-cancer9-classesisic\n",
        "\n",
        "Tutorial for how to load Kaggle dataset can be found in https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eus_4tUgfEk9",
        "outputId": "2f978142-44bf-4ab2-dadc-eb4e55ccb438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRGeofw-8tK"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR2MJBYq-oiB",
        "outputId": "346111cf-1681-419c-8b4a-7321a5c50389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "\n",
        "!pip install imbalanced-learn\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mnEebdJH6Ex"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv') \n",
        "df_val = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv') \n",
        "num_classes = 7\n",
        "#df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFSe3uekK67v"
      },
      "outputs": [],
      "source": [
        "#decode one hot label\n",
        "df_train[\"Labels\"] = (df_train.iloc[:, 1:]).idxmax(axis=1)\n",
        "df_val[\"Labels\"] = (df_val.iloc[:, 1:]).idxmax(axis=1)\n",
        "\n",
        "#drop one-hot column\n",
        "df_train = df_train.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "df_val = df_val.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "\n",
        "#make filepaths of the image\n",
        "dir_train = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_Input/'\n",
        "dir_val = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_Input/'\n",
        "df_train['FilePaths'] = dir_train + df_train['image'] + '.jpg'\n",
        "df_val['FilePaths'] = dir_val + df_val['image'] + '.jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38f3dgvyBqFM"
      },
      "source": [
        "Label Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "2IncA-_o_n5w",
        "outputId": "36b929fa-c3aa-48f7-a9ee-7d317d309257"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Text(0, 0, 'DF'),\n",
              " Text(0, 0, 'VASC'),\n",
              " Text(0, 0, 'AKIEC'),\n",
              " Text(0, 0, 'BCC'),\n",
              " Text(0, 0, 'BKL'),\n",
              " Text(0, 0, 'MEL'),\n",
              " Text(0, 0, 'NV')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = df_train['Labels'].value_counts(ascending=True)\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKMKSb4Bkym"
      },
      "source": [
        "Plot 3 images per label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnVuqbFBW3K"
      },
      "outputs": [],
      "source": [
        "def plot_images_per_label(df, label, cols: int, size: tuple):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=cols, figsize=size)\n",
        "\n",
        "    cntMax = cols\n",
        "    cntCur = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if(row['Labels'] == label and cntCur < cntMax):\n",
        "            axs[cntCur].imshow(plt.imread(df.FilePaths[index]))\n",
        "            axs[cntCur].set_title(df.Labels[index])\n",
        "\n",
        "            cntCur += 1\n",
        "        else:\n",
        "            if(cntCur >= cntMax):\n",
        "                break\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# unique labels\n",
        "labels = sorted(df_train['Labels'].unique())\n",
        "#for label in labels:\n",
        "#    plot_images_per_label(df_train, label, 3, (12,9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRKKrNacAZtl"
      },
      "source": [
        "Drop duplicate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERwfyPDHP-zC"
      },
      "outputs": [],
      "source": [
        "df_group = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_LesionGroupings.csv') \n",
        "df_train = df_train.set_index('image').join(df_group.set_index('image'))\n",
        "df_train = df_train.drop_duplicates(subset=['lesion_id'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h35T8vKRVV1Y"
      },
      "source": [
        "Manual undersampling majority class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeldhlTdVQlT"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.drop(df_train[df_train['Labels'] == 'NV'].sample(frac=.3).index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKjC59JOB_6d"
      },
      "source": [
        "Prepare X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-c7Xghg4SB4"
      },
      "outputs": [],
      "source": [
        "# input image size\n",
        "IMAGE_W = 224\n",
        "IMAGE_H = 224\n",
        "IMG_SIZE = (IMAGE_W,IMAGE_H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jyCpXnlFoQK"
      },
      "outputs": [],
      "source": [
        "#TIME CONSUMING OPERATION\n",
        "#from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "#X = []\n",
        "#for img in df['FilePaths']:\n",
        "    #img_arr = load_img(img, target_size=IMG_SIZE)\n",
        "#    with load_img(img, target_size=IMG_SIZE) as img_arr:\n",
        "#      X.append(img_to_array(img_arr))\n",
        "\n",
        "#X = np.array(X)\n",
        "df_train['image_px'] = df_train['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "df_val['image_px'] = df_val['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZyZMydSgvZo",
        "outputId": "0e1821f8-3736-4079-b159-b2a0532029d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8003, 224, 224, 3)\n",
            "(193, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "X_train = np.asarray(df_train['image_px'].tolist())\n",
        "X_val = np.asarray(df_val['image_px'].tolist())\n",
        "print(np.array(X_train).shape)\n",
        "print(np.array(X_val).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAxUkXy8ueYG"
      },
      "outputs": [],
      "source": [
        "#Normalization\n",
        "#X_train_mean = np.mean(X_train)\n",
        "#X_train_std = np.std(X_train)\n",
        "\n",
        "#X_train = (X_train - X_train_mean)/X_train_std\n",
        "#X_val = (X_val - X_train_mean)/X_train_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqYLmicGAjZz",
        "outputId": "747dddf6-55c4-456e-f39a-7a5d34d4622e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'NV': 4693, 'MEL': 1113, 'BKL': 1099, 'BCC': 514, 'AKIEC': 327, 'VASC': 142, 'DF': 115})\n",
            "(8003,)\n"
          ]
        }
      ],
      "source": [
        "y_train = np.array(df_train['Labels'].values)\n",
        "\n",
        "# summarize class distribution\n",
        "from collections import Counter\n",
        "counter = Counter(y_train)\n",
        "print(counter)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEuVIGc3g859",
        "outputId": "7e6143c2-3a8d-46d3-9440-df68756b998d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'NV': 123, 'BKL': 22, 'MEL': 21, 'BCC': 15, 'AKIEC': 8, 'VASC': 3, 'DF': 1})\n",
            "(193,)\n"
          ]
        }
      ],
      "source": [
        "y_val = np.array(df_val['Labels'].values)\n",
        "print(Counter(y_val))\n",
        "print(y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfvEVGIQhIr2",
        "outputId": "496e7f2a-0423-4313-bab9-e9a9a63c070a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 5, 5, ..., 4, 2, 5])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#label encoding\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_val = label_encoder.fit_transform(y_val)\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLHRj-xqk98R"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
        "                                                 classes=np.unique(y_train),\n",
        "                                                 y=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAIOkkQogqZI"
      },
      "outputs": [],
      "source": [
        "class_weights = {i : class_weights[i] for i in range(7)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8Jc9A3IgwHS",
        "outputId": "17f9577a-9285-4701-a0e5-f8daffa5ea51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 4.375273044997815,\n",
              " 1: 2.78349082823791,\n",
              " 2: 1.301832835044846,\n",
              " 3: 12.440993788819876,\n",
              " 4: 1.2854575792581184,\n",
              " 5: 0.21338020666879728,\n",
              " 6: 10.075452716297788}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZv-B-ygCD57"
      },
      "source": [
        "#SMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDskF1wjGffh"
      },
      "outputs": [],
      "source": [
        "def SMOTE_Data(X, y):\n",
        "  sm = SMOTE(random_state=42, k_neighbors=5)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X.reshape((-1, IMAGE_W * IMAGE_H * 3)), y)\n",
        "  X_resampled.reshape(-1, IMAGE_W, IMAGE_H, 3)\n",
        "  return X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brshqGvOCDJL",
        "outputId": "62c7d6f1-dd65-4a84-ed5f-01d91a88caa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32851, 150528)\n",
            "(32851,)\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = SMOTE_Data(X_train, y_train) #beware of the actual parameter\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GfOtcbV5vVZ",
        "outputId": "124a8626-20f3-4fe4-a463-20245773a69f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({5: 4693, 4: 4693, 2: 4693, 3: 4693, 0: 4693, 1: 4693, 6: 4693})\n"
          ]
        }
      ],
      "source": [
        "counter = Counter(y_train)\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl-nmACZOZpg",
        "outputId": "1124e3a8-67ea-454c-91b3-80ce486b9f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape:  (32851, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.reshape(-1, IMAGE_W, IMAGE_H, 3)\n",
        "print('X_train shape: ',X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBXx28B9yGu"
      },
      "source": [
        "#DeepSMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi8RRUWEBD3s",
        "outputId": "4451daeb-0165-40d7-b76a-c4c90fcbe1fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4986, 32, 32, 3)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-Xqj-WQ90L_"
      },
      "outputs": [],
      "source": [
        "#optional\n",
        "X=X_train\n",
        "y=y_train\n",
        "\n",
        "from numpy import moveaxis\n",
        "dec_x = moveaxis(X, 3, 1)\n",
        "dec_x = dec_x.astype('float32') / 255.\n",
        "#dec_x = X_train \n",
        "dec_y = y\n",
        "\n",
        "#create counter for encoder\n",
        "counter = sorted(counter.items())\n",
        "counter = [value for _, value in counter]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kMMmX7r-fV5",
        "outputId": "d3648cdf-7d3e-4255-d712-dec90b542d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11.3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(torch.version.cuda) #10.1\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 3#1    # number of channels in the input data \n",
        "\n",
        "args['n_z'] = 600 #300     # number of dimensions in latent space. \n",
        "\n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 1000       # how many epochs to run for\n",
        "args['batch_size'] = 12   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = True       # train networks if True, else load networks from\n",
        "\n",
        "args['patience'] = 20\n",
        "\n",
        "## create encoder model and decoder model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        \n",
        "        # convolutional filters, work excellent with image data\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
        "            #nn.ReLU(True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            #nn.ReLU(True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            #nn.ReLU(True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            \n",
        "            #nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 0, bias=False),\n",
        "            \n",
        "            #3d and 32 by 32\n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 1, 0, bias=False),\n",
        "            \n",
        "            nn.BatchNorm2d(self.dim_h * 8), # 40 X 8 = 320\n",
        "            #nn.ReLU(True),\n",
        "            nn.LeakyReLU(0.2, inplace=True) )#,\n",
        "            #nn.Conv2d(self.dim_h * 8, 1, 2, 1, 0, bias=False))\n",
        "            #nn.Conv2d(self.dim_h * 8, 1, 4, 1, 0, bias=False))\n",
        "        # final layer is fully connected\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('enc')\n",
        "        #print('input ',x.size()) #torch.Size([100, 3,32,32])\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        #print('aft squeeze ',x.size()) #torch.Size([128, 320])\n",
        "        #aft squeeze  torch.Size([100, 320])\n",
        "        x = self.fc(x)\n",
        "        #print('out ',x.size()) #torch.Size([128, 20])\n",
        "        #out  torch.Size([100, 300])\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 2**4 * 8 * 8),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 16, self.dim_h * 8, 4),\n",
        "            nn.BatchNorm2d(self.dim_h * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, 3, 4, stride=2, padding=2),\n",
        "            #nn.Sigmoid())\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('dec')\n",
        "        #print('input ',x.size())\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 2**4, 8, 8)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"functions to create SMOTE images\"\"\"\n",
        "\n",
        "def biased_get_class(c):\n",
        "    \n",
        "    xbeg = dec_x[dec_y == c]\n",
        "    ybeg = dec_y[dec_y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "\n",
        "    # determining the number of samples to generate\n",
        "    #n_to_sample = 10 \n",
        "\n",
        "    # fitting the model\n",
        "    n_neigh = 5 + 1\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "#xsamp, ysamp = SM(xclass,yclass)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHtLISURIMTg",
        "outputId": "233e5edf-bf0b-46f0-8bfd-f436597063bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Epoch: 0 \tTrain Loss: 0.460336 \tmse loss: 0.240478 \tmse2 loss: 0.219858\n",
            "Saving..\n",
            "Epoch: 1 \tTrain Loss: 0.135306 \tmse loss: 0.070629 \tmse2 loss: 0.064677\n",
            "Saving..\n",
            "Epoch: 2 \tTrain Loss: 0.109741 \tmse loss: 0.058832 \tmse2 loss: 0.050909\n",
            "Saving..\n",
            "Epoch: 3 \tTrain Loss: 0.095568 \tmse loss: 0.049689 \tmse2 loss: 0.045879\n",
            "Saving..\n",
            "Epoch: 4 \tTrain Loss: 0.085469 \tmse loss: 0.045838 \tmse2 loss: 0.039631\n",
            "Saving..\n",
            "Epoch: 5 \tTrain Loss: 0.078804 \tmse loss: 0.043548 \tmse2 loss: 0.035256\n",
            "Saving..\n",
            "Epoch: 6 \tTrain Loss: 0.075646 \tmse loss: 0.041972 \tmse2 loss: 0.033674\n",
            "Saving..\n",
            "Epoch: 7 \tTrain Loss: 0.073979 \tmse loss: 0.040878 \tmse2 loss: 0.033101\n",
            "Saving..\n",
            "Epoch: 8 \tTrain Loss: 0.065938 \tmse loss: 0.037266 \tmse2 loss: 0.028672\n",
            "Saving..\n",
            "Epoch: 9 \tTrain Loss: 0.061393 \tmse loss: 0.035039 \tmse2 loss: 0.026353\n",
            "Saving..\n",
            "Epoch: 10 \tTrain Loss: 0.060747 \tmse loss: 0.034673 \tmse2 loss: 0.026074\n",
            "Saving..\n",
            "Epoch: 11 \tTrain Loss: 0.057085 \tmse loss: 0.032988 \tmse2 loss: 0.024097\n",
            "Saving..\n",
            "Epoch: 12 \tTrain Loss: 0.054603 \tmse loss: 0.032039 \tmse2 loss: 0.022565\n",
            "Saving..\n",
            "Epoch: 13 \tTrain Loss: 0.050429 \tmse loss: 0.029791 \tmse2 loss: 0.020638\n",
            "Saving..\n",
            "Epoch: 14 \tTrain Loss: 0.046142 \tmse loss: 0.027602 \tmse2 loss: 0.018541\n",
            "Saving..\n",
            "Epoch: 15 \tTrain Loss: 0.046039 \tmse loss: 0.027895 \tmse2 loss: 0.018143\n",
            "Saving..\n",
            "Epoch: 16 \tTrain Loss: 0.043569 \tmse loss: 0.026308 \tmse2 loss: 0.017261\n",
            "Saving..\n",
            "Epoch: 17 \tTrain Loss: 0.043104 \tmse loss: 0.026006 \tmse2 loss: 0.017098\n",
            "Saving..\n",
            "Epoch: 18 \tTrain Loss: 0.040572 \tmse loss: 0.024558 \tmse2 loss: 0.016013\n",
            "Saving..\n",
            "Epoch: 19 \tTrain Loss: 0.038300 \tmse loss: 0.023391 \tmse2 loss: 0.014909\n",
            "Saving..\n",
            "Epoch: 20 \tTrain Loss: 0.036967 \tmse loss: 0.022767 \tmse2 loss: 0.014200\n",
            "Saving..\n",
            "Epoch: 21 \tTrain Loss: 0.036967 \tmse loss: 0.022921 \tmse2 loss: 0.014046\n",
            "Epoch: 22 \tTrain Loss: 0.035027 \tmse loss: 0.021855 \tmse2 loss: 0.013172\n",
            "Saving..\n",
            "Epoch: 23 \tTrain Loss: 0.033657 \tmse loss: 0.021259 \tmse2 loss: 0.012399\n",
            "Saving..\n",
            "Epoch: 24 \tTrain Loss: 0.032991 \tmse loss: 0.020740 \tmse2 loss: 0.012252\n",
            "Saving..\n",
            "Epoch: 25 \tTrain Loss: 0.031015 \tmse loss: 0.019649 \tmse2 loss: 0.011366\n",
            "Saving..\n",
            "Epoch: 26 \tTrain Loss: 0.030275 \tmse loss: 0.019245 \tmse2 loss: 0.011030\n",
            "Saving..\n",
            "Epoch: 27 \tTrain Loss: 0.029004 \tmse loss: 0.018351 \tmse2 loss: 0.010652\n",
            "Saving..\n",
            "Epoch: 28 \tTrain Loss: 0.028609 \tmse loss: 0.018064 \tmse2 loss: 0.010545\n",
            "Saving..\n",
            "Epoch: 29 \tTrain Loss: 0.027706 \tmse loss: 0.017893 \tmse2 loss: 0.009813\n",
            "Saving..\n",
            "Epoch: 30 \tTrain Loss: 0.025469 \tmse loss: 0.016538 \tmse2 loss: 0.008932\n",
            "Saving..\n",
            "Epoch: 31 \tTrain Loss: 0.026071 \tmse loss: 0.016664 \tmse2 loss: 0.009408\n",
            "Epoch: 32 \tTrain Loss: 0.025624 \tmse loss: 0.016482 \tmse2 loss: 0.009142\n",
            "Epoch: 33 \tTrain Loss: 0.024203 \tmse loss: 0.015747 \tmse2 loss: 0.008456\n",
            "Saving..\n",
            "Epoch: 34 \tTrain Loss: 0.024451 \tmse loss: 0.015649 \tmse2 loss: 0.008803\n",
            "Epoch: 35 \tTrain Loss: 0.023914 \tmse loss: 0.015583 \tmse2 loss: 0.008331\n",
            "Saving..\n",
            "Epoch: 36 \tTrain Loss: 0.021918 \tmse loss: 0.014221 \tmse2 loss: 0.007696\n",
            "Saving..\n",
            "Epoch: 37 \tTrain Loss: 0.022003 \tmse loss: 0.014464 \tmse2 loss: 0.007538\n",
            "Epoch: 38 \tTrain Loss: 0.021623 \tmse loss: 0.014253 \tmse2 loss: 0.007370\n",
            "Saving..\n",
            "Epoch: 39 \tTrain Loss: 0.021462 \tmse loss: 0.013977 \tmse2 loss: 0.007485\n",
            "Saving..\n",
            "Epoch: 40 \tTrain Loss: 0.021446 \tmse loss: 0.014052 \tmse2 loss: 0.007394\n",
            "Saving..\n",
            "Epoch: 41 \tTrain Loss: 0.019796 \tmse loss: 0.013020 \tmse2 loss: 0.006776\n",
            "Saving..\n",
            "Epoch: 42 \tTrain Loss: 0.019911 \tmse loss: 0.013204 \tmse2 loss: 0.006707\n",
            "Epoch: 43 \tTrain Loss: 0.020062 \tmse loss: 0.013419 \tmse2 loss: 0.006643\n",
            "Epoch: 44 \tTrain Loss: 0.019150 \tmse loss: 0.012785 \tmse2 loss: 0.006364\n",
            "Saving..\n",
            "Epoch: 45 \tTrain Loss: 0.018729 \tmse loss: 0.012455 \tmse2 loss: 0.006274\n",
            "Saving..\n",
            "Epoch: 46 \tTrain Loss: 0.019136 \tmse loss: 0.012741 \tmse2 loss: 0.006395\n",
            "Epoch: 47 \tTrain Loss: 0.017625 \tmse loss: 0.011815 \tmse2 loss: 0.005809\n",
            "Saving..\n",
            "Epoch: 48 \tTrain Loss: 0.018012 \tmse loss: 0.012037 \tmse2 loss: 0.005975\n",
            "Epoch: 49 \tTrain Loss: 0.017280 \tmse loss: 0.011525 \tmse2 loss: 0.005755\n",
            "Saving..\n",
            "Epoch: 50 \tTrain Loss: 0.017411 \tmse loss: 0.011715 \tmse2 loss: 0.005696\n",
            "Epoch: 51 \tTrain Loss: 0.017844 \tmse loss: 0.011775 \tmse2 loss: 0.006069\n",
            "Epoch: 52 \tTrain Loss: 0.016566 \tmse loss: 0.011062 \tmse2 loss: 0.005504\n",
            "Saving..\n",
            "Epoch: 53 \tTrain Loss: 0.016378 \tmse loss: 0.010894 \tmse2 loss: 0.005484\n",
            "Saving..\n",
            "Epoch: 54 \tTrain Loss: 0.017184 \tmse loss: 0.011550 \tmse2 loss: 0.005633\n",
            "Epoch: 55 \tTrain Loss: 0.015793 \tmse loss: 0.010701 \tmse2 loss: 0.005092\n",
            "Saving..\n",
            "Epoch: 56 \tTrain Loss: 0.015438 \tmse loss: 0.010300 \tmse2 loss: 0.005138\n",
            "Saving..\n",
            "Epoch: 57 \tTrain Loss: 0.016157 \tmse loss: 0.010662 \tmse2 loss: 0.005495\n",
            "Epoch: 58 \tTrain Loss: 0.015244 \tmse loss: 0.010235 \tmse2 loss: 0.005009\n",
            "Saving..\n",
            "Epoch: 59 \tTrain Loss: 0.014930 \tmse loss: 0.010095 \tmse2 loss: 0.004835\n",
            "Saving..\n",
            "Epoch: 60 \tTrain Loss: 0.015409 \tmse loss: 0.010347 \tmse2 loss: 0.005062\n",
            "Epoch: 61 \tTrain Loss: 0.015219 \tmse loss: 0.010300 \tmse2 loss: 0.004919\n",
            "Epoch: 62 \tTrain Loss: 0.014006 \tmse loss: 0.009613 \tmse2 loss: 0.004393\n",
            "Saving..\n",
            "Epoch: 63 \tTrain Loss: 0.014490 \tmse loss: 0.009821 \tmse2 loss: 0.004669\n",
            "Epoch: 64 \tTrain Loss: 0.014394 \tmse loss: 0.009778 \tmse2 loss: 0.004616\n",
            "Epoch: 65 \tTrain Loss: 0.014505 \tmse loss: 0.009847 \tmse2 loss: 0.004658\n",
            "Epoch: 66 \tTrain Loss: 0.014208 \tmse loss: 0.009588 \tmse2 loss: 0.004620\n",
            "Epoch: 67 \tTrain Loss: 0.013376 \tmse loss: 0.009139 \tmse2 loss: 0.004237\n",
            "Saving..\n",
            "Epoch: 68 \tTrain Loss: 0.013505 \tmse loss: 0.009141 \tmse2 loss: 0.004364\n",
            "Epoch: 69 \tTrain Loss: 0.013663 \tmse loss: 0.009328 \tmse2 loss: 0.004335\n",
            "Epoch: 70 \tTrain Loss: 0.013582 \tmse loss: 0.009176 \tmse2 loss: 0.004407\n",
            "Epoch: 71 \tTrain Loss: 0.013027 \tmse loss: 0.008761 \tmse2 loss: 0.004266\n",
            "Saving..\n",
            "Epoch: 72 \tTrain Loss: 0.012496 \tmse loss: 0.008503 \tmse2 loss: 0.003993\n",
            "Saving..\n",
            "Epoch: 73 \tTrain Loss: 0.012871 \tmse loss: 0.008745 \tmse2 loss: 0.004126\n",
            "Epoch: 74 \tTrain Loss: 0.012556 \tmse loss: 0.008532 \tmse2 loss: 0.004024\n",
            "Epoch: 75 \tTrain Loss: 0.013121 \tmse loss: 0.008932 \tmse2 loss: 0.004188\n",
            "Epoch: 76 \tTrain Loss: 0.012304 \tmse loss: 0.008359 \tmse2 loss: 0.003945\n",
            "Saving..\n",
            "Epoch: 77 \tTrain Loss: 0.011884 \tmse loss: 0.008084 \tmse2 loss: 0.003800\n",
            "Saving..\n",
            "Epoch: 78 \tTrain Loss: 0.012064 \tmse loss: 0.008280 \tmse2 loss: 0.003783\n",
            "Epoch: 79 \tTrain Loss: 0.012178 \tmse loss: 0.008239 \tmse2 loss: 0.003939\n",
            "Epoch: 80 \tTrain Loss: 0.012322 \tmse loss: 0.008342 \tmse2 loss: 0.003980\n",
            "Epoch: 81 \tTrain Loss: 0.012100 \tmse loss: 0.008222 \tmse2 loss: 0.003878\n",
            "Epoch: 82 \tTrain Loss: 0.011397 \tmse loss: 0.007763 \tmse2 loss: 0.003634\n",
            "Saving..\n",
            "Epoch: 83 \tTrain Loss: 0.011462 \tmse loss: 0.007844 \tmse2 loss: 0.003618\n",
            "Epoch: 84 \tTrain Loss: 0.012527 \tmse loss: 0.008553 \tmse2 loss: 0.003975\n",
            "Epoch: 85 \tTrain Loss: 0.011341 \tmse loss: 0.007850 \tmse2 loss: 0.003490\n",
            "Saving..\n",
            "Epoch: 86 \tTrain Loss: 0.010976 \tmse loss: 0.007601 \tmse2 loss: 0.003375\n",
            "Saving..\n",
            "Epoch: 87 \tTrain Loss: 0.010960 \tmse loss: 0.007536 \tmse2 loss: 0.003424\n",
            "Saving..\n",
            "Epoch: 88 \tTrain Loss: 0.010934 \tmse loss: 0.007500 \tmse2 loss: 0.003434\n",
            "Saving..\n",
            "Epoch: 89 \tTrain Loss: 0.010952 \tmse loss: 0.007504 \tmse2 loss: 0.003448\n",
            "Epoch: 90 \tTrain Loss: 0.010898 \tmse loss: 0.007466 \tmse2 loss: 0.003432\n",
            "Saving..\n",
            "Epoch: 91 \tTrain Loss: 0.010589 \tmse loss: 0.007292 \tmse2 loss: 0.003296\n",
            "Saving..\n",
            "Epoch: 92 \tTrain Loss: 0.010472 \tmse loss: 0.007210 \tmse2 loss: 0.003261\n",
            "Saving..\n",
            "Epoch: 93 \tTrain Loss: 0.010358 \tmse loss: 0.007162 \tmse2 loss: 0.003197\n",
            "Saving..\n",
            "Epoch: 94 \tTrain Loss: 0.010446 \tmse loss: 0.007190 \tmse2 loss: 0.003256\n",
            "Epoch: 95 \tTrain Loss: 0.010558 \tmse loss: 0.007315 \tmse2 loss: 0.003243\n",
            "Epoch: 96 \tTrain Loss: 0.010062 \tmse loss: 0.006993 \tmse2 loss: 0.003069\n",
            "Saving..\n",
            "Epoch: 97 \tTrain Loss: 0.010361 \tmse loss: 0.007120 \tmse2 loss: 0.003241\n",
            "Epoch: 98 \tTrain Loss: 0.011163 \tmse loss: 0.007587 \tmse2 loss: 0.003575\n",
            "Epoch: 99 \tTrain Loss: 0.010155 \tmse loss: 0.007001 \tmse2 loss: 0.003154\n",
            "/content/drive/MyDrive/PHD/Model/DeepSMOTE/f_enc.pth\n",
            "/content/drive/MyDrive/PHD/Model/DeepSMOTE/f_dec.pth\n"
          ]
        }
      ],
      "source": [
        "#Begin the training\n",
        "batch_size = args['batch_size']\n",
        "patience = args['patience']\n",
        "encoder = Encoder(args)\n",
        "decoder = Decoder(args)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "decoder = decoder.to(device)\n",
        "encoder = encoder.to(device)\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "#decoder loss function\n",
        "criterion = nn.MSELoss()\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "num_workers = 0\n",
        "\n",
        "#torch.Tensor returns float so if want long then use torch.tensor\n",
        "tensor_x = torch.Tensor(dec_x)\n",
        "tensor_y = torch.tensor(dec_y,dtype=torch.long)\n",
        "mnist_bal = TensorDataset(tensor_x,tensor_y) \n",
        "train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
        "    batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "t0 = time.time()\n",
        "if args['train']:\n",
        "    enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
        "    dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
        "\n",
        "    for epoch in range(args['epochs']):\n",
        "        train_loss = 0.0\n",
        "        tmse_loss = 0.0\n",
        "        tdiscr_loss = 0.0\n",
        "        # train for one epoch -- set nets to train mode\n",
        "        encoder.train()\n",
        "        decoder.train()\n",
        "    \n",
        "        for images,labs in train_loader:\n",
        "        \n",
        "            # zero gradients for each batch\n",
        "            encoder.zero_grad()\n",
        "            decoder.zero_grad()\n",
        "            images, labs = images.to(device), labs.to(device)\n",
        "            labsn = labs.detach().cpu().numpy()\n",
        "#            print('images shape', images.shape)\n",
        "            # run images\n",
        "            z_hat = encoder(images)\n",
        "#            print('images shape after encoding', z_hat.shape)\n",
        "        \n",
        "            x_hat = decoder(z_hat) #decoder outputs tanh\n",
        "#            print('images shape after decoding', x_hat.shape)\n",
        "            mse = criterion(x_hat,images)\n",
        "                    \n",
        "            resx = []\n",
        "            resy = []\n",
        "        \n",
        "            tc = np.random.choice(num_classes,1)\n",
        "            #tc = 9\n",
        "            xbeg = dec_x[dec_y == tc]\n",
        "            ybeg = dec_y[dec_y == tc] \n",
        "            xlen = len(xbeg)\n",
        "            nsamp = min(xlen, 100)\n",
        "            ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
        "            xclass = xbeg[ind]\n",
        "            yclass = ybeg[ind]\n",
        "        \n",
        "            xclen = len(xclass)\n",
        "            xcminus = np.arange(1,xclen)\n",
        "            \n",
        "            xcplus = np.append(xcminus,0)\n",
        "            xcnew = (xclass[[xcplus],:])\n",
        "            xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
        "        \n",
        "            xcnew = torch.Tensor(xcnew)\n",
        "            xcnew = xcnew.to(device)\n",
        "        \n",
        "            #encode xclass to feature space\n",
        "            xclass = torch.Tensor(xclass)\n",
        "            xclass = xclass.to(device)\n",
        "            xclass = encoder(xclass)\n",
        "        \n",
        "            xclass = xclass.detach().cpu().numpy()\n",
        "        \n",
        "            xc_enc = (xclass[[xcplus],:])\n",
        "            xc_enc = np.squeeze(xc_enc)\n",
        "        \n",
        "            xc_enc = torch.Tensor(xc_enc)\n",
        "            xc_enc = xc_enc.to(device)\n",
        "            \n",
        "            ximg = decoder(xc_enc)\n",
        "            \n",
        "            mse2 = criterion(ximg,xcnew)\n",
        "        \n",
        "            comb_loss = mse2 + mse\n",
        "            comb_loss.backward()\n",
        "        \n",
        "            enc_optim.step()\n",
        "            dec_optim.step()\n",
        "        \n",
        "            train_loss += comb_loss.item()*images.size(0)\n",
        "            tmse_loss += mse.item()*images.size(0)\n",
        "            tdiscr_loss += mse2.item()*images.size(0)\n",
        "\n",
        "        train_loss = train_loss/len(train_loader)\n",
        "        tmse_loss = tmse_loss/len(train_loader)\n",
        "        tdiscr_loss = tdiscr_loss/len(train_loader)\n",
        "        print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
        "                train_loss,tmse_loss,tdiscr_loss))\n",
        "        \n",
        "    \n",
        "    \n",
        "        #store the best encoder and decoder models\n",
        "        #here, /crs5 is a reference to 5 way cross validation, but is not\n",
        "        #necessary for illustration purposes\n",
        "        if train_loss < best_loss:\n",
        "            print('Saving..')\n",
        "            patience = args['patience']\n",
        "            path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/bst_enc.pth'\n",
        "            path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/bst_dec.pth'\n",
        "          \n",
        "            torch.save(encoder.state_dict(), path_enc)\n",
        "            torch.save(decoder.state_dict(), path_dec)\n",
        "    \n",
        "            best_loss = train_loss\n",
        "        else:\n",
        "            patience -= 1\n",
        "\n",
        "        if patience == 0:\n",
        "            print('Out of patience. \\n')\n",
        "            break\n",
        "    \n",
        "    \n",
        "    #in addition, store the final model (may not be the best) for\n",
        "    #informational purposes\n",
        "    path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/f_enc.pth'\n",
        "    path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/f_dec.pth'\n",
        "    print(path_enc)\n",
        "    print(path_dec)\n",
        "    torch.save(encoder.state_dict(), path_enc)\n",
        "    torch.save(decoder.state_dict(), path_dec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoqPIVKqF5Ug",
        "outputId": "5e2d2d09-4032-4d5e-c744-70f5fe20b6fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train imgs shape  (4986, 3, 32, 32)\n",
            "decy  (4986,)\n",
            "(11732, 3072)\n",
            "(11732,)\n"
          ]
        }
      ],
      "source": [
        "#Generate artificial images\n",
        "import torch\n",
        "np.printoptions(precision=5,suppress=True)\n",
        "\n",
        "#path on the computer where the models are stored\n",
        "modpth = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/'\n",
        "\n",
        "encf = []\n",
        "decf = []\n",
        "for p in range(1):\n",
        "    enc = modpth + '/bst_enc.pth'\n",
        "    dec = modpth + '/bst_dec.pth'\n",
        "    encf.append(enc)\n",
        "    decf.append(dec)\n",
        "\n",
        "for m in range(1):\n",
        "    print('train imgs shape ',dec_x.shape) #(45000,3,32,32)\n",
        "    print('decy ',dec_y.shape)\n",
        "    \n",
        "    #generate some images \n",
        "    train_on_gpu = torch.cuda.is_available()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    path_enc = encf[m]\n",
        "    path_dec = decf[m]\n",
        "\n",
        "    encoder = Encoder(args)\n",
        "    encoder.load_state_dict(torch.load(path_enc), strict=False)\n",
        "    encoder = encoder.to(device)\n",
        "\n",
        "    decoder = Decoder(args)\n",
        "    decoder.load_state_dict(torch.load(path_dec), strict=False)\n",
        "    decoder = decoder.to(device)\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    imbal = counter#[114, 376, 95, 438, 357, 462, 77, 181, 139]\n",
        "\n",
        "    resx = []\n",
        "    resy = []\n",
        "\n",
        "    for i in [0,1,2,3,4,6]: #skip class 5 since it's max class\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        xclass, yclass = biased_get_class(i)\n",
        "#        print(xclass.shape) #(500, 3, 32, 32)\n",
        "#        print(yclass[0]) #(500,)\n",
        "            \n",
        "        #encode xclass to feature space\n",
        "        xclass = torch.Tensor(xclass)\n",
        "        xclass = xclass.to(device)\n",
        "        xclass = encoder(xclass)\n",
        "            \n",
        "        xclass = xclass.detach().cpu().numpy()\n",
        "        n = imbal[5] - imbal[i]\n",
        "        xsamp, ysamp = G_SM(xclass,yclass,n,i)\n",
        "#        print(xsamp.shape) #(4500, 600)\n",
        "#        print(len(ysamp)) #4500\n",
        "        ysamp = np.array(ysamp)\n",
        "    \n",
        "        \"\"\"to generate samples for resnet\"\"\"   \n",
        "        xsamp = torch.Tensor(xsamp)\n",
        "        xsamp = xsamp.to(device)\n",
        "        ximg = decoder(xsamp)\n",
        "\n",
        "        ximn = ximg.detach().cpu().numpy()\n",
        "#        print(ximn.shape) \n",
        "        resx.append(ximn)\n",
        "        resy.append(ysamp)\n",
        "    \n",
        "    resx1 = np.vstack(resx)\n",
        "    resy1 = np.hstack(resy)\n",
        "#    print(resx1.shape) #(34720, 3, 32, 32)\n",
        "\n",
        "    resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "#    print(resx1.shape) #(34720, 3072)\n",
        "    \n",
        "    dec_x1 = dec_x.reshape(dec_x.shape[0],-1)\n",
        "#    print('decx1 ',dec_x1.shape)\n",
        "    combx = np.vstack((resx1,dec_x1))\n",
        "    comby = np.hstack((resy1,dec_y))\n",
        "\n",
        "    print(combx.shape) #(45000, 3, 32, 32)\n",
        "    print(comby.shape) #(45000,)\n",
        "#    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76bpFBQcHLIY",
        "outputId": "778df4ec-f880-41df-c474-8c2f533b4c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape:  (11732, 32, 32, 3)\n",
            "Counter({0: 1676, 1: 1676, 2: 1676, 3: 1676, 4: 1676, 6: 1676, 5: 1676})\n"
          ]
        }
      ],
      "source": [
        "X_train = combx.reshape(-1, 3, IMAGE_W, IMAGE_H)\n",
        "X_train = moveaxis(X_train, 1, 3)\n",
        "print('X_train shape: ',X_train.shape)\n",
        "y_train = comby\n",
        "print(Counter(comby))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SqyvSgX8sLO"
      },
      "outputs": [],
      "source": [
        "X_train = X_train * 255\n",
        "X_train = X_train.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US0KkIaVlTdU"
      },
      "source": [
        "#Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a_lPCqbibaS",
        "outputId": "5e25dd58-ddd6-4d79-d191-48181f4ab387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_train shape:  (32851, 1)\n"
          ]
        }
      ],
      "source": [
        "y_train = y_train.reshape(-1, 1)\n",
        "y_val = y_val.reshape(-1, 1)\n",
        "print('y_train shape: ',y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja5ZmgbCvDw5"
      },
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "y_val = to_categorical(y_val, num_classes = num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioaoIkk4G2pf"
      },
      "outputs": [],
      "source": [
        "#X_val = X_val.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QM00erNGU32",
        "outputId": "d0111aa7-34fc-405f-c65e-4195f4e6b8fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32851, 224, 224, 3)\n",
            "(32851, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jrJ33lUDkCM"
      },
      "source": [
        "#Split dataset to train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6qneWL_Bs2U",
        "outputId": "22c2c4ff-5544-4dd8-b69a-af96b4b97616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data:  (9814, 32, 32, 3)\n",
            "Remaining Data:  (201, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "# stratified train and rem (20%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.02, stratify=y, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Remaining Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vai7M7WSXVY4",
        "outputId": "b9bc1199-a518-4a8b-9d6e-4d6038efbafe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Data:  (416, 32, 32, 3)\n",
            "Val Data:  (416, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "# stratified val and test (50%) \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_rem, y_rem, test_size=0.5, stratify=y_rem, random_state=1)\n",
        "\n",
        "print('Test Data: ', X_test.shape)\n",
        "print('Val Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I15HgVuhjFlm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVVOQPNiHXHw",
        "outputId": "a1e60310-4f3b-4063-8465-4d7120d225c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data:  (2015, 32, 32, 3)\n",
            "Test Data:  (224, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "#optional\n",
        "# stratified train and test (10%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Test Data: ', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_oaUYYgJNV7",
        "outputId": "ca2f393a-0599-4cfc-b330-45d723f2bb06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data:  (3369, 32, 32, 3)\n",
            "Val Data:  (375, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "#optional\n",
        "# stratified train and val (10%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, stratify=y, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Val Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZklPzWxCCtTW"
      },
      "source": [
        "Create and compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jFShRvTHnqi"
      },
      "outputs": [],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kef4r_zxjgk"
      },
      "outputs": [],
      "source": [
        "#Data Augmentation\n",
        "dataaugment = ImageDataGenerator(\n",
        "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "        shear_range = 10) \n",
        "\n",
        "dataaugment.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7sLC2svMuJ"
      },
      "source": [
        "# Classification\n",
        "Model from https://github.com/AnasBrital98/CNN-From-Scratch/tree/master/Inception-V3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8eRZiucdYnP"
      },
      "outputs": [],
      "source": [
        "#USe TF.data\n",
        "import tensorflow as tf\n",
        "training_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "validation_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xalxBh-3LNC"
      },
      "outputs": [],
      "source": [
        "autotune = tf.data.AUTOTUNE\n",
        "train_data_batches = training_data.shuffle(buffer_size=40000).batch(32).prefetch(buffer_size=autotune)\n",
        "valid_data_batches = validation_data.shuffle(buffer_size=10000).batch(32).prefetch(buffer_size=autotune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge4UmF5R0a7V"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "rescale_layer = tf.keras.Sequential([layers.experimental.preprocessing.Rescaling(1./255)])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2), \n",
        "  layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3)),\n",
        "  layers.experimental.preprocessing.RandomTranslation(0.3, 0.3, fill_mode='reflect', interpolation='bilinear',)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu6rr5apdJtf"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "#Callbacks\n",
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/best_model.h5'\n",
        "mc = ModelCheckpoint(best_model_fpath, monitor='val_prc', mode='max', verbose=1, save_best_only=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=10, verbose=0, factor=0.5, min_lr=0.00001)\n",
        "early_stopping_monitor = EarlyStopping(patience=30,monitor='val_prc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwwLiXUSG0IZ",
        "outputId": "ff36fadf-0797-4610-d5a2-694d4921398a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.6458 - accuracy: 0.3716 - prc: 0.4136\n",
            "Epoch 1: val_prc improved from -inf to 0.80877, saving model to /content/drive/MyDrive/PHD/Model/best_model.h5\n",
            "1027/1027 [==============================] - 123s 116ms/step - loss: 1.6458 - accuracy: 0.3716 - prc: 0.4136 - val_loss: 0.8532 - val_accuracy: 0.6477 - val_prc: 0.8088 - lr: 5.0000e-04\n",
            "Epoch 2/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.3457 - accuracy: 0.4609 - prc: 0.5326\n",
            "Epoch 2: val_prc did not improve from 0.80877\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 1.3457 - accuracy: 0.4609 - prc: 0.5326 - val_loss: 0.9636 - val_accuracy: 0.6788 - val_prc: 0.7617 - lr: 5.0000e-04\n",
            "Epoch 3/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.2500 - accuracy: 0.5045 - prc: 0.5774\n",
            "Epoch 3: val_prc did not improve from 0.80877\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 1.2500 - accuracy: 0.5045 - prc: 0.5774 - val_loss: 0.9215 - val_accuracy: 0.6632 - val_prc: 0.7902 - lr: 5.0000e-04\n",
            "Epoch 4/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.2136 - accuracy: 0.5224 - prc: 0.5961\n",
            "Epoch 4: val_prc improved from 0.80877 to 0.82365, saving model to /content/drive/MyDrive/PHD/Model/best_model.h5\n",
            "1027/1027 [==============================] - 116s 113ms/step - loss: 1.2136 - accuracy: 0.5224 - prc: 0.5961 - val_loss: 0.8219 - val_accuracy: 0.6995 - val_prc: 0.8236 - lr: 5.0000e-04\n",
            "Epoch 5/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.1846 - accuracy: 0.5343 - prc: 0.6107\n",
            "Epoch 5: val_prc did not improve from 0.82365\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 1.1846 - accuracy: 0.5343 - prc: 0.6107 - val_loss: 0.8284 - val_accuracy: 0.6995 - val_prc: 0.8200 - lr: 5.0000e-04\n",
            "Epoch 6/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.1681 - accuracy: 0.5406 - prc: 0.6187\n",
            "Epoch 6: val_prc did not improve from 0.82365\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 1.1681 - accuracy: 0.5406 - prc: 0.6187 - val_loss: 0.9487 - val_accuracy: 0.7150 - val_prc: 0.8040 - lr: 5.0000e-04\n",
            "Epoch 7/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.1466 - accuracy: 0.5516 - prc: 0.6323\n",
            "Epoch 7: val_prc improved from 0.82365 to 0.83080, saving model to /content/drive/MyDrive/PHD/Model/best_model.h5\n",
            "1027/1027 [==============================] - 116s 113ms/step - loss: 1.1466 - accuracy: 0.5516 - prc: 0.6323 - val_loss: 0.8409 - val_accuracy: 0.7461 - val_prc: 0.8308 - lr: 5.0000e-04\n",
            "Epoch 8/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.1395 - accuracy: 0.5542 - prc: 0.6345\n",
            "Epoch 8: val_prc did not improve from 0.83080\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 1.1395 - accuracy: 0.5542 - prc: 0.6345 - val_loss: 1.0059 - val_accuracy: 0.6425 - val_prc: 0.7743 - lr: 5.0000e-04\n",
            "Epoch 9/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.1258 - accuracy: 0.5648 - prc: 0.6424\n",
            "Epoch 9: val_prc did not improve from 0.83080\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 1.1258 - accuracy: 0.5648 - prc: 0.6424 - val_loss: 0.8562 - val_accuracy: 0.7202 - val_prc: 0.8239 - lr: 5.0000e-04\n",
            "Epoch 10/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.1102 - accuracy: 0.5697 - prc: 0.6488\n",
            "Epoch 10: val_prc improved from 0.83080 to 0.83790, saving model to /content/drive/MyDrive/PHD/Model/best_model.h5\n",
            "1027/1027 [==============================] - 116s 113ms/step - loss: 1.1102 - accuracy: 0.5697 - prc: 0.6488 - val_loss: 0.7598 - val_accuracy: 0.7306 - val_prc: 0.8379 - lr: 5.0000e-04\n",
            "Epoch 11/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0975 - accuracy: 0.5720 - prc: 0.6554\n",
            "Epoch 11: val_prc did not improve from 0.83790\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 1.0975 - accuracy: 0.5720 - prc: 0.6554 - val_loss: 0.8367 - val_accuracy: 0.7409 - val_prc: 0.8359 - lr: 5.0000e-04\n",
            "Epoch 12/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0935 - accuracy: 0.5728 - prc: 0.6593\n",
            "Epoch 12: val_prc did not improve from 0.83790\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 1.0935 - accuracy: 0.5728 - prc: 0.6593 - val_loss: 0.8029 - val_accuracy: 0.7098 - val_prc: 0.8334 - lr: 5.0000e-04\n",
            "Epoch 13/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0760 - accuracy: 0.5821 - prc: 0.6675\n",
            "Epoch 13: val_prc improved from 0.83790 to 0.84525, saving model to /content/drive/MyDrive/PHD/Model/best_model.h5\n",
            "1027/1027 [==============================] - 116s 113ms/step - loss: 1.0760 - accuracy: 0.5821 - prc: 0.6675 - val_loss: 0.7716 - val_accuracy: 0.7461 - val_prc: 0.8453 - lr: 5.0000e-04\n",
            "Epoch 14/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0646 - accuracy: 0.5878 - prc: 0.6729\n",
            "Epoch 14: val_prc did not improve from 0.84525\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 1.0646 - accuracy: 0.5878 - prc: 0.6729 - val_loss: 0.8918 - val_accuracy: 0.6632 - val_prc: 0.8080 - lr: 5.0000e-04\n",
            "Epoch 15/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0488 - accuracy: 0.5958 - prc: 0.6813\n",
            "Epoch 15: val_prc did not improve from 0.84525\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 1.0488 - accuracy: 0.5958 - prc: 0.6813 - val_loss: 0.8131 - val_accuracy: 0.6891 - val_prc: 0.8342 - lr: 5.0000e-04\n",
            "Epoch 16/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0374 - accuracy: 0.6022 - prc: 0.6853\n",
            "Epoch 16: val_prc did not improve from 0.84525\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 1.0374 - accuracy: 0.6022 - prc: 0.6853 - val_loss: 0.8540 - val_accuracy: 0.7150 - val_prc: 0.8310 - lr: 5.0000e-04\n",
            "Epoch 17/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0276 - accuracy: 0.6056 - prc: 0.6897\n",
            "Epoch 17: val_prc did not improve from 0.84525\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 1.0276 - accuracy: 0.6056 - prc: 0.6897 - val_loss: 0.8360 - val_accuracy: 0.7098 - val_prc: 0.8271 - lr: 5.0000e-04\n",
            "Epoch 18/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0306 - accuracy: 0.6061 - prc: 0.6900\n",
            "Epoch 18: val_prc improved from 0.84525 to 0.85219, saving model to /content/drive/MyDrive/PHD/Model/best_model.h5\n",
            "1027/1027 [==============================] - 116s 113ms/step - loss: 1.0306 - accuracy: 0.6061 - prc: 0.6900 - val_loss: 0.7494 - val_accuracy: 0.7358 - val_prc: 0.8522 - lr: 5.0000e-04\n",
            "Epoch 19/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0152 - accuracy: 0.6061 - prc: 0.6958\n",
            "Epoch 19: val_prc did not improve from 0.85219\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 1.0152 - accuracy: 0.6061 - prc: 0.6958 - val_loss: 0.8764 - val_accuracy: 0.7047 - val_prc: 0.8180 - lr: 5.0000e-04\n",
            "Epoch 20/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0127 - accuracy: 0.6154 - prc: 0.6978\n",
            "Epoch 20: val_prc did not improve from 0.85219\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 1.0127 - accuracy: 0.6154 - prc: 0.6978 - val_loss: 0.7873 - val_accuracy: 0.7306 - val_prc: 0.8354 - lr: 5.0000e-04\n",
            "Epoch 21/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0092 - accuracy: 0.6127 - prc: 0.6981\n",
            "Epoch 21: val_prc did not improve from 0.85219\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 1.0092 - accuracy: 0.6127 - prc: 0.6981 - val_loss: 0.7420 - val_accuracy: 0.7098 - val_prc: 0.8440 - lr: 5.0000e-04\n",
            "Epoch 22/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 1.0013 - accuracy: 0.6152 - prc: 0.7025\n",
            "Epoch 22: val_prc did not improve from 0.85219\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 1.0013 - accuracy: 0.6152 - prc: 0.7025 - val_loss: 0.7841 - val_accuracy: 0.7202 - val_prc: 0.8465 - lr: 5.0000e-04\n",
            "Epoch 23/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9993 - accuracy: 0.6162 - prc: 0.7039\n",
            "Epoch 23: val_prc did not improve from 0.85219\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9993 - accuracy: 0.6162 - prc: 0.7039 - val_loss: 0.8997 - val_accuracy: 0.6995 - val_prc: 0.8201 - lr: 5.0000e-04\n",
            "Epoch 24/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9959 - accuracy: 0.6191 - prc: 0.7053\n",
            "Epoch 24: val_prc did not improve from 0.85219\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9959 - accuracy: 0.6191 - prc: 0.7053 - val_loss: 0.8230 - val_accuracy: 0.7254 - val_prc: 0.8447 - lr: 5.0000e-04\n",
            "Epoch 25/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9961 - accuracy: 0.6171 - prc: 0.7056\n",
            "Epoch 25: val_prc did not improve from 0.85219\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9961 - accuracy: 0.6171 - prc: 0.7056 - val_loss: 0.8028 - val_accuracy: 0.7358 - val_prc: 0.8511 - lr: 5.0000e-04\n",
            "Epoch 26/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9900 - accuracy: 0.6235 - prc: 0.7091\n",
            "Epoch 26: val_prc did not improve from 0.85219\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9900 - accuracy: 0.6235 - prc: 0.7091 - val_loss: 0.8373 - val_accuracy: 0.7306 - val_prc: 0.8341 - lr: 5.0000e-04\n",
            "Epoch 27/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9804 - accuracy: 0.6246 - prc: 0.7132\n",
            "Epoch 27: val_prc improved from 0.85219 to 0.85432, saving model to /content/drive/MyDrive/PHD/Model/best_model.h5\n",
            "1027/1027 [==============================] - 117s 114ms/step - loss: 0.9804 - accuracy: 0.6246 - prc: 0.7132 - val_loss: 0.7759 - val_accuracy: 0.7202 - val_prc: 0.8543 - lr: 5.0000e-04\n",
            "Epoch 28/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9770 - accuracy: 0.6257 - prc: 0.7137\n",
            "Epoch 28: val_prc did not improve from 0.85432\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9770 - accuracy: 0.6257 - prc: 0.7137 - val_loss: 0.7847 - val_accuracy: 0.7409 - val_prc: 0.8519 - lr: 5.0000e-04\n",
            "Epoch 29/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9771 - accuracy: 0.6266 - prc: 0.7135\n",
            "Epoch 29: val_prc did not improve from 0.85432\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9771 - accuracy: 0.6266 - prc: 0.7135 - val_loss: 0.7889 - val_accuracy: 0.7358 - val_prc: 0.8488 - lr: 5.0000e-04\n",
            "Epoch 30/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9774 - accuracy: 0.6274 - prc: 0.7139\n",
            "Epoch 30: val_prc did not improve from 0.85432\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9774 - accuracy: 0.6274 - prc: 0.7139 - val_loss: 0.7802 - val_accuracy: 0.7254 - val_prc: 0.8507 - lr: 5.0000e-04\n",
            "Epoch 31/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9741 - accuracy: 0.6277 - prc: 0.7171\n",
            "Epoch 31: val_prc did not improve from 0.85432\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9741 - accuracy: 0.6277 - prc: 0.7171 - val_loss: 0.7803 - val_accuracy: 0.7409 - val_prc: 0.8494 - lr: 5.0000e-04\n",
            "Epoch 32/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9803 - accuracy: 0.6231 - prc: 0.7116\n",
            "Epoch 32: val_prc improved from 0.85432 to 0.85907, saving model to /content/drive/MyDrive/PHD/Model/best_model.h5\n",
            "1027/1027 [==============================] - 116s 113ms/step - loss: 0.9803 - accuracy: 0.6231 - prc: 0.7116 - val_loss: 0.7335 - val_accuracy: 0.7358 - val_prc: 0.8591 - lr: 5.0000e-04\n",
            "Epoch 33/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9686 - accuracy: 0.6298 - prc: 0.7180\n",
            "Epoch 33: val_prc did not improve from 0.85907\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9686 - accuracy: 0.6298 - prc: 0.7180 - val_loss: 0.7538 - val_accuracy: 0.7254 - val_prc: 0.8562 - lr: 5.0000e-04\n",
            "Epoch 34/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9684 - accuracy: 0.6293 - prc: 0.7181\n",
            "Epoch 34: val_prc improved from 0.85907 to 0.86216, saving model to /content/drive/MyDrive/PHD/Model/best_model.h5\n",
            "1027/1027 [==============================] - 117s 114ms/step - loss: 0.9684 - accuracy: 0.6293 - prc: 0.7181 - val_loss: 0.7297 - val_accuracy: 0.7668 - val_prc: 0.8622 - lr: 5.0000e-04\n",
            "Epoch 35/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9666 - accuracy: 0.6312 - prc: 0.7186\n",
            "Epoch 35: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9666 - accuracy: 0.6312 - prc: 0.7186 - val_loss: 0.8611 - val_accuracy: 0.7150 - val_prc: 0.8240 - lr: 5.0000e-04\n",
            "Epoch 36/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9662 - accuracy: 0.6307 - prc: 0.7188\n",
            "Epoch 36: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9662 - accuracy: 0.6307 - prc: 0.7188 - val_loss: 0.8389 - val_accuracy: 0.7358 - val_prc: 0.8403 - lr: 5.0000e-04\n",
            "Epoch 37/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9564 - accuracy: 0.6319 - prc: 0.7231\n",
            "Epoch 37: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9564 - accuracy: 0.6319 - prc: 0.7231 - val_loss: 0.8840 - val_accuracy: 0.6995 - val_prc: 0.8301 - lr: 5.0000e-04\n",
            "Epoch 38/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9561 - accuracy: 0.6328 - prc: 0.7232\n",
            "Epoch 38: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9561 - accuracy: 0.6328 - prc: 0.7232 - val_loss: 0.9099 - val_accuracy: 0.7150 - val_prc: 0.8111 - lr: 5.0000e-04\n",
            "Epoch 39/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9594 - accuracy: 0.6336 - prc: 0.7220\n",
            "Epoch 39: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9594 - accuracy: 0.6336 - prc: 0.7220 - val_loss: 0.8139 - val_accuracy: 0.7098 - val_prc: 0.8359 - lr: 5.0000e-04\n",
            "Epoch 40/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9528 - accuracy: 0.6355 - prc: 0.7247\n",
            "Epoch 40: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9528 - accuracy: 0.6355 - prc: 0.7247 - val_loss: 0.7011 - val_accuracy: 0.7461 - val_prc: 0.8596 - lr: 5.0000e-04\n",
            "Epoch 41/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9537 - accuracy: 0.6333 - prc: 0.7240\n",
            "Epoch 41: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9537 - accuracy: 0.6333 - prc: 0.7240 - val_loss: 0.7209 - val_accuracy: 0.7306 - val_prc: 0.8530 - lr: 5.0000e-04\n",
            "Epoch 42/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9572 - accuracy: 0.6350 - prc: 0.7241\n",
            "Epoch 42: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9572 - accuracy: 0.6350 - prc: 0.7241 - val_loss: 0.9088 - val_accuracy: 0.7047 - val_prc: 0.8043 - lr: 5.0000e-04\n",
            "Epoch 43/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9516 - accuracy: 0.6350 - prc: 0.7260\n",
            "Epoch 43: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9516 - accuracy: 0.6350 - prc: 0.7260 - val_loss: 0.7761 - val_accuracy: 0.7306 - val_prc: 0.8517 - lr: 5.0000e-04\n",
            "Epoch 44/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9532 - accuracy: 0.6355 - prc: 0.7256\n",
            "Epoch 44: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9532 - accuracy: 0.6355 - prc: 0.7256 - val_loss: 0.8449 - val_accuracy: 0.7202 - val_prc: 0.8289 - lr: 5.0000e-04\n",
            "Epoch 45/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9515 - accuracy: 0.6336 - prc: 0.7252\n",
            "Epoch 45: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9515 - accuracy: 0.6336 - prc: 0.7252 - val_loss: 0.9482 - val_accuracy: 0.7047 - val_prc: 0.8061 - lr: 5.0000e-04\n",
            "Epoch 46/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9409 - accuracy: 0.6409 - prc: 0.7304\n",
            "Epoch 46: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9409 - accuracy: 0.6409 - prc: 0.7304 - val_loss: 0.9402 - val_accuracy: 0.7150 - val_prc: 0.8054 - lr: 5.0000e-04\n",
            "Epoch 47/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9414 - accuracy: 0.6366 - prc: 0.7301\n",
            "Epoch 47: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9414 - accuracy: 0.6366 - prc: 0.7301 - val_loss: 0.7978 - val_accuracy: 0.7202 - val_prc: 0.8459 - lr: 5.0000e-04\n",
            "Epoch 48/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9515 - accuracy: 0.6353 - prc: 0.7251\n",
            "Epoch 48: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9515 - accuracy: 0.6353 - prc: 0.7251 - val_loss: 0.8231 - val_accuracy: 0.7358 - val_prc: 0.8418 - lr: 5.0000e-04\n",
            "Epoch 49/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9437 - accuracy: 0.6410 - prc: 0.7301\n",
            "Epoch 49: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9437 - accuracy: 0.6410 - prc: 0.7301 - val_loss: 0.8160 - val_accuracy: 0.7150 - val_prc: 0.8468 - lr: 5.0000e-04\n",
            "Epoch 50/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9416 - accuracy: 0.6434 - prc: 0.7314\n",
            "Epoch 50: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9416 - accuracy: 0.6434 - prc: 0.7314 - val_loss: 0.8251 - val_accuracy: 0.7254 - val_prc: 0.8379 - lr: 5.0000e-04\n",
            "Epoch 51/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9414 - accuracy: 0.6411 - prc: 0.7304\n",
            "Epoch 51: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9414 - accuracy: 0.6411 - prc: 0.7304 - val_loss: 0.8020 - val_accuracy: 0.7409 - val_prc: 0.8507 - lr: 5.0000e-04\n",
            "Epoch 52/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9427 - accuracy: 0.6412 - prc: 0.7309\n",
            "Epoch 52: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9427 - accuracy: 0.6412 - prc: 0.7309 - val_loss: 0.7473 - val_accuracy: 0.7513 - val_prc: 0.8566 - lr: 5.0000e-04\n",
            "Epoch 53/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9384 - accuracy: 0.6401 - prc: 0.7321\n",
            "Epoch 53: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9384 - accuracy: 0.6401 - prc: 0.7321 - val_loss: 0.7252 - val_accuracy: 0.7565 - val_prc: 0.8560 - lr: 5.0000e-04\n",
            "Epoch 54/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9355 - accuracy: 0.6414 - prc: 0.7340\n",
            "Epoch 54: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9355 - accuracy: 0.6414 - prc: 0.7340 - val_loss: 0.8640 - val_accuracy: 0.7202 - val_prc: 0.8304 - lr: 5.0000e-04\n",
            "Epoch 55/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9355 - accuracy: 0.6423 - prc: 0.7343\n",
            "Epoch 55: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9355 - accuracy: 0.6423 - prc: 0.7343 - val_loss: 0.7763 - val_accuracy: 0.7306 - val_prc: 0.8541 - lr: 5.0000e-04\n",
            "Epoch 56/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9380 - accuracy: 0.6417 - prc: 0.7322\n",
            "Epoch 56: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9380 - accuracy: 0.6417 - prc: 0.7322 - val_loss: 0.9083 - val_accuracy: 0.6995 - val_prc: 0.8231 - lr: 5.0000e-04\n",
            "Epoch 57/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9373 - accuracy: 0.6427 - prc: 0.7330\n",
            "Epoch 57: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9373 - accuracy: 0.6427 - prc: 0.7330 - val_loss: 0.7916 - val_accuracy: 0.7150 - val_prc: 0.8413 - lr: 5.0000e-04\n",
            "Epoch 58/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9309 - accuracy: 0.6422 - prc: 0.7349\n",
            "Epoch 58: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9309 - accuracy: 0.6422 - prc: 0.7349 - val_loss: 0.7533 - val_accuracy: 0.7461 - val_prc: 0.8536 - lr: 5.0000e-04\n",
            "Epoch 59/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9321 - accuracy: 0.6444 - prc: 0.7343\n",
            "Epoch 59: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9321 - accuracy: 0.6444 - prc: 0.7343 - val_loss: 0.7311 - val_accuracy: 0.7409 - val_prc: 0.8595 - lr: 5.0000e-04\n",
            "Epoch 60/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9274 - accuracy: 0.6466 - prc: 0.7370\n",
            "Epoch 60: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9274 - accuracy: 0.6466 - prc: 0.7370 - val_loss: 0.7505 - val_accuracy: 0.7513 - val_prc: 0.8570 - lr: 5.0000e-04\n",
            "Epoch 61/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9252 - accuracy: 0.6484 - prc: 0.7385\n",
            "Epoch 61: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9252 - accuracy: 0.6484 - prc: 0.7385 - val_loss: 0.8224 - val_accuracy: 0.7202 - val_prc: 0.8354 - lr: 5.0000e-04\n",
            "Epoch 62/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9359 - accuracy: 0.6421 - prc: 0.7325\n",
            "Epoch 62: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9359 - accuracy: 0.6421 - prc: 0.7325 - val_loss: 0.7806 - val_accuracy: 0.7668 - val_prc: 0.8582 - lr: 5.0000e-04\n",
            "Epoch 63/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9306 - accuracy: 0.6421 - prc: 0.7350\n",
            "Epoch 63: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9306 - accuracy: 0.6421 - prc: 0.7350 - val_loss: 0.7849 - val_accuracy: 0.7358 - val_prc: 0.8395 - lr: 5.0000e-04\n",
            "Epoch 64/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9318 - accuracy: 0.6449 - prc: 0.7362\n",
            "Epoch 64: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9318 - accuracy: 0.6449 - prc: 0.7362 - val_loss: 0.9199 - val_accuracy: 0.6995 - val_prc: 0.8068 - lr: 5.0000e-04\n",
            "Epoch 65/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9377 - accuracy: 0.6430 - prc: 0.7331\n",
            "Epoch 65: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9377 - accuracy: 0.6430 - prc: 0.7331 - val_loss: 0.7736 - val_accuracy: 0.7409 - val_prc: 0.8434 - lr: 5.0000e-04\n",
            "Epoch 66/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9265 - accuracy: 0.6467 - prc: 0.7378\n",
            "Epoch 66: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9265 - accuracy: 0.6467 - prc: 0.7378 - val_loss: 0.8574 - val_accuracy: 0.7047 - val_prc: 0.8245 - lr: 5.0000e-04\n",
            "Epoch 67/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9220 - accuracy: 0.6498 - prc: 0.7392\n",
            "Epoch 67: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9220 - accuracy: 0.6498 - prc: 0.7392 - val_loss: 0.8588 - val_accuracy: 0.7409 - val_prc: 0.8372 - lr: 5.0000e-04\n",
            "Epoch 68/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9311 - accuracy: 0.6451 - prc: 0.7355\n",
            "Epoch 68: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9311 - accuracy: 0.6451 - prc: 0.7355 - val_loss: 0.7764 - val_accuracy: 0.7306 - val_prc: 0.8411 - lr: 5.0000e-04\n",
            "Epoch 69/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.6469 - prc: 0.7385\n",
            "Epoch 69: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9254 - accuracy: 0.6469 - prc: 0.7385 - val_loss: 0.8315 - val_accuracy: 0.7306 - val_prc: 0.8376 - lr: 5.0000e-04\n",
            "Epoch 70/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9211 - accuracy: 0.6498 - prc: 0.7403\n",
            "Epoch 70: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9211 - accuracy: 0.6498 - prc: 0.7403 - val_loss: 0.8512 - val_accuracy: 0.7047 - val_prc: 0.8360 - lr: 5.0000e-04\n",
            "Epoch 71/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9289 - accuracy: 0.6448 - prc: 0.7365\n",
            "Epoch 71: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9289 - accuracy: 0.6448 - prc: 0.7365 - val_loss: 0.8425 - val_accuracy: 0.7202 - val_prc: 0.8409 - lr: 5.0000e-04\n",
            "Epoch 72/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9246 - accuracy: 0.6479 - prc: 0.7385\n",
            "Epoch 72: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9246 - accuracy: 0.6479 - prc: 0.7385 - val_loss: 0.8132 - val_accuracy: 0.7202 - val_prc: 0.8381 - lr: 5.0000e-04\n",
            "Epoch 73/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9231 - accuracy: 0.6488 - prc: 0.7386\n",
            "Epoch 73: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9231 - accuracy: 0.6488 - prc: 0.7386 - val_loss: 0.7585 - val_accuracy: 0.7461 - val_prc: 0.8529 - lr: 5.0000e-04\n",
            "Epoch 74/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9221 - accuracy: 0.6465 - prc: 0.7399\n",
            "Epoch 74: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9221 - accuracy: 0.6465 - prc: 0.7399 - val_loss: 0.7883 - val_accuracy: 0.7047 - val_prc: 0.8418 - lr: 5.0000e-04\n",
            "Epoch 75/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9241 - accuracy: 0.6460 - prc: 0.7377\n",
            "Epoch 75: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9241 - accuracy: 0.6460 - prc: 0.7377 - val_loss: 0.8581 - val_accuracy: 0.7098 - val_prc: 0.8281 - lr: 5.0000e-04\n",
            "Epoch 76/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9264 - accuracy: 0.6446 - prc: 0.7387\n",
            "Epoch 76: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9264 - accuracy: 0.6446 - prc: 0.7387 - val_loss: 0.7588 - val_accuracy: 0.7461 - val_prc: 0.8500 - lr: 5.0000e-04\n",
            "Epoch 77/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9236 - accuracy: 0.6478 - prc: 0.7391\n",
            "Epoch 77: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9236 - accuracy: 0.6478 - prc: 0.7391 - val_loss: 0.7917 - val_accuracy: 0.7306 - val_prc: 0.8481 - lr: 5.0000e-04\n",
            "Epoch 78/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9195 - accuracy: 0.6480 - prc: 0.7418\n",
            "Epoch 78: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9195 - accuracy: 0.6480 - prc: 0.7418 - val_loss: 0.8338 - val_accuracy: 0.7047 - val_prc: 0.8326 - lr: 5.0000e-04\n",
            "Epoch 79/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9203 - accuracy: 0.6502 - prc: 0.7407\n",
            "Epoch 79: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9203 - accuracy: 0.6502 - prc: 0.7407 - val_loss: 0.7661 - val_accuracy: 0.7513 - val_prc: 0.8485 - lr: 5.0000e-04\n",
            "Epoch 80/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9169 - accuracy: 0.6488 - prc: 0.7425\n",
            "Epoch 80: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9169 - accuracy: 0.6488 - prc: 0.7425 - val_loss: 0.7795 - val_accuracy: 0.7617 - val_prc: 0.8500 - lr: 5.0000e-04\n",
            "Epoch 81/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9170 - accuracy: 0.6516 - prc: 0.7417\n",
            "Epoch 81: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9170 - accuracy: 0.6516 - prc: 0.7417 - val_loss: 0.8739 - val_accuracy: 0.7150 - val_prc: 0.8186 - lr: 5.0000e-04\n",
            "Epoch 82/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9193 - accuracy: 0.6497 - prc: 0.7410\n",
            "Epoch 82: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9193 - accuracy: 0.6497 - prc: 0.7410 - val_loss: 0.8533 - val_accuracy: 0.7202 - val_prc: 0.8341 - lr: 5.0000e-04\n",
            "Epoch 83/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9152 - accuracy: 0.6496 - prc: 0.7421\n",
            "Epoch 83: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9152 - accuracy: 0.6496 - prc: 0.7421 - val_loss: 0.8276 - val_accuracy: 0.7202 - val_prc: 0.8303 - lr: 5.0000e-04\n",
            "Epoch 84/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9188 - accuracy: 0.6481 - prc: 0.7419\n",
            "Epoch 84: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9188 - accuracy: 0.6481 - prc: 0.7419 - val_loss: 0.7634 - val_accuracy: 0.7150 - val_prc: 0.8459 - lr: 5.0000e-04\n",
            "Epoch 85/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9181 - accuracy: 0.6486 - prc: 0.7424\n",
            "Epoch 85: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9181 - accuracy: 0.6486 - prc: 0.7424 - val_loss: 0.9075 - val_accuracy: 0.7098 - val_prc: 0.8183 - lr: 5.0000e-04\n",
            "Epoch 86/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9134 - accuracy: 0.6524 - prc: 0.7436\n",
            "Epoch 86: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9134 - accuracy: 0.6524 - prc: 0.7436 - val_loss: 0.8426 - val_accuracy: 0.7047 - val_prc: 0.8253 - lr: 5.0000e-04\n",
            "Epoch 87/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9132 - accuracy: 0.6527 - prc: 0.7428\n",
            "Epoch 87: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9132 - accuracy: 0.6527 - prc: 0.7428 - val_loss: 0.8245 - val_accuracy: 0.7254 - val_prc: 0.8383 - lr: 5.0000e-04\n",
            "Epoch 88/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9125 - accuracy: 0.6526 - prc: 0.7439\n",
            "Epoch 88: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9125 - accuracy: 0.6526 - prc: 0.7439 - val_loss: 0.8183 - val_accuracy: 0.7409 - val_prc: 0.8401 - lr: 5.0000e-04\n",
            "Epoch 89/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9206 - accuracy: 0.6510 - prc: 0.7404\n",
            "Epoch 89: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9206 - accuracy: 0.6510 - prc: 0.7404 - val_loss: 0.8837 - val_accuracy: 0.7409 - val_prc: 0.8258 - lr: 5.0000e-04\n",
            "Epoch 90/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.6543 - prc: 0.7448\n",
            "Epoch 90: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9121 - accuracy: 0.6543 - prc: 0.7448 - val_loss: 0.9883 - val_accuracy: 0.7047 - val_prc: 0.8052 - lr: 5.0000e-04\n",
            "Epoch 91/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9129 - accuracy: 0.6515 - prc: 0.7439\n",
            "Epoch 91: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9129 - accuracy: 0.6515 - prc: 0.7439 - val_loss: 0.9944 - val_accuracy: 0.6995 - val_prc: 0.8123 - lr: 5.0000e-04\n",
            "Epoch 92/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9145 - accuracy: 0.6509 - prc: 0.7439\n",
            "Epoch 92: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9145 - accuracy: 0.6509 - prc: 0.7439 - val_loss: 0.8511 - val_accuracy: 0.7254 - val_prc: 0.8358 - lr: 5.0000e-04\n",
            "Epoch 93/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9203 - accuracy: 0.6481 - prc: 0.7412\n",
            "Epoch 93: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9203 - accuracy: 0.6481 - prc: 0.7412 - val_loss: 0.7797 - val_accuracy: 0.7409 - val_prc: 0.8597 - lr: 5.0000e-04\n",
            "Epoch 94/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9126 - accuracy: 0.6527 - prc: 0.7442\n",
            "Epoch 94: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9126 - accuracy: 0.6527 - prc: 0.7442 - val_loss: 0.7603 - val_accuracy: 0.7461 - val_prc: 0.8551 - lr: 5.0000e-04\n",
            "Epoch 95/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9057 - accuracy: 0.6553 - prc: 0.7472\n",
            "Epoch 95: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9057 - accuracy: 0.6553 - prc: 0.7472 - val_loss: 0.7938 - val_accuracy: 0.7461 - val_prc: 0.8486 - lr: 5.0000e-04\n",
            "Epoch 96/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9049 - accuracy: 0.6550 - prc: 0.7480\n",
            "Epoch 96: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9049 - accuracy: 0.6550 - prc: 0.7480 - val_loss: 0.8387 - val_accuracy: 0.7098 - val_prc: 0.8380 - lr: 5.0000e-04\n",
            "Epoch 97/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9129 - accuracy: 0.6512 - prc: 0.7440\n",
            "Epoch 97: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9129 - accuracy: 0.6512 - prc: 0.7440 - val_loss: 0.7914 - val_accuracy: 0.7150 - val_prc: 0.8452 - lr: 5.0000e-04\n",
            "Epoch 98/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9168 - accuracy: 0.6521 - prc: 0.7435\n",
            "Epoch 98: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9168 - accuracy: 0.6521 - prc: 0.7435 - val_loss: 0.8134 - val_accuracy: 0.6995 - val_prc: 0.8354 - lr: 5.0000e-04\n",
            "Epoch 99/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9151 - accuracy: 0.6497 - prc: 0.7443\n",
            "Epoch 99: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9151 - accuracy: 0.6497 - prc: 0.7443 - val_loss: 0.7763 - val_accuracy: 0.7098 - val_prc: 0.8499 - lr: 5.0000e-04\n",
            "Epoch 100/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9135 - accuracy: 0.6478 - prc: 0.7422\n",
            "Epoch 100: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9135 - accuracy: 0.6478 - prc: 0.7422 - val_loss: 0.7963 - val_accuracy: 0.7047 - val_prc: 0.8445 - lr: 5.0000e-04\n",
            "Epoch 101/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9095 - accuracy: 0.6536 - prc: 0.7446\n",
            "Epoch 101: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9095 - accuracy: 0.6536 - prc: 0.7446 - val_loss: 0.8328 - val_accuracy: 0.7306 - val_prc: 0.8387 - lr: 5.0000e-04\n",
            "Epoch 102/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9135 - accuracy: 0.6521 - prc: 0.7439\n",
            "Epoch 102: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9135 - accuracy: 0.6521 - prc: 0.7439 - val_loss: 0.8443 - val_accuracy: 0.7098 - val_prc: 0.8375 - lr: 5.0000e-04\n",
            "Epoch 103/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9123 - accuracy: 0.6501 - prc: 0.7435\n",
            "Epoch 103: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9123 - accuracy: 0.6501 - prc: 0.7435 - val_loss: 0.8602 - val_accuracy: 0.6995 - val_prc: 0.8375 - lr: 5.0000e-04\n",
            "Epoch 104/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9097 - accuracy: 0.6506 - prc: 0.7449\n",
            "Epoch 104: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.9097 - accuracy: 0.6506 - prc: 0.7449 - val_loss: 0.8168 - val_accuracy: 0.7358 - val_prc: 0.8479 - lr: 5.0000e-04\n",
            "Epoch 105/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9138 - accuracy: 0.6519 - prc: 0.7435\n",
            "Epoch 105: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.9138 - accuracy: 0.6519 - prc: 0.7435 - val_loss: 0.8308 - val_accuracy: 0.7150 - val_prc: 0.8355 - lr: 5.0000e-04\n",
            "Epoch 106/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.9108 - accuracy: 0.6521 - prc: 0.7436\n",
            "Epoch 106: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.9108 - accuracy: 0.6521 - prc: 0.7436 - val_loss: 0.7264 - val_accuracy: 0.7358 - val_prc: 0.8560 - lr: 5.0000e-04\n",
            "Epoch 107/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8919 - accuracy: 0.6580 - prc: 0.7535\n",
            "Epoch 107: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.8919 - accuracy: 0.6580 - prc: 0.7535 - val_loss: 0.7548 - val_accuracy: 0.7409 - val_prc: 0.8513 - lr: 2.5000e-04\n",
            "Epoch 108/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8829 - accuracy: 0.6631 - prc: 0.7575\n",
            "Epoch 108: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8829 - accuracy: 0.6631 - prc: 0.7575 - val_loss: 0.7613 - val_accuracy: 0.7098 - val_prc: 0.8509 - lr: 2.5000e-04\n",
            "Epoch 109/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8813 - accuracy: 0.6611 - prc: 0.7573\n",
            "Epoch 109: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8813 - accuracy: 0.6611 - prc: 0.7573 - val_loss: 0.8118 - val_accuracy: 0.7047 - val_prc: 0.8374 - lr: 2.5000e-04\n",
            "Epoch 110/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8766 - accuracy: 0.6655 - prc: 0.7603\n",
            "Epoch 110: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8766 - accuracy: 0.6655 - prc: 0.7603 - val_loss: 0.8306 - val_accuracy: 0.6839 - val_prc: 0.8341 - lr: 2.5000e-04\n",
            "Epoch 111/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8814 - accuracy: 0.6628 - prc: 0.7577\n",
            "Epoch 111: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8814 - accuracy: 0.6628 - prc: 0.7577 - val_loss: 0.7992 - val_accuracy: 0.7306 - val_prc: 0.8445 - lr: 2.5000e-04\n",
            "Epoch 112/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8809 - accuracy: 0.6642 - prc: 0.7568\n",
            "Epoch 112: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8809 - accuracy: 0.6642 - prc: 0.7568 - val_loss: 0.7495 - val_accuracy: 0.7098 - val_prc: 0.8527 - lr: 2.5000e-04\n",
            "Epoch 113/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8783 - accuracy: 0.6645 - prc: 0.7589\n",
            "Epoch 113: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.8783 - accuracy: 0.6645 - prc: 0.7589 - val_loss: 0.8122 - val_accuracy: 0.7358 - val_prc: 0.8398 - lr: 2.5000e-04\n",
            "Epoch 114/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8742 - accuracy: 0.6657 - prc: 0.7606\n",
            "Epoch 114: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8742 - accuracy: 0.6657 - prc: 0.7606 - val_loss: 0.8912 - val_accuracy: 0.7202 - val_prc: 0.8255 - lr: 2.5000e-04\n",
            "Epoch 115/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8804 - accuracy: 0.6642 - prc: 0.7582\n",
            "Epoch 115: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8804 - accuracy: 0.6642 - prc: 0.7582 - val_loss: 0.8326 - val_accuracy: 0.7358 - val_prc: 0.8387 - lr: 2.5000e-04\n",
            "Epoch 116/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8712 - accuracy: 0.6646 - prc: 0.7611\n",
            "Epoch 116: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8712 - accuracy: 0.6646 - prc: 0.7611 - val_loss: 0.7905 - val_accuracy: 0.7254 - val_prc: 0.8448 - lr: 2.5000e-04\n",
            "Epoch 117/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8736 - accuracy: 0.6659 - prc: 0.7612\n",
            "Epoch 117: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8736 - accuracy: 0.6659 - prc: 0.7612 - val_loss: 0.8107 - val_accuracy: 0.7358 - val_prc: 0.8450 - lr: 2.5000e-04\n",
            "Epoch 118/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8754 - accuracy: 0.6629 - prc: 0.7595\n",
            "Epoch 118: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8754 - accuracy: 0.6629 - prc: 0.7595 - val_loss: 0.8969 - val_accuracy: 0.6891 - val_prc: 0.8223 - lr: 2.5000e-04\n",
            "Epoch 119/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8735 - accuracy: 0.6651 - prc: 0.7603\n",
            "Epoch 119: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8735 - accuracy: 0.6651 - prc: 0.7603 - val_loss: 0.9232 - val_accuracy: 0.7047 - val_prc: 0.8242 - lr: 2.5000e-04\n",
            "Epoch 120/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8772 - accuracy: 0.6617 - prc: 0.7583\n",
            "Epoch 120: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8772 - accuracy: 0.6617 - prc: 0.7583 - val_loss: 0.7466 - val_accuracy: 0.7513 - val_prc: 0.8596 - lr: 2.5000e-04\n",
            "Epoch 121/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8695 - accuracy: 0.6652 - prc: 0.7622\n",
            "Epoch 121: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8695 - accuracy: 0.6652 - prc: 0.7622 - val_loss: 0.9019 - val_accuracy: 0.7254 - val_prc: 0.8244 - lr: 2.5000e-04\n",
            "Epoch 122/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8763 - accuracy: 0.6647 - prc: 0.7593\n",
            "Epoch 122: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.8763 - accuracy: 0.6647 - prc: 0.7593 - val_loss: 0.8634 - val_accuracy: 0.6995 - val_prc: 0.8345 - lr: 2.5000e-04\n",
            "Epoch 123/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8691 - accuracy: 0.6698 - prc: 0.7629\n",
            "Epoch 123: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8691 - accuracy: 0.6698 - prc: 0.7629 - val_loss: 0.7962 - val_accuracy: 0.7513 - val_prc: 0.8465 - lr: 2.5000e-04\n",
            "Epoch 124/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8702 - accuracy: 0.6676 - prc: 0.7618\n",
            "Epoch 124: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.8702 - accuracy: 0.6676 - prc: 0.7618 - val_loss: 0.8475 - val_accuracy: 0.7306 - val_prc: 0.8384 - lr: 2.5000e-04\n",
            "Epoch 125/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8644 - accuracy: 0.6686 - prc: 0.7646\n",
            "Epoch 125: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 114s 111ms/step - loss: 0.8644 - accuracy: 0.6686 - prc: 0.7646 - val_loss: 0.7507 - val_accuracy: 0.7409 - val_prc: 0.8549 - lr: 2.5000e-04\n",
            "Epoch 126/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8653 - accuracy: 0.6666 - prc: 0.7640\n",
            "Epoch 126: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.8653 - accuracy: 0.6666 - prc: 0.7640 - val_loss: 0.8275 - val_accuracy: 0.7150 - val_prc: 0.8395 - lr: 2.5000e-04\n",
            "Epoch 127/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8703 - accuracy: 0.6666 - prc: 0.7624\n",
            "Epoch 127: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8703 - accuracy: 0.6666 - prc: 0.7624 - val_loss: 0.8555 - val_accuracy: 0.7254 - val_prc: 0.8323 - lr: 2.5000e-04\n",
            "Epoch 128/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8703 - accuracy: 0.6648 - prc: 0.7621\n",
            "Epoch 128: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8703 - accuracy: 0.6648 - prc: 0.7621 - val_loss: 0.8466 - val_accuracy: 0.7202 - val_prc: 0.8280 - lr: 2.5000e-04\n",
            "Epoch 129/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8631 - accuracy: 0.6719 - prc: 0.7656\n",
            "Epoch 129: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 111ms/step - loss: 0.8631 - accuracy: 0.6719 - prc: 0.7656 - val_loss: 0.8705 - val_accuracy: 0.7306 - val_prc: 0.8303 - lr: 2.5000e-04\n",
            "Epoch 130/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8673 - accuracy: 0.6666 - prc: 0.7634\n",
            "Epoch 130: val_prc did not improve from 0.86216\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8673 - accuracy: 0.6666 - prc: 0.7634 - val_loss: 0.8823 - val_accuracy: 0.7254 - val_prc: 0.8274 - lr: 2.5000e-04\n",
            "Epoch 131/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8685 - accuracy: 0.6707 - prc: 0.7634\n",
            "Epoch 131: val_prc improved from 0.86216 to 0.86456, saving model to /content/drive/MyDrive/PHD/Model/best_model.h5\n",
            "1027/1027 [==============================] - 117s 114ms/step - loss: 0.8685 - accuracy: 0.6707 - prc: 0.7634 - val_loss: 0.7415 - val_accuracy: 0.7565 - val_prc: 0.8646 - lr: 2.5000e-04\n",
            "Epoch 132/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8627 - accuracy: 0.6701 - prc: 0.7659\n",
            "Epoch 132: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8627 - accuracy: 0.6701 - prc: 0.7659 - val_loss: 0.8516 - val_accuracy: 0.7202 - val_prc: 0.8373 - lr: 2.5000e-04\n",
            "Epoch 133/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8711 - accuracy: 0.6677 - prc: 0.7615\n",
            "Epoch 133: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8711 - accuracy: 0.6677 - prc: 0.7615 - val_loss: 0.7963 - val_accuracy: 0.7409 - val_prc: 0.8474 - lr: 2.5000e-04\n",
            "Epoch 134/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8690 - accuracy: 0.6663 - prc: 0.7626\n",
            "Epoch 134: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8690 - accuracy: 0.6663 - prc: 0.7626 - val_loss: 0.9452 - val_accuracy: 0.7047 - val_prc: 0.8124 - lr: 2.5000e-04\n",
            "Epoch 135/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8650 - accuracy: 0.6674 - prc: 0.7635\n",
            "Epoch 135: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8650 - accuracy: 0.6674 - prc: 0.7635 - val_loss: 0.8086 - val_accuracy: 0.7202 - val_prc: 0.8494 - lr: 2.5000e-04\n",
            "Epoch 136/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8676 - accuracy: 0.6696 - prc: 0.7638\n",
            "Epoch 136: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8676 - accuracy: 0.6696 - prc: 0.7638 - val_loss: 0.7976 - val_accuracy: 0.7306 - val_prc: 0.8497 - lr: 2.5000e-04\n",
            "Epoch 137/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8643 - accuracy: 0.6690 - prc: 0.7648\n",
            "Epoch 137: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8643 - accuracy: 0.6690 - prc: 0.7648 - val_loss: 0.8900 - val_accuracy: 0.6891 - val_prc: 0.8263 - lr: 2.5000e-04\n",
            "Epoch 138/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8712 - accuracy: 0.6659 - prc: 0.7620\n",
            "Epoch 138: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8712 - accuracy: 0.6659 - prc: 0.7620 - val_loss: 0.7980 - val_accuracy: 0.7306 - val_prc: 0.8477 - lr: 2.5000e-04\n",
            "Epoch 139/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8662 - accuracy: 0.6712 - prc: 0.7639\n",
            "Epoch 139: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8662 - accuracy: 0.6712 - prc: 0.7639 - val_loss: 0.7857 - val_accuracy: 0.7358 - val_prc: 0.8472 - lr: 2.5000e-04\n",
            "Epoch 140/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8618 - accuracy: 0.6702 - prc: 0.7657\n",
            "Epoch 140: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8618 - accuracy: 0.6702 - prc: 0.7657 - val_loss: 0.8211 - val_accuracy: 0.7254 - val_prc: 0.8397 - lr: 2.5000e-04\n",
            "Epoch 141/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8684 - accuracy: 0.6689 - prc: 0.7632\n",
            "Epoch 141: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8684 - accuracy: 0.6689 - prc: 0.7632 - val_loss: 0.8056 - val_accuracy: 0.7254 - val_prc: 0.8447 - lr: 2.5000e-04\n",
            "Epoch 142/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8694 - accuracy: 0.6664 - prc: 0.7630\n",
            "Epoch 142: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8694 - accuracy: 0.6664 - prc: 0.7630 - val_loss: 0.7548 - val_accuracy: 0.7513 - val_prc: 0.8554 - lr: 2.5000e-04\n",
            "Epoch 143/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8663 - accuracy: 0.6661 - prc: 0.7633\n",
            "Epoch 143: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8663 - accuracy: 0.6661 - prc: 0.7633 - val_loss: 0.7927 - val_accuracy: 0.7461 - val_prc: 0.8531 - lr: 2.5000e-04\n",
            "Epoch 144/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8627 - accuracy: 0.6696 - prc: 0.7652\n",
            "Epoch 144: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8627 - accuracy: 0.6696 - prc: 0.7652 - val_loss: 0.8137 - val_accuracy: 0.7254 - val_prc: 0.8446 - lr: 2.5000e-04\n",
            "Epoch 145/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8646 - accuracy: 0.6675 - prc: 0.7647\n",
            "Epoch 145: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8646 - accuracy: 0.6675 - prc: 0.7647 - val_loss: 0.8221 - val_accuracy: 0.7202 - val_prc: 0.8396 - lr: 2.5000e-04\n",
            "Epoch 146/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8634 - accuracy: 0.6675 - prc: 0.7651\n",
            "Epoch 146: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8634 - accuracy: 0.6675 - prc: 0.7651 - val_loss: 0.8859 - val_accuracy: 0.7098 - val_prc: 0.8298 - lr: 2.5000e-04\n",
            "Epoch 147/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8621 - accuracy: 0.6711 - prc: 0.7654\n",
            "Epoch 147: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8621 - accuracy: 0.6711 - prc: 0.7654 - val_loss: 0.9227 - val_accuracy: 0.7047 - val_prc: 0.8190 - lr: 2.5000e-04\n",
            "Epoch 148/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8672 - accuracy: 0.6665 - prc: 0.7635\n",
            "Epoch 148: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8672 - accuracy: 0.6665 - prc: 0.7635 - val_loss: 0.8870 - val_accuracy: 0.7150 - val_prc: 0.8297 - lr: 2.5000e-04\n",
            "Epoch 149/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8594 - accuracy: 0.6720 - prc: 0.7674\n",
            "Epoch 149: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8594 - accuracy: 0.6720 - prc: 0.7674 - val_loss: 0.8833 - val_accuracy: 0.7358 - val_prc: 0.8370 - lr: 2.5000e-04\n",
            "Epoch 150/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8609 - accuracy: 0.6708 - prc: 0.7659\n",
            "Epoch 150: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8609 - accuracy: 0.6708 - prc: 0.7659 - val_loss: 0.9505 - val_accuracy: 0.7254 - val_prc: 0.8179 - lr: 2.5000e-04\n",
            "Epoch 151/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8675 - accuracy: 0.6692 - prc: 0.7635\n",
            "Epoch 151: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8675 - accuracy: 0.6692 - prc: 0.7635 - val_loss: 0.9012 - val_accuracy: 0.7098 - val_prc: 0.8269 - lr: 2.5000e-04\n",
            "Epoch 152/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8673 - accuracy: 0.6688 - prc: 0.7628\n",
            "Epoch 152: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8673 - accuracy: 0.6688 - prc: 0.7628 - val_loss: 0.9591 - val_accuracy: 0.6943 - val_prc: 0.8202 - lr: 2.5000e-04\n",
            "Epoch 153/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8652 - accuracy: 0.6689 - prc: 0.7644\n",
            "Epoch 153: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8652 - accuracy: 0.6689 - prc: 0.7644 - val_loss: 0.9593 - val_accuracy: 0.6943 - val_prc: 0.8189 - lr: 2.5000e-04\n",
            "Epoch 154/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8675 - accuracy: 0.6680 - prc: 0.7637\n",
            "Epoch 154: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8675 - accuracy: 0.6680 - prc: 0.7637 - val_loss: 0.9378 - val_accuracy: 0.7098 - val_prc: 0.8256 - lr: 2.5000e-04\n",
            "Epoch 155/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8623 - accuracy: 0.6725 - prc: 0.7671\n",
            "Epoch 155: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8623 - accuracy: 0.6725 - prc: 0.7671 - val_loss: 0.9156 - val_accuracy: 0.7150 - val_prc: 0.8255 - lr: 2.5000e-04\n",
            "Epoch 156/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8637 - accuracy: 0.6679 - prc: 0.7661\n",
            "Epoch 156: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8637 - accuracy: 0.6679 - prc: 0.7661 - val_loss: 0.8531 - val_accuracy: 0.7358 - val_prc: 0.8369 - lr: 2.5000e-04\n",
            "Epoch 157/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8628 - accuracy: 0.6671 - prc: 0.7648\n",
            "Epoch 157: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8628 - accuracy: 0.6671 - prc: 0.7648 - val_loss: 0.8389 - val_accuracy: 0.7306 - val_prc: 0.8445 - lr: 2.5000e-04\n",
            "Epoch 158/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8607 - accuracy: 0.6737 - prc: 0.7664\n",
            "Epoch 158: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8607 - accuracy: 0.6737 - prc: 0.7664 - val_loss: 0.9536 - val_accuracy: 0.6891 - val_prc: 0.8094 - lr: 2.5000e-04\n",
            "Epoch 159/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8597 - accuracy: 0.6712 - prc: 0.7653\n",
            "Epoch 159: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8597 - accuracy: 0.6712 - prc: 0.7653 - val_loss: 0.7822 - val_accuracy: 0.7358 - val_prc: 0.8481 - lr: 2.5000e-04\n",
            "Epoch 160/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8519 - accuracy: 0.6737 - prc: 0.7693\n",
            "Epoch 160: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8519 - accuracy: 0.6737 - prc: 0.7693 - val_loss: 0.8937 - val_accuracy: 0.6943 - val_prc: 0.8203 - lr: 1.2500e-04\n",
            "Epoch 161/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8460 - accuracy: 0.6759 - prc: 0.7719\n",
            "Epoch 161: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8460 - accuracy: 0.6759 - prc: 0.7719 - val_loss: 0.8328 - val_accuracy: 0.7409 - val_prc: 0.8423 - lr: 1.2500e-04\n",
            "Epoch 162/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8476 - accuracy: 0.6791 - prc: 0.7723\n",
            "Epoch 162: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8476 - accuracy: 0.6791 - prc: 0.7723 - val_loss: 0.8779 - val_accuracy: 0.7202 - val_prc: 0.8322 - lr: 1.2500e-04\n",
            "Epoch 163/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8433 - accuracy: 0.6752 - prc: 0.7739\n",
            "Epoch 163: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8433 - accuracy: 0.6752 - prc: 0.7739 - val_loss: 0.8062 - val_accuracy: 0.7409 - val_prc: 0.8467 - lr: 1.2500e-04\n",
            "Epoch 164/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8432 - accuracy: 0.6757 - prc: 0.7736\n",
            "Epoch 164: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8432 - accuracy: 0.6757 - prc: 0.7736 - val_loss: 0.9158 - val_accuracy: 0.7202 - val_prc: 0.8241 - lr: 1.2500e-04\n",
            "Epoch 165/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8533 - accuracy: 0.6713 - prc: 0.7694\n",
            "Epoch 165: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8533 - accuracy: 0.6713 - prc: 0.7694 - val_loss: 0.8158 - val_accuracy: 0.7150 - val_prc: 0.8425 - lr: 1.2500e-04\n",
            "Epoch 166/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8469 - accuracy: 0.6784 - prc: 0.7721\n",
            "Epoch 166: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8469 - accuracy: 0.6784 - prc: 0.7721 - val_loss: 0.8324 - val_accuracy: 0.7202 - val_prc: 0.8387 - lr: 1.2500e-04\n",
            "Epoch 167/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8496 - accuracy: 0.6754 - prc: 0.7707\n",
            "Epoch 167: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8496 - accuracy: 0.6754 - prc: 0.7707 - val_loss: 0.8328 - val_accuracy: 0.7358 - val_prc: 0.8410 - lr: 1.2500e-04\n",
            "Epoch 168/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8387 - accuracy: 0.6784 - prc: 0.7754\n",
            "Epoch 168: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8387 - accuracy: 0.6784 - prc: 0.7754 - val_loss: 0.8849 - val_accuracy: 0.7098 - val_prc: 0.8278 - lr: 1.2500e-04\n",
            "Epoch 169/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8440 - accuracy: 0.6771 - prc: 0.7739\n",
            "Epoch 169: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8440 - accuracy: 0.6771 - prc: 0.7739 - val_loss: 0.8648 - val_accuracy: 0.7306 - val_prc: 0.8333 - lr: 1.2500e-04\n",
            "Epoch 170/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8445 - accuracy: 0.6776 - prc: 0.7729\n",
            "Epoch 170: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8445 - accuracy: 0.6776 - prc: 0.7729 - val_loss: 0.8096 - val_accuracy: 0.7306 - val_prc: 0.8443 - lr: 1.2500e-04\n",
            "Epoch 171/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8491 - accuracy: 0.6730 - prc: 0.7714\n",
            "Epoch 171: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8491 - accuracy: 0.6730 - prc: 0.7714 - val_loss: 0.8132 - val_accuracy: 0.7358 - val_prc: 0.8428 - lr: 1.2500e-04\n",
            "Epoch 172/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8473 - accuracy: 0.6771 - prc: 0.7727\n",
            "Epoch 172: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8473 - accuracy: 0.6771 - prc: 0.7727 - val_loss: 0.8197 - val_accuracy: 0.7358 - val_prc: 0.8381 - lr: 1.2500e-04\n",
            "Epoch 173/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8514 - accuracy: 0.6730 - prc: 0.7712\n",
            "Epoch 173: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8514 - accuracy: 0.6730 - prc: 0.7712 - val_loss: 0.8061 - val_accuracy: 0.7358 - val_prc: 0.8433 - lr: 1.2500e-04\n",
            "Epoch 174/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8432 - accuracy: 0.6770 - prc: 0.7735\n",
            "Epoch 174: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8432 - accuracy: 0.6770 - prc: 0.7735 - val_loss: 0.9052 - val_accuracy: 0.7150 - val_prc: 0.8258 - lr: 1.2500e-04\n",
            "Epoch 175/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8434 - accuracy: 0.6796 - prc: 0.7731\n",
            "Epoch 175: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8434 - accuracy: 0.6796 - prc: 0.7731 - val_loss: 0.8054 - val_accuracy: 0.7202 - val_prc: 0.8469 - lr: 1.2500e-04\n",
            "Epoch 176/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8364 - accuracy: 0.6818 - prc: 0.7771\n",
            "Epoch 176: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8364 - accuracy: 0.6818 - prc: 0.7771 - val_loss: 0.8424 - val_accuracy: 0.7254 - val_prc: 0.8389 - lr: 1.2500e-04\n",
            "Epoch 177/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8460 - accuracy: 0.6773 - prc: 0.7728\n",
            "Epoch 177: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8460 - accuracy: 0.6773 - prc: 0.7728 - val_loss: 0.8351 - val_accuracy: 0.7150 - val_prc: 0.8383 - lr: 1.2500e-04\n",
            "Epoch 178/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8451 - accuracy: 0.6775 - prc: 0.7740\n",
            "Epoch 178: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8451 - accuracy: 0.6775 - prc: 0.7740 - val_loss: 0.8501 - val_accuracy: 0.7358 - val_prc: 0.8363 - lr: 1.2500e-04\n",
            "Epoch 179/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8430 - accuracy: 0.6757 - prc: 0.7733\n",
            "Epoch 179: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8430 - accuracy: 0.6757 - prc: 0.7733 - val_loss: 0.8650 - val_accuracy: 0.7098 - val_prc: 0.8297 - lr: 1.2500e-04\n",
            "Epoch 180/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8431 - accuracy: 0.6778 - prc: 0.7739\n",
            "Epoch 180: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8431 - accuracy: 0.6778 - prc: 0.7739 - val_loss: 0.8099 - val_accuracy: 0.7358 - val_prc: 0.8453 - lr: 1.2500e-04\n",
            "Epoch 181/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8415 - accuracy: 0.6772 - prc: 0.7738\n",
            "Epoch 181: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8415 - accuracy: 0.6772 - prc: 0.7738 - val_loss: 0.8361 - val_accuracy: 0.7254 - val_prc: 0.8400 - lr: 1.2500e-04\n",
            "Epoch 182/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8406 - accuracy: 0.6780 - prc: 0.7758\n",
            "Epoch 182: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8406 - accuracy: 0.6780 - prc: 0.7758 - val_loss: 0.8458 - val_accuracy: 0.7047 - val_prc: 0.8356 - lr: 1.2500e-04\n",
            "Epoch 183/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8409 - accuracy: 0.6785 - prc: 0.7744\n",
            "Epoch 183: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8409 - accuracy: 0.6785 - prc: 0.7744 - val_loss: 0.8546 - val_accuracy: 0.7202 - val_prc: 0.8370 - lr: 1.2500e-04\n",
            "Epoch 184/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8432 - accuracy: 0.6768 - prc: 0.7737\n",
            "Epoch 184: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8432 - accuracy: 0.6768 - prc: 0.7737 - val_loss: 0.8275 - val_accuracy: 0.7202 - val_prc: 0.8411 - lr: 1.2500e-04\n",
            "Epoch 185/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8421 - accuracy: 0.6763 - prc: 0.7743\n",
            "Epoch 185: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8421 - accuracy: 0.6763 - prc: 0.7743 - val_loss: 0.8213 - val_accuracy: 0.7409 - val_prc: 0.8426 - lr: 1.2500e-04\n",
            "Epoch 186/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8412 - accuracy: 0.6764 - prc: 0.7751\n",
            "Epoch 186: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8412 - accuracy: 0.6764 - prc: 0.7751 - val_loss: 0.8162 - val_accuracy: 0.7306 - val_prc: 0.8421 - lr: 1.2500e-04\n",
            "Epoch 187/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8359 - accuracy: 0.6786 - prc: 0.7768\n",
            "Epoch 187: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8359 - accuracy: 0.6786 - prc: 0.7768 - val_loss: 0.8202 - val_accuracy: 0.7254 - val_prc: 0.8405 - lr: 6.2500e-05\n",
            "Epoch 188/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8301 - accuracy: 0.6844 - prc: 0.7807\n",
            "Epoch 188: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8301 - accuracy: 0.6844 - prc: 0.7807 - val_loss: 0.8509 - val_accuracy: 0.7150 - val_prc: 0.8351 - lr: 6.2500e-05\n",
            "Epoch 189/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8430 - accuracy: 0.6758 - prc: 0.7728\n",
            "Epoch 189: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8430 - accuracy: 0.6758 - prc: 0.7728 - val_loss: 0.8522 - val_accuracy: 0.7306 - val_prc: 0.8350 - lr: 6.2500e-05\n",
            "Epoch 190/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8340 - accuracy: 0.6833 - prc: 0.7780\n",
            "Epoch 190: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8340 - accuracy: 0.6833 - prc: 0.7780 - val_loss: 0.8179 - val_accuracy: 0.7202 - val_prc: 0.8410 - lr: 6.2500e-05\n",
            "Epoch 191/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8359 - accuracy: 0.6805 - prc: 0.7771\n",
            "Epoch 191: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8359 - accuracy: 0.6805 - prc: 0.7771 - val_loss: 0.8413 - val_accuracy: 0.7098 - val_prc: 0.8355 - lr: 6.2500e-05\n",
            "Epoch 192/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8378 - accuracy: 0.6810 - prc: 0.7759\n",
            "Epoch 192: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8378 - accuracy: 0.6810 - prc: 0.7759 - val_loss: 0.8326 - val_accuracy: 0.7254 - val_prc: 0.8393 - lr: 6.2500e-05\n",
            "Epoch 193/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8368 - accuracy: 0.6806 - prc: 0.7771\n",
            "Epoch 193: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8368 - accuracy: 0.6806 - prc: 0.7771 - val_loss: 0.7803 - val_accuracy: 0.7202 - val_prc: 0.8503 - lr: 6.2500e-05\n",
            "Epoch 194/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8315 - accuracy: 0.6835 - prc: 0.7797\n",
            "Epoch 194: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8315 - accuracy: 0.6835 - prc: 0.7797 - val_loss: 0.8510 - val_accuracy: 0.6891 - val_prc: 0.8332 - lr: 6.2500e-05\n",
            "Epoch 195/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8334 - accuracy: 0.6832 - prc: 0.7780\n",
            "Epoch 195: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8334 - accuracy: 0.6832 - prc: 0.7780 - val_loss: 0.8573 - val_accuracy: 0.7254 - val_prc: 0.8323 - lr: 6.2500e-05\n",
            "Epoch 196/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8321 - accuracy: 0.6818 - prc: 0.7790\n",
            "Epoch 196: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8321 - accuracy: 0.6818 - prc: 0.7790 - val_loss: 0.8411 - val_accuracy: 0.7358 - val_prc: 0.8371 - lr: 6.2500e-05\n",
            "Epoch 197/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8368 - accuracy: 0.6815 - prc: 0.7770\n",
            "Epoch 197: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8368 - accuracy: 0.6815 - prc: 0.7770 - val_loss: 0.8392 - val_accuracy: 0.7098 - val_prc: 0.8383 - lr: 6.2500e-05\n",
            "Epoch 198/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8357 - accuracy: 0.6784 - prc: 0.7762\n",
            "Epoch 198: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8357 - accuracy: 0.6784 - prc: 0.7762 - val_loss: 0.8094 - val_accuracy: 0.7306 - val_prc: 0.8439 - lr: 6.2500e-05\n",
            "Epoch 199/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8343 - accuracy: 0.6824 - prc: 0.7781\n",
            "Epoch 199: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8343 - accuracy: 0.6824 - prc: 0.7781 - val_loss: 0.8428 - val_accuracy: 0.7409 - val_prc: 0.8368 - lr: 3.1250e-05\n",
            "Epoch 200/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8354 - accuracy: 0.6822 - prc: 0.7772\n",
            "Epoch 200: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8354 - accuracy: 0.6822 - prc: 0.7772 - val_loss: 0.8210 - val_accuracy: 0.7150 - val_prc: 0.8411 - lr: 3.1250e-05\n",
            "Epoch 201/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8329 - accuracy: 0.6821 - prc: 0.7793\n",
            "Epoch 201: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8329 - accuracy: 0.6821 - prc: 0.7793 - val_loss: 0.8523 - val_accuracy: 0.6995 - val_prc: 0.8358 - lr: 3.1250e-05\n",
            "Epoch 202/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8284 - accuracy: 0.6844 - prc: 0.7816\n",
            "Epoch 202: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8284 - accuracy: 0.6844 - prc: 0.7816 - val_loss: 0.8591 - val_accuracy: 0.7150 - val_prc: 0.8334 - lr: 3.1250e-05\n",
            "Epoch 203/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8303 - accuracy: 0.6821 - prc: 0.7791\n",
            "Epoch 203: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8303 - accuracy: 0.6821 - prc: 0.7791 - val_loss: 0.8238 - val_accuracy: 0.7306 - val_prc: 0.8426 - lr: 3.1250e-05\n",
            "Epoch 204/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8302 - accuracy: 0.6816 - prc: 0.7796\n",
            "Epoch 204: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8302 - accuracy: 0.6816 - prc: 0.7796 - val_loss: 0.8210 - val_accuracy: 0.7306 - val_prc: 0.8450 - lr: 3.1250e-05\n",
            "Epoch 205/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8273 - accuracy: 0.6832 - prc: 0.7818\n",
            "Epoch 205: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8273 - accuracy: 0.6832 - prc: 0.7818 - val_loss: 0.8890 - val_accuracy: 0.7202 - val_prc: 0.8288 - lr: 3.1250e-05\n",
            "Epoch 206/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8292 - accuracy: 0.6830 - prc: 0.7802\n",
            "Epoch 206: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8292 - accuracy: 0.6830 - prc: 0.7802 - val_loss: 0.8320 - val_accuracy: 0.7254 - val_prc: 0.8420 - lr: 3.1250e-05\n",
            "Epoch 207/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8297 - accuracy: 0.6836 - prc: 0.7802\n",
            "Epoch 207: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8297 - accuracy: 0.6836 - prc: 0.7802 - val_loss: 0.8257 - val_accuracy: 0.7047 - val_prc: 0.8398 - lr: 3.1250e-05\n",
            "Epoch 208/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8291 - accuracy: 0.6824 - prc: 0.7807\n",
            "Epoch 208: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8291 - accuracy: 0.6824 - prc: 0.7807 - val_loss: 0.8346 - val_accuracy: 0.7202 - val_prc: 0.8405 - lr: 3.1250e-05\n",
            "Epoch 209/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8320 - accuracy: 0.6831 - prc: 0.7797\n",
            "Epoch 209: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8320 - accuracy: 0.6831 - prc: 0.7797 - val_loss: 0.8138 - val_accuracy: 0.7202 - val_prc: 0.8457 - lr: 3.1250e-05\n",
            "Epoch 210/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8317 - accuracy: 0.6844 - prc: 0.7799\n",
            "Epoch 210: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8317 - accuracy: 0.6844 - prc: 0.7799 - val_loss: 0.8204 - val_accuracy: 0.7150 - val_prc: 0.8459 - lr: 3.1250e-05\n",
            "Epoch 211/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8268 - accuracy: 0.6827 - prc: 0.7808\n",
            "Epoch 211: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8268 - accuracy: 0.6827 - prc: 0.7808 - val_loss: 0.8327 - val_accuracy: 0.7202 - val_prc: 0.8437 - lr: 3.1250e-05\n",
            "Epoch 212/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8296 - accuracy: 0.6848 - prc: 0.7800\n",
            "Epoch 212: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8296 - accuracy: 0.6848 - prc: 0.7800 - val_loss: 0.8414 - val_accuracy: 0.7254 - val_prc: 0.8395 - lr: 3.1250e-05\n",
            "Epoch 213/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8270 - accuracy: 0.6846 - prc: 0.7807\n",
            "Epoch 213: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8270 - accuracy: 0.6846 - prc: 0.7807 - val_loss: 0.8378 - val_accuracy: 0.7306 - val_prc: 0.8391 - lr: 3.1250e-05\n",
            "Epoch 214/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8306 - accuracy: 0.6828 - prc: 0.7801\n",
            "Epoch 214: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8306 - accuracy: 0.6828 - prc: 0.7801 - val_loss: 0.8109 - val_accuracy: 0.7150 - val_prc: 0.8464 - lr: 3.1250e-05\n",
            "Epoch 215/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8353 - accuracy: 0.6814 - prc: 0.7775\n",
            "Epoch 215: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8353 - accuracy: 0.6814 - prc: 0.7775 - val_loss: 0.8268 - val_accuracy: 0.7098 - val_prc: 0.8405 - lr: 3.1250e-05\n",
            "Epoch 216/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8286 - accuracy: 0.6842 - prc: 0.7806\n",
            "Epoch 216: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8286 - accuracy: 0.6842 - prc: 0.7806 - val_loss: 0.8312 - val_accuracy: 0.7098 - val_prc: 0.8381 - lr: 3.1250e-05\n",
            "Epoch 217/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8255 - accuracy: 0.6845 - prc: 0.7818\n",
            "Epoch 217: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8255 - accuracy: 0.6845 - prc: 0.7818 - val_loss: 0.8582 - val_accuracy: 0.7254 - val_prc: 0.8343 - lr: 3.1250e-05\n",
            "Epoch 218/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8275 - accuracy: 0.6814 - prc: 0.7810\n",
            "Epoch 218: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8275 - accuracy: 0.6814 - prc: 0.7810 - val_loss: 0.8331 - val_accuracy: 0.7202 - val_prc: 0.8385 - lr: 3.1250e-05\n",
            "Epoch 219/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8298 - accuracy: 0.6825 - prc: 0.7803\n",
            "Epoch 219: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8298 - accuracy: 0.6825 - prc: 0.7803 - val_loss: 0.8412 - val_accuracy: 0.7202 - val_prc: 0.8368 - lr: 3.1250e-05\n",
            "Epoch 220/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8312 - accuracy: 0.6840 - prc: 0.7792\n",
            "Epoch 220: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8312 - accuracy: 0.6840 - prc: 0.7792 - val_loss: 0.8218 - val_accuracy: 0.7202 - val_prc: 0.8420 - lr: 3.1250e-05\n",
            "Epoch 221/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8357 - accuracy: 0.6795 - prc: 0.7770\n",
            "Epoch 221: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8357 - accuracy: 0.6795 - prc: 0.7770 - val_loss: 0.8398 - val_accuracy: 0.7306 - val_prc: 0.8366 - lr: 3.1250e-05\n",
            "Epoch 222/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8254 - accuracy: 0.6840 - prc: 0.7816\n",
            "Epoch 222: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8254 - accuracy: 0.6840 - prc: 0.7816 - val_loss: 0.8336 - val_accuracy: 0.7254 - val_prc: 0.8391 - lr: 3.1250e-05\n",
            "Epoch 223/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8250 - accuracy: 0.6884 - prc: 0.7821\n",
            "Epoch 223: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8250 - accuracy: 0.6884 - prc: 0.7821 - val_loss: 0.8261 - val_accuracy: 0.7254 - val_prc: 0.8410 - lr: 3.1250e-05\n",
            "Epoch 224/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8287 - accuracy: 0.6820 - prc: 0.7803\n",
            "Epoch 224: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8287 - accuracy: 0.6820 - prc: 0.7803 - val_loss: 0.8068 - val_accuracy: 0.7202 - val_prc: 0.8447 - lr: 3.1250e-05\n",
            "Epoch 225/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8281 - accuracy: 0.6850 - prc: 0.7809\n",
            "Epoch 225: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8281 - accuracy: 0.6850 - prc: 0.7809 - val_loss: 0.8375 - val_accuracy: 0.7202 - val_prc: 0.8377 - lr: 3.1250e-05\n",
            "Epoch 226/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8274 - accuracy: 0.6840 - prc: 0.7814\n",
            "Epoch 226: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8274 - accuracy: 0.6840 - prc: 0.7814 - val_loss: 0.8248 - val_accuracy: 0.7150 - val_prc: 0.8405 - lr: 3.1250e-05\n",
            "Epoch 227/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8235 - accuracy: 0.6847 - prc: 0.7827\n",
            "Epoch 227: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8235 - accuracy: 0.6847 - prc: 0.7827 - val_loss: 0.8431 - val_accuracy: 0.7150 - val_prc: 0.8390 - lr: 3.1250e-05\n",
            "Epoch 228/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8277 - accuracy: 0.6820 - prc: 0.7805\n",
            "Epoch 228: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8277 - accuracy: 0.6820 - prc: 0.7805 - val_loss: 0.8247 - val_accuracy: 0.7098 - val_prc: 0.8412 - lr: 3.1250e-05\n",
            "Epoch 229/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8238 - accuracy: 0.6860 - prc: 0.7819\n",
            "Epoch 229: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8238 - accuracy: 0.6860 - prc: 0.7819 - val_loss: 0.8344 - val_accuracy: 0.7202 - val_prc: 0.8398 - lr: 3.1250e-05\n",
            "Epoch 230/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8282 - accuracy: 0.6850 - prc: 0.7820\n",
            "Epoch 230: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8282 - accuracy: 0.6850 - prc: 0.7820 - val_loss: 0.8287 - val_accuracy: 0.7254 - val_prc: 0.8412 - lr: 3.1250e-05\n",
            "Epoch 231/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8305 - accuracy: 0.6827 - prc: 0.7806\n",
            "Epoch 231: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8305 - accuracy: 0.6827 - prc: 0.7806 - val_loss: 0.8381 - val_accuracy: 0.7254 - val_prc: 0.8382 - lr: 3.1250e-05\n",
            "Epoch 232/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8261 - accuracy: 0.6844 - prc: 0.7819\n",
            "Epoch 232: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8261 - accuracy: 0.6844 - prc: 0.7819 - val_loss: 0.8083 - val_accuracy: 0.7254 - val_prc: 0.8449 - lr: 3.1250e-05\n",
            "Epoch 233/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8222 - accuracy: 0.6868 - prc: 0.7833\n",
            "Epoch 233: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8222 - accuracy: 0.6868 - prc: 0.7833 - val_loss: 0.8268 - val_accuracy: 0.7202 - val_prc: 0.8433 - lr: 3.1250e-05\n",
            "Epoch 234/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8257 - accuracy: 0.6834 - prc: 0.7810\n",
            "Epoch 234: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8257 - accuracy: 0.6834 - prc: 0.7810 - val_loss: 0.8396 - val_accuracy: 0.7358 - val_prc: 0.8395 - lr: 3.1250e-05\n",
            "Epoch 235/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8311 - accuracy: 0.6834 - prc: 0.7797\n",
            "Epoch 235: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8311 - accuracy: 0.6834 - prc: 0.7797 - val_loss: 0.8501 - val_accuracy: 0.7306 - val_prc: 0.8354 - lr: 3.1250e-05\n",
            "Epoch 236/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8251 - accuracy: 0.6814 - prc: 0.7828\n",
            "Epoch 236: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8251 - accuracy: 0.6814 - prc: 0.7828 - val_loss: 0.8384 - val_accuracy: 0.7254 - val_prc: 0.8372 - lr: 3.1250e-05\n",
            "Epoch 237/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8231 - accuracy: 0.6860 - prc: 0.7836\n",
            "Epoch 237: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8231 - accuracy: 0.6860 - prc: 0.7836 - val_loss: 0.8225 - val_accuracy: 0.7150 - val_prc: 0.8393 - lr: 3.1250e-05\n",
            "Epoch 238/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8198 - accuracy: 0.6887 - prc: 0.7847\n",
            "Epoch 238: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8198 - accuracy: 0.6887 - prc: 0.7847 - val_loss: 0.8053 - val_accuracy: 0.7254 - val_prc: 0.8471 - lr: 3.1250e-05\n",
            "Epoch 239/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8278 - accuracy: 0.6826 - prc: 0.7810\n",
            "Epoch 239: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8278 - accuracy: 0.6826 - prc: 0.7810 - val_loss: 0.8493 - val_accuracy: 0.7150 - val_prc: 0.8365 - lr: 3.1250e-05\n",
            "Epoch 240/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8249 - accuracy: 0.6843 - prc: 0.7812\n",
            "Epoch 240: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8249 - accuracy: 0.6843 - prc: 0.7812 - val_loss: 0.8367 - val_accuracy: 0.7150 - val_prc: 0.8383 - lr: 3.1250e-05\n",
            "Epoch 241/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8286 - accuracy: 0.6830 - prc: 0.7801\n",
            "Epoch 241: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8286 - accuracy: 0.6830 - prc: 0.7801 - val_loss: 0.8363 - val_accuracy: 0.7306 - val_prc: 0.8391 - lr: 3.1250e-05\n",
            "Epoch 242/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8302 - accuracy: 0.6836 - prc: 0.7791\n",
            "Epoch 242: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8302 - accuracy: 0.6836 - prc: 0.7791 - val_loss: 0.8082 - val_accuracy: 0.7150 - val_prc: 0.8439 - lr: 3.1250e-05\n",
            "Epoch 243/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8293 - accuracy: 0.6844 - prc: 0.7802\n",
            "Epoch 243: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8293 - accuracy: 0.6844 - prc: 0.7802 - val_loss: 0.8005 - val_accuracy: 0.7202 - val_prc: 0.8466 - lr: 3.1250e-05\n",
            "Epoch 244/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8277 - accuracy: 0.6870 - prc: 0.7818\n",
            "Epoch 244: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8277 - accuracy: 0.6870 - prc: 0.7818 - val_loss: 0.7982 - val_accuracy: 0.7254 - val_prc: 0.8465 - lr: 3.1250e-05\n",
            "Epoch 245/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8259 - accuracy: 0.6825 - prc: 0.7810\n",
            "Epoch 245: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8259 - accuracy: 0.6825 - prc: 0.7810 - val_loss: 0.8185 - val_accuracy: 0.7202 - val_prc: 0.8422 - lr: 3.1250e-05\n",
            "Epoch 246/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8316 - accuracy: 0.6836 - prc: 0.7796\n",
            "Epoch 246: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8316 - accuracy: 0.6836 - prc: 0.7796 - val_loss: 0.8157 - val_accuracy: 0.7358 - val_prc: 0.8452 - lr: 3.1250e-05\n",
            "Epoch 247/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8258 - accuracy: 0.6845 - prc: 0.7815\n",
            "Epoch 247: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8258 - accuracy: 0.6845 - prc: 0.7815 - val_loss: 0.8492 - val_accuracy: 0.7150 - val_prc: 0.8354 - lr: 3.1250e-05\n",
            "Epoch 248/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8182 - accuracy: 0.6866 - prc: 0.7843\n",
            "Epoch 248: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8182 - accuracy: 0.6866 - prc: 0.7843 - val_loss: 0.8214 - val_accuracy: 0.7150 - val_prc: 0.8438 - lr: 3.1250e-05\n",
            "Epoch 249/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8287 - accuracy: 0.6824 - prc: 0.7804\n",
            "Epoch 249: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8287 - accuracy: 0.6824 - prc: 0.7804 - val_loss: 0.8195 - val_accuracy: 0.7254 - val_prc: 0.8413 - lr: 3.1250e-05\n",
            "Epoch 250/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8245 - accuracy: 0.6848 - prc: 0.7812\n",
            "Epoch 250: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8245 - accuracy: 0.6848 - prc: 0.7812 - val_loss: 0.8195 - val_accuracy: 0.7150 - val_prc: 0.8417 - lr: 3.1250e-05\n",
            "Epoch 251/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8262 - accuracy: 0.6825 - prc: 0.7813\n",
            "Epoch 251: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8262 - accuracy: 0.6825 - prc: 0.7813 - val_loss: 0.8184 - val_accuracy: 0.7202 - val_prc: 0.8422 - lr: 3.1250e-05\n",
            "Epoch 252/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8266 - accuracy: 0.6860 - prc: 0.7818\n",
            "Epoch 252: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8266 - accuracy: 0.6860 - prc: 0.7818 - val_loss: 0.8513 - val_accuracy: 0.7202 - val_prc: 0.8356 - lr: 3.1250e-05\n",
            "Epoch 253/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8220 - accuracy: 0.6875 - prc: 0.7835\n",
            "Epoch 253: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8220 - accuracy: 0.6875 - prc: 0.7835 - val_loss: 0.8110 - val_accuracy: 0.7306 - val_prc: 0.8443 - lr: 3.1250e-05\n",
            "Epoch 254/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8259 - accuracy: 0.6842 - prc: 0.7815\n",
            "Epoch 254: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8259 - accuracy: 0.6842 - prc: 0.7815 - val_loss: 0.8257 - val_accuracy: 0.7202 - val_prc: 0.8402 - lr: 3.1250e-05\n",
            "Epoch 255/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8292 - accuracy: 0.6798 - prc: 0.7787\n",
            "Epoch 255: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8292 - accuracy: 0.6798 - prc: 0.7787 - val_loss: 0.8278 - val_accuracy: 0.7254 - val_prc: 0.8420 - lr: 3.1250e-05\n",
            "Epoch 256/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8258 - accuracy: 0.6826 - prc: 0.7815\n",
            "Epoch 256: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8258 - accuracy: 0.6826 - prc: 0.7815 - val_loss: 0.8305 - val_accuracy: 0.7150 - val_prc: 0.8409 - lr: 3.1250e-05\n",
            "Epoch 257/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8284 - accuracy: 0.6817 - prc: 0.7800\n",
            "Epoch 257: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8284 - accuracy: 0.6817 - prc: 0.7800 - val_loss: 0.8200 - val_accuracy: 0.7098 - val_prc: 0.8419 - lr: 3.1250e-05\n",
            "Epoch 258/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8246 - accuracy: 0.6850 - prc: 0.7825\n",
            "Epoch 258: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8246 - accuracy: 0.6850 - prc: 0.7825 - val_loss: 0.8093 - val_accuracy: 0.7202 - val_prc: 0.8452 - lr: 3.1250e-05\n",
            "Epoch 259/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8273 - accuracy: 0.6840 - prc: 0.7811\n",
            "Epoch 259: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8273 - accuracy: 0.6840 - prc: 0.7811 - val_loss: 0.8236 - val_accuracy: 0.7254 - val_prc: 0.8414 - lr: 1.5625e-05\n",
            "Epoch 260/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8243 - accuracy: 0.6853 - prc: 0.7824\n",
            "Epoch 260: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8243 - accuracy: 0.6853 - prc: 0.7824 - val_loss: 0.8204 - val_accuracy: 0.7306 - val_prc: 0.8419 - lr: 1.5625e-05\n",
            "Epoch 261/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8248 - accuracy: 0.6833 - prc: 0.7808\n",
            "Epoch 261: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8248 - accuracy: 0.6833 - prc: 0.7808 - val_loss: 0.8351 - val_accuracy: 0.7150 - val_prc: 0.8388 - lr: 1.5625e-05\n",
            "Epoch 262/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8284 - accuracy: 0.6849 - prc: 0.7808\n",
            "Epoch 262: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8284 - accuracy: 0.6849 - prc: 0.7808 - val_loss: 0.8289 - val_accuracy: 0.7254 - val_prc: 0.8400 - lr: 1.5625e-05\n",
            "Epoch 263/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8238 - accuracy: 0.6853 - prc: 0.7822\n",
            "Epoch 263: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8238 - accuracy: 0.6853 - prc: 0.7822 - val_loss: 0.8477 - val_accuracy: 0.7202 - val_prc: 0.8359 - lr: 1.5625e-05\n",
            "Epoch 264/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8220 - accuracy: 0.6816 - prc: 0.7820\n",
            "Epoch 264: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8220 - accuracy: 0.6816 - prc: 0.7820 - val_loss: 0.8063 - val_accuracy: 0.7254 - val_prc: 0.8450 - lr: 1.5625e-05\n",
            "Epoch 265/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8255 - accuracy: 0.6839 - prc: 0.7813\n",
            "Epoch 265: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8255 - accuracy: 0.6839 - prc: 0.7813 - val_loss: 0.8333 - val_accuracy: 0.7254 - val_prc: 0.8394 - lr: 1.5625e-05\n",
            "Epoch 266/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8247 - accuracy: 0.6844 - prc: 0.7819\n",
            "Epoch 266: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8247 - accuracy: 0.6844 - prc: 0.7819 - val_loss: 0.8267 - val_accuracy: 0.7254 - val_prc: 0.8416 - lr: 1.5625e-05\n",
            "Epoch 267/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8278 - accuracy: 0.6837 - prc: 0.7805\n",
            "Epoch 267: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8278 - accuracy: 0.6837 - prc: 0.7805 - val_loss: 0.8137 - val_accuracy: 0.7202 - val_prc: 0.8420 - lr: 1.5625e-05\n",
            "Epoch 268/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8223 - accuracy: 0.6862 - prc: 0.7834\n",
            "Epoch 268: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8223 - accuracy: 0.6862 - prc: 0.7834 - val_loss: 0.8201 - val_accuracy: 0.7150 - val_prc: 0.8401 - lr: 1.5625e-05\n",
            "Epoch 269/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8259 - accuracy: 0.6826 - prc: 0.7809\n",
            "Epoch 269: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8259 - accuracy: 0.6826 - prc: 0.7809 - val_loss: 0.8288 - val_accuracy: 0.7150 - val_prc: 0.8386 - lr: 1.0000e-05\n",
            "Epoch 270/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8206 - accuracy: 0.6870 - prc: 0.7840\n",
            "Epoch 270: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8206 - accuracy: 0.6870 - prc: 0.7840 - val_loss: 0.8388 - val_accuracy: 0.7150 - val_prc: 0.8380 - lr: 1.0000e-05\n",
            "Epoch 271/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8253 - accuracy: 0.6849 - prc: 0.7827\n",
            "Epoch 271: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8253 - accuracy: 0.6849 - prc: 0.7827 - val_loss: 0.8139 - val_accuracy: 0.7202 - val_prc: 0.8429 - lr: 1.0000e-05\n",
            "Epoch 272/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8197 - accuracy: 0.6861 - prc: 0.7833\n",
            "Epoch 272: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8197 - accuracy: 0.6861 - prc: 0.7833 - val_loss: 0.8293 - val_accuracy: 0.7150 - val_prc: 0.8396 - lr: 1.0000e-05\n",
            "Epoch 273/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8242 - accuracy: 0.6840 - prc: 0.7831\n",
            "Epoch 273: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8242 - accuracy: 0.6840 - prc: 0.7831 - val_loss: 0.8188 - val_accuracy: 0.7202 - val_prc: 0.8404 - lr: 1.0000e-05\n",
            "Epoch 274/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8241 - accuracy: 0.6832 - prc: 0.7828\n",
            "Epoch 274: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8241 - accuracy: 0.6832 - prc: 0.7828 - val_loss: 0.8210 - val_accuracy: 0.7202 - val_prc: 0.8417 - lr: 1.0000e-05\n",
            "Epoch 275/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8242 - accuracy: 0.6857 - prc: 0.7825\n",
            "Epoch 275: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8242 - accuracy: 0.6857 - prc: 0.7825 - val_loss: 0.8186 - val_accuracy: 0.7098 - val_prc: 0.8413 - lr: 1.0000e-05\n",
            "Epoch 276/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8227 - accuracy: 0.6870 - prc: 0.7829\n",
            "Epoch 276: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 116s 112ms/step - loss: 0.8227 - accuracy: 0.6870 - prc: 0.7829 - val_loss: 0.8140 - val_accuracy: 0.7098 - val_prc: 0.8423 - lr: 1.0000e-05\n",
            "Epoch 277/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8303 - accuracy: 0.6848 - prc: 0.7797\n",
            "Epoch 277: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8303 - accuracy: 0.6848 - prc: 0.7797 - val_loss: 0.8326 - val_accuracy: 0.7150 - val_prc: 0.8385 - lr: 1.0000e-05\n",
            "Epoch 278/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8187 - accuracy: 0.6868 - prc: 0.7845\n",
            "Epoch 278: val_prc did not improve from 0.86456\n",
            "1027/1027 [==============================] - 115s 112ms/step - loss: 0.8187 - accuracy: 0.6868 - prc: 0.7845 - val_loss: 0.8183 - val_accuracy: 0.7150 - val_prc: 0.8412 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "# this could also be the output a different Keras model or layer\n",
        "input_shape = X_train.shape[1:]\n",
        "input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "x = data_augmentation(input_tensor)\n",
        "#x = rescale_layer(x)\n",
        "\n",
        "base_model = ResNet50(input_shape=input_shape, weights='imagenet', include_top=False)\n",
        "x = base_model(x)\n",
        "\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "#x = base_model.output\n",
        "#x = GlobalAveragePooling2D()(x)\n",
        "#x = Dropout(0.2)(x)\n",
        "x = Flatten()(x)\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(128, activation='relu')(x)\n",
        "#x = Dropout(0.2)(x)\n",
        "# and a logistic layer -- let's say we have 200 classes\n",
        "predictions = Dense(7, activation='softmax')(x)\n",
        "#predictions = Dense(9)(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=input_tensor, outputs=predictions)\n",
        "\n",
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "#model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "#model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# train the model on the new data for a few epochs\n",
        "#hst = model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val, y_val))\n",
        "\n",
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=['accuracy', keras.metrics.AUC(name='prc', curve='PR')])\n",
        "#hst = model.fit(dataaugment.flow(X_train,y_train, batch_size=BATCH_SIZE),\n",
        "hst = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,      \n",
        "                    #steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6WA3NkiOqePA"
      },
      "outputs": [],
      "source": [
        "# load the saved model\n",
        "best_model = load_model(best_model_fpath)\n",
        "# evaluate the model\n",
        "#_, train_acc = best_model.evaluate(X_train, y_train, verbose=0)\n",
        "#_, test_acc = best_model.evaluate(X_val, y_val, verbose=0)\n",
        "#print('Train: %.3f, Val: %.3f' % (train_acc, test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W3IyWjdGG4Xq",
        "outputId": "e401a609-37f2-4df9-85a6-f4979247b642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "balanced accuracy on training 0.6069221637088672\n",
            "balanced accuracy on validation 0.5952252741451348\n"
          ]
        }
      ],
      "source": [
        "y_val_pred = best_model.predict(X_val)\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vXnW3lmCgln3",
        "outputId": "b63e2d30-1697-4baa-f072-1a943015ded6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PgksTFkOAq"
      },
      "source": [
        "#Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l8bZo4LoEiQf"
      },
      "outputs": [],
      "source": [
        "#finetune_model_fpath = '/content/drive/MyDrive/PHD/Model/finetune_model.h5'\n",
        "#mc_finetune = ModelCheckpoint(finetune_model_fpath, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr1jnSM7yzJc",
        "outputId": "9f126adf-2cac-4993-c00e-01804c7de12b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8228 - accuracy: 0.6843"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 116s 110ms/step - loss: 0.8228 - accuracy: 0.6843 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 2/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8268 - accuracy: 0.6858"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8268 - accuracy: 0.6858 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 3/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8219 - accuracy: 0.6850"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8219 - accuracy: 0.6850 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 4/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8241 - accuracy: 0.6859"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8241 - accuracy: 0.6859 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 5/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8191 - accuracy: 0.6860"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8191 - accuracy: 0.6860 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 6/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8212 - accuracy: 0.6849"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8212 - accuracy: 0.6849 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 7/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8239 - accuracy: 0.6860"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 113s 110ms/step - loss: 0.8239 - accuracy: 0.6860 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 8/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8234 - accuracy: 0.6856"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8234 - accuracy: 0.6856 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 9/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8283 - accuracy: 0.6822"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8283 - accuracy: 0.6822 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 10/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8232 - accuracy: 0.6853"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8232 - accuracy: 0.6853 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 11/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8259 - accuracy: 0.6853"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8259 - accuracy: 0.6853 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 12/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8203 - accuracy: 0.6845"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8203 - accuracy: 0.6845 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 13/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8210 - accuracy: 0.6862"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8210 - accuracy: 0.6862 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 14/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8221 - accuracy: 0.6853"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8221 - accuracy: 0.6853 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 15/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8246 - accuracy: 0.6824"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8246 - accuracy: 0.6824 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-04\n",
            "Epoch 16/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8235 - accuracy: 0.6866"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8235 - accuracy: 0.6866 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 17/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8234 - accuracy: 0.6869"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8234 - accuracy: 0.6869 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 18/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8266 - accuracy: 0.6832"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8266 - accuracy: 0.6832 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 19/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8212 - accuracy: 0.6832"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8212 - accuracy: 0.6832 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 20/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8229 - accuracy: 0.6847"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8229 - accuracy: 0.6847 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 21/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8215 - accuracy: 0.6880"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8215 - accuracy: 0.6880 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 22/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8216 - accuracy: 0.6873"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8216 - accuracy: 0.6873 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 23/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8239 - accuracy: 0.6861"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8239 - accuracy: 0.6861 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 24/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8246 - accuracy: 0.6855"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8246 - accuracy: 0.6855 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 25/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8223 - accuracy: 0.6831"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8223 - accuracy: 0.6831 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 26/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8277 - accuracy: 0.6859"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8277 - accuracy: 0.6859 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 27/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8213 - accuracy: 0.6842"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 113s 110ms/step - loss: 0.8213 - accuracy: 0.6842 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 28/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8239 - accuracy: 0.6856"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 113s 110ms/step - loss: 0.8239 - accuracy: 0.6856 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 29/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8264 - accuracy: 0.6856"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8264 - accuracy: 0.6856 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 30/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8241 - accuracy: 0.6861"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8241 - accuracy: 0.6861 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 31/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8211 - accuracy: 0.6857"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8211 - accuracy: 0.6857 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 32/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8167 - accuracy: 0.6897"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8167 - accuracy: 0.6897 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 33/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8255 - accuracy: 0.6834"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8255 - accuracy: 0.6834 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 34/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8241 - accuracy: 0.6841"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8241 - accuracy: 0.6841 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 35/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8248 - accuracy: 0.6851"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8248 - accuracy: 0.6851 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 36/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8222 - accuracy: 0.6853"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8222 - accuracy: 0.6853 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 37/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8279 - accuracy: 0.6845"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8279 - accuracy: 0.6845 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 38/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8217 - accuracy: 0.6825"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8217 - accuracy: 0.6825 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 39/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8267 - accuracy: 0.6855"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8267 - accuracy: 0.6855 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 40/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8272 - accuracy: 0.6817"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8272 - accuracy: 0.6817 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 41/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8226 - accuracy: 0.6855"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8226 - accuracy: 0.6855 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 42/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8167 - accuracy: 0.6870"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8167 - accuracy: 0.6870 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 2.5000e-05\n",
            "Epoch 43/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8205 - accuracy: 0.6855"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8205 - accuracy: 0.6855 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.2500e-05\n",
            "Epoch 44/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8262 - accuracy: 0.6837"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8262 - accuracy: 0.6837 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.2500e-05\n",
            "Epoch 45/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8236 - accuracy: 0.6841"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8236 - accuracy: 0.6841 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.2500e-05\n",
            "Epoch 46/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8260 - accuracy: 0.6840"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8260 - accuracy: 0.6840 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.2500e-05\n",
            "Epoch 47/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8257 - accuracy: 0.6817"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8257 - accuracy: 0.6817 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.2500e-05\n",
            "Epoch 48/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8198 - accuracy: 0.6842"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8198 - accuracy: 0.6842 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.2500e-05\n",
            "Epoch 49/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8262 - accuracy: 0.6864"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8262 - accuracy: 0.6864 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.2500e-05\n",
            "Epoch 50/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8268 - accuracy: 0.6840"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8268 - accuracy: 0.6840 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.2500e-05\n",
            "Epoch 51/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8207 - accuracy: 0.6870"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8207 - accuracy: 0.6870 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.2500e-05\n",
            "Epoch 52/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8244 - accuracy: 0.6840"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8244 - accuracy: 0.6840 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.2500e-05\n",
            "Epoch 53/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8189 - accuracy: 0.6905"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8189 - accuracy: 0.6905 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-05\n",
            "Epoch 54/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8231 - accuracy: 0.6818"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8231 - accuracy: 0.6818 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-05\n",
            "Epoch 55/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8232 - accuracy: 0.6839"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8232 - accuracy: 0.6839 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-05\n",
            "Epoch 56/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8288 - accuracy: 0.6833"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8288 - accuracy: 0.6833 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-05\n",
            "Epoch 57/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8194 - accuracy: 0.6879"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8194 - accuracy: 0.6879 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-05\n",
            "Epoch 58/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8287 - accuracy: 0.6827"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 113s 110ms/step - loss: 0.8287 - accuracy: 0.6827 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-05\n",
            "Epoch 59/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8197 - accuracy: 0.6873"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8197 - accuracy: 0.6873 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-05\n",
            "Epoch 60/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8175 - accuracy: 0.6900"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8175 - accuracy: 0.6900 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-05\n",
            "Epoch 61/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8235 - accuracy: 0.6888"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1027/1027 [==============================] - 112s 109ms/step - loss: 0.8235 - accuracy: 0.6888 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-05\n",
            "Epoch 62/1000\n",
            "1027/1027 [==============================] - ETA: 0s - loss: 0.8238 - accuracy: 0.6863"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_prc available, skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1027/1027 [==============================] - 112s 109ms/step - loss: 0.8238 - accuracy: 0.6863 - val_loss: 0.8183 - val_accuracy: 0.7150 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "# we chose to train the top 2 resnet blocks, i.e. we will freeze\n",
        "# the first 49 layers and unfreeze the rest:\n",
        "limit = 171\n",
        "for layer in model.layers[:limit]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[limit:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "# we need to recompile the model for these modifications to take effect\n",
        "# we use SGD with a low learning rate\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "#model.compile(optimizer=SGD(learning_rate=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
        "# alongside the top Dense layers\n",
        "#hst2 = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val, y_val))\n",
        "optimizer_SGD = SGD(learning_rate=0.0001, momentum=0.9)\n",
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=['accuracy', keras.metrics.AUC(name='prc', curve='PR')])\n",
        "#hst2 = model.fit(dataaugment.flow(X_train,y_train, batch_size=BATCH_SIZE),\n",
        "hst2 = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,\n",
        " #                   steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QoiQK0xECpU"
      },
      "outputs": [],
      "source": [
        "# load the saved model\n",
        "finetune_model = load_model(best_model_fpath)\n",
        "# evaluate the model\n",
        "#_, train_acc = finetune_model.evaluate(X_train, y_train, verbose=0)\n",
        "#_, test_acc = finetune_model.evaluate(X_val, y_val, verbose=0)\n",
        "#print('Train: %.3f, Val: %.3f' % (train_acc, test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "3EeAKIX_EB9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ba9463-2521-427d-d612-7dca7d49dc7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "balanced accuracy on validation 0.5952252741451348\n"
          ]
        }
      ],
      "source": [
        "y_val_pred = finetune_model.predict(X_val)\n",
        "y_train_pred = finetune_model.predict(X_train)\n",
        "\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "gH92sRmlETE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5a5a5b-3a8f-4d97-9900-e77e714a10ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score on val data:  (0.5480594343000358, 0.5952252741451348, 0.4979639157140638, None)\n"
          ]
        }
      ],
      "source": [
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))\n",
        "#print('Score on test data: ',precision_recall_fscore_support(y_test, y_pred2, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "vO1aAQBmiy0K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e575f321-5579-422d-aef7-0e6ce6819520"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst2.history['accuracy'])\n",
        "plt.plot(hst2.history['val_accuracy'])\n",
        "plt.title('model accuracy after tunning')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K908bbiYwbS"
      },
      "source": [
        "#Testing\n",
        "Result from ISIC Live\n",
        "last_model: 0.506\n",
        "best_model: 0.478"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "NeMY2yvMYxsC"
      },
      "outputs": [],
      "source": [
        "dir_test = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Test_Input/'\n",
        "filepaths = sorted( filter( lambda x: (os.path.isfile(os.path.join(dir_test, x))) and (x.endswith('.jpg')),\n",
        "                        os.listdir(dir_test) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6ic95mefkpG3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "f85c8947-9fb2-4402-e24b-39c508703cd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 image                                          FilePaths\n",
              "0     ISIC_0034524.jpg  /content/drive/MyDrive/PHD/Datasets/isic2018/I...\n",
              "1     ISIC_0034525.jpg  /content/drive/MyDrive/PHD/Datasets/isic2018/I...\n",
              "2     ISIC_0034526.jpg  /content/drive/MyDrive/PHD/Datasets/isic2018/I...\n",
              "3     ISIC_0034527.jpg  /content/drive/MyDrive/PHD/Datasets/isic2018/I...\n",
              "4     ISIC_0034528.jpg  /content/drive/MyDrive/PHD/Datasets/isic2018/I...\n",
              "...                ...                                                ...\n",
              "1507  ISIC_0036060.jpg  /content/drive/MyDrive/PHD/Datasets/isic2018/I...\n",
              "1508  ISIC_0036061.jpg  /content/drive/MyDrive/PHD/Datasets/isic2018/I...\n",
              "1509  ISIC_0036062.jpg  /content/drive/MyDrive/PHD/Datasets/isic2018/I...\n",
              "1510  ISIC_0036063.jpg  /content/drive/MyDrive/PHD/Datasets/isic2018/I...\n",
              "1511  ISIC_0036064.jpg  /content/drive/MyDrive/PHD/Datasets/isic2018/I...\n",
              "\n",
              "[1512 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c1d5209-6f9a-49fa-982b-f64ddd632c26\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>FilePaths</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ISIC_0034524.jpg</td>\n",
              "      <td>/content/drive/MyDrive/PHD/Datasets/isic2018/I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ISIC_0034525.jpg</td>\n",
              "      <td>/content/drive/MyDrive/PHD/Datasets/isic2018/I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISIC_0034526.jpg</td>\n",
              "      <td>/content/drive/MyDrive/PHD/Datasets/isic2018/I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ISIC_0034527.jpg</td>\n",
              "      <td>/content/drive/MyDrive/PHD/Datasets/isic2018/I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ISIC_0034528.jpg</td>\n",
              "      <td>/content/drive/MyDrive/PHD/Datasets/isic2018/I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1507</th>\n",
              "      <td>ISIC_0036060.jpg</td>\n",
              "      <td>/content/drive/MyDrive/PHD/Datasets/isic2018/I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1508</th>\n",
              "      <td>ISIC_0036061.jpg</td>\n",
              "      <td>/content/drive/MyDrive/PHD/Datasets/isic2018/I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1509</th>\n",
              "      <td>ISIC_0036062.jpg</td>\n",
              "      <td>/content/drive/MyDrive/PHD/Datasets/isic2018/I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1510</th>\n",
              "      <td>ISIC_0036063.jpg</td>\n",
              "      <td>/content/drive/MyDrive/PHD/Datasets/isic2018/I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1511</th>\n",
              "      <td>ISIC_0036064.jpg</td>\n",
              "      <td>/content/drive/MyDrive/PHD/Datasets/isic2018/I...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1512 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c1d5209-6f9a-49fa-982b-f64ddd632c26')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c1d5209-6f9a-49fa-982b-f64ddd632c26 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c1d5209-6f9a-49fa-982b-f64ddd632c26');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "df_test = pd.DataFrame(filepaths, columns =['image'])\n",
        "df_test['FilePaths'] = dir_test + df_test['image']\n",
        "#df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "NBa1TxPuY8ni"
      },
      "outputs": [],
      "source": [
        "df_test['image'] = df_test['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "60LYAT7VsNOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250ff47a-6c6e-40e7-c950-d600f9a18bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1512, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "X_test = np.asarray(df_test['image'].tolist())\n",
        "print(np.array(X_test).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7ml90JZ8FK"
      },
      "source": [
        "Calculate y_pred from training and testing for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "dIX0AmEFNv3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86fe2da0-3eba-4fba-ee42-c8ff0fff5709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y_pred2 (1512, 7)\n",
            "y_pred2 1512\n"
          ]
        }
      ],
      "source": [
        "# predicted labels\n",
        "Y_pred2 = model.predict(X_test)\n",
        "print(\"Y_pred2\", Y_pred2.shape)\n",
        "# rounded labels\n",
        "y_pred2 = np.argmax(Y_pred2, axis=1)\n",
        "print(\"y_pred2\", y_pred2.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "7oeArO5CtxGb"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame(Y_pred2, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "df_pred['image'] = df_test['FilePaths'].map(lambda x: x.replace(dir_test, '').replace('.jpg', ''))\n",
        "df_pred = df_pred[['image', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "sOnjc3RJ0e4T"
      },
      "outputs": [],
      "source": [
        "df_pred.set_index(\"image\", inplace = True)\n",
        "df_pred.to_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/response_SMOTEOversampling_last.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK4zbtoaAaC"
      },
      "source": [
        "#Confusion Metric on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "E4nEpmkZaTZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea76e83-81a6-469b-d404-1dc445918d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2   0   3   1   2   0   0]\n",
            " [  2   6   1   5   0   1   0]\n",
            " [  0   0  13   1   4   4   0]\n",
            " [  0   0   0   1   0   0   0]\n",
            " [  0   0   4   1  15   1   0]\n",
            " [  0   2   4   1   7 108   1]\n",
            " [  0   0   0   2   0   0   1]]\n"
          ]
        }
      ],
      "source": [
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1))\n",
        "\n",
        "print(cf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "gVtvW3YeaLlC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "1a55d66f-5af9-495a-da91-8c22c53e53f3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "ax = sns.heatmap(cf_matrix / cf_matrix.sum(axis=1, keepdims=True), annot=True, \n",
        "            cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix \\n');\n",
        "ax.set_xlabel('\\nPredicted')\n",
        "ax.set_ylabel('Actual ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "ax.yaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,3)\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Ey-1yjWGeKs7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "outputId": "8de78d76-eaa9-415c-e4ab-bc6d8e292395"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0, 'AKIEC'),\n",
              " Text(0, 0, 'BCC'),\n",
              " Text(0, 0, 'BKL'),\n",
              " Text(0, 0, 'DF'),\n",
              " Text(0, 0, 'MEL'),\n",
              " Text(0, 0, 'NV'),\n",
              " Text(0, 0, 'VASC')]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = df_val['Labels'].value_counts().sort_index()\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RcRGeofw-8tK",
        "uZv-B-ygCD57",
        "cNBXx28B9yGu",
        "US0KkIaVlTdU",
        "0jrJ33lUDkCM",
        "3K908bbiYwbS"
      ],
      "machine_shape": "hm",
      "name": "Skin Cancer Diagnosis using ISIC 2018 Dataset.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}